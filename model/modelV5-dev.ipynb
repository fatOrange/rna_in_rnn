{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.activations import relu\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解决keras 显存问题\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.25\n",
    "set_session(tf.Session(config=config)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = 10\n",
    "input_dim = 1\n",
    "MAX_LENGTH = 300 # 单个字符的长度\n",
    "output_length = 10\n",
    "output_dim = 6\n",
    "\n",
    "samples = 100\n",
    "hidden_dim = 6\n",
    "hidden_size = 300 # a half of Max_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_size = [embedding_length,embedding_dim,input_length]\n",
    "embedding_size = [1000,64,300]\n",
    "# hidden_size is the length of the kernel utils  = enc_units\n",
    "class EncoderRNN(Model):\n",
    "    def __init__(self, embedding_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = Embedding(embedding_size[0], embedding_size[1],input_length=embedding_size[2])\n",
    "        self.gru = GRU(hidden_size, return_sequences=True, return_state=True)\n",
    "        self.deepgru = GRU(hidden_size*2, return_sequences=True, return_state=True)\n",
    "        self.lastdeepgru = GRU(hidden_size*2, return_sequences=False, return_state=True)\n",
    "        self.bigru = Bidirectional(self.gru)\n",
    "\n",
    "    def __call__(self, en_input):\n",
    "        state_h = []\n",
    "        emb = self.embedding(en_inputs)\n",
    "        encoder_out, fwd_h1, bck_h1 = self.bigru(emb)\n",
    "        state_h.append(concatenate([fwd_h1, bck_h1]))\n",
    "        if hidden_dim>1:\n",
    "            for i in range(1,hidden_dim):\n",
    "                encoder_out, en_hidden = self.deepgru(encoder_out)\n",
    "                state_h.append(en_hidden)\n",
    "        output = encoder_out\n",
    "        hidden = state_h\n",
    "        return output, hidden\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        init_state = [tf.zeros((1, self.hidden_size)) for i in range(2)]\n",
    "        return init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(Model):\n",
    "    def __init__(self, embedding_size, hidden_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = Embedding(embedding_size[0], embedding_size[1],input_length=embedding_size[2])\n",
    "        self.gru = GRU(hidden_size, return_sequences=True, return_state=True)\n",
    "        self.deepgru = GRU(hidden_size*2, return_sequences=True, return_state=True)\n",
    "        self.lastdeepgru = GRU(hidden_size*2, return_sequences=False, return_state=True)\n",
    "        self.bigru = Bidirectional(self.gru)\n",
    "        \n",
    "        self.out = Dense(output_length)\n",
    "        self.softmax = Activation('softmax')\n",
    "\n",
    "    def __call__(self, de_input, hiddens):\n",
    "        state_h = []\n",
    "        emb = self.embedding(en_inputs)\n",
    "        emb = Activation('relu')(emb)\n",
    "        # !挖个坑，这里没有初始化hidden_state\n",
    "        output, fwd_h, bck_h = self.bigru(emb)\n",
    "        state_h.append(concatenate([fwd_h, bck_h]))\n",
    "        if len(hiddens)>2:\n",
    "            for hidden in hiddens[1:-1]:\n",
    "                output, de_hidden = self.deepgru(output, hidden)\n",
    "                state_h.append(de_hidden)\n",
    "        if len(hiddens)>1:\n",
    "            output, de_hidden = self.lastdeepgru(output, hidden)\n",
    "            state_h.append(de_hidden)\n",
    "        hidden = state_h\n",
    "        output = self.softmax(output)\n",
    "        print(type(output))\n",
    "        return output, hidden\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        init_state = tf.zeros((1, self.hidden_size))\n",
    "        return init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttnDecoderRNN(Model):\n",
    "    def __init__(self, embedding_size ,hidden_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = Embedding(embedding_size[0], embedding_size[1],input_length=embedding_size[2],name ='atten_embed')\n",
    "        self.attn = Dense(self.max_length,name = 'atten_attn')\n",
    "        self.attn_combine = Dense(self.hidden_size,name = 'atten_combine')\n",
    "        self.dropout = Dropout(self.dropout_p,name ='atten_dropout')\n",
    "        \n",
    "        self.gru = GRU(hidden_size, return_sequences=True, return_state=True,name = 'atten_gru')\n",
    "        self.deepgru = GRU(hidden_size*2, return_sequences=True, return_state=True,name = 'atten_deepgru')\n",
    "        self.lastdeepgru = GRU(hidden_size*2, return_sequences=False, return_state=True,name='atten_lastdeepgru')\n",
    "        self.bigru = Bidirectional(self.gru,name='atten_bigru')\n",
    "        self.batch_dot = Lambda(lambda layers:K.batch_dot(layers[0],layers[1]))\n",
    "        self.out = Dense(output_length)\n",
    "        self.softmax =Softmax(axis=-1)\n",
    "        #包装层\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def __call__(self, de_input, hiddens, encoder_outputs):\n",
    "        state_h = []\n",
    "        embedded = self.embedding(de_input)\n",
    "        print(embedded.shape)\n",
    "        # TODO ; use lambda https://www.cnblogs.com/jqpy1994/p/11433746.html or  https://keras.io/zh/layers/core/\n",
    "        #embedded = K.reshape(embedded,[samples,embedding_size[1]*embedding_size[2]])\n",
    "        embedded = Reshape((1,embedding_size[1]*embedding_size[2]))(embedded)\n",
    "        print(type(embedded))\n",
    "        embedded = Lambda(lambda x:K.squeeze(x,1))(embedded)\n",
    "    \n",
    "        embedded = self.dropout(embedded) # Dim:(Batch Size , Decoder Hidden Size + Embedding Size)\n",
    "        if type(hiddens) == type(list()):\n",
    "            hidden = hiddens[-1]\n",
    "        \n",
    "        # hidden = K.reshape(hidden,[samples,self.hidden_size*2]) \n",
    "        hidden = Reshape((1,self.hidden_size*2))(hidden)\n",
    "        hidden = Lambda(lambda x:K.squeeze(x,1))(hidden)\n",
    "        concat = Concatenate(1,name='atten_concat2')([embedded, hidden])\n",
    "        # note: 从这里开始，把两个向量拼接起来    \n",
    "        attn_weights =self.softmax(\n",
    "            Dense(self.max_length)(concat))\n",
    "        atten_weights = Reshape((1,-1))(attn_weights)\n",
    "        attn_applied = self.batch_dot([atten_weights,encoder_outputs])\n",
    "        print(attn_applied.shape)\n",
    "        attn_applied = Lambda(lambda x:K.squeeze(x,1))(attn_applied)\n",
    "\n",
    "        \n",
    "        output = Concatenate(1)([embedded, attn_applied])\n",
    "\n",
    "        output = self.attn_combine(output)\n",
    "        \n",
    "        output = ReLU()(output)\n",
    "        output = Reshape((1,-1))(output)\n",
    "        output, fwd_h, bck_h = self.bigru(output)\n",
    "        state_h.append(concatenate([fwd_h, bck_h]))\n",
    "        if len(hiddens)>2:\n",
    "            for hidden in hiddens[1:-1]:\n",
    "                output, de_hidden = self.deepgru(output, hidden)\n",
    "                state_h.append(de_hidden)\n",
    "        if len(hiddens)>1:\n",
    "            output, de_hidden = self.lastdeepgru(output, hidden)\n",
    "            state_h.append(de_hidden)\n",
    "        hidden = state_h\n",
    "#         output = self.softmax(output)\n",
    "        output = Reshape((1,-1))(output)\n",
    "        output = TimeDistributed(Dense(MAX_LENGTH, activation='softmax'))(output)\n",
    "        output = Lambda(lambda x:K.squeeze(x,1))(output)\n",
    "        print(output.shape)\n",
    "        return output, hidden\n",
    "    \n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Model,load_model\n",
    "#############\n",
    "# 写表文件\n",
    "stem_tab_file = '../table/stem.txt'\n",
    "dbn_tab_file = '../table/dbn.txt'\n",
    "flag_tab_file = '../table/flag.txt'\n",
    "#\n",
    "#############\n",
    "# 训练文件和测试文件\n",
    "train_folder = '../data/train/'\n",
    "test_folder = '../data/test/'\n",
    "vaild_folder = '../data/vaild/'\n",
    "\n",
    "dbn_file_in ='dbn.in.txt'\n",
    "dbn_file_out ='dbn.out.txt'\n",
    "\n",
    "flag_file_in ='flag.in.txt'\n",
    "flag_file_out = 'flag.out.txt'\n",
    "#\n",
    "#############\n",
    "# 保存模型\n",
    "model_file ='./save_model/keras_seq2seq_embedding.h5'\n",
    "if not os.path.exists('./save_model'):\n",
    "    os.makedirs('./save_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解决keras 显存问题\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "set_session(tf.Session(config=config)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Base:\n",
    "    def __init__(self,filename,num_word_threshold):\n",
    "        self._bash_to_id = {}\n",
    "        self._num_word_threshold = num_word_threshold\n",
    "        self._read_dict(filename)\n",
    "    \n",
    "    def _read_dict(self,filename):\n",
    "        with open(filename,'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            # print(line)\n",
    "            word,frequency = line.strip('\\r\\n').split('\\t')\n",
    "            frequency = int(frequency)\n",
    "            if frequency < self._num_word_threshold:\n",
    "                continue\n",
    "            idx = len(self._bash_to_id)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            elif word == '<PAD>':\n",
    "                self._pad = idx\n",
    "            elif word == '<GO>':\n",
    "                self._go = idx\n",
    "            elif word == '<EOS>':\n",
    "                self._eos = idx\n",
    "            self._bash_to_id[word] = idx\n",
    "            \n",
    "    @property\n",
    "    def unk(self):\n",
    "         return len(self._bash_to_id)\n",
    "    @property\n",
    "    def pad(self):\n",
    "         return len(self._bash_to_id)+1 \n",
    "    @property\n",
    "    def go(self):\n",
    "         return len(self._bash_to_id)+2\n",
    "    @property\n",
    "    def eos(self):\n",
    "         return len(self._bash_to_id)+3\n",
    "        \n",
    "    def size(self):\n",
    "         return len(self._bash_to_id)\n",
    "    def base_to_id(self,word):\n",
    "        #完成\n",
    "         return self._bash_to_id.get(word,self._unk)\n",
    "    def sequence_to_id(self,sequeuce):\n",
    "        word_ids = [self.base_to_id(cur_word) for cur_word in sequeuce.split()]\n",
    "        return word_ids\n",
    "    \n",
    "    ### \n",
    "    # 不需要id2base 所以没写\n",
    "    ###\n",
    "num_word_threshold = 5000 # 频率低于5000的不要\n",
    "base =Base(stem_tab_file,num_word_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base.sequence_to_id(\"A G C U A C G G C C A U A C A U A G A U G A A A A U A C C G G A U C C C G U C C G A U C U C C G A A G U C A A G C A U C U A A U G G C G A C G U C A G U A C U G U G A U G G G G G A C C G C A C G G G A A U A C G U C G U G C U G U A G U\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "class CategoryDict:\n",
    "    def __init__(self,filename):\n",
    "        self._category_to_id = {}\n",
    "        self._id_to_category = {}\n",
    "        with open(filename , 'r') as f: \n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            category,_ = line.strip('\\r\\n').split('\\t')\n",
    "            idx = len(self._category_to_id)\n",
    "            self._category_to_id[category] = idx\n",
    "            self._id_to_category[idx] = category\n",
    "    def size(self):\n",
    "        return len(self._category_to_id)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def go(self):\n",
    "         return len(self._category_to_id)\n",
    "    @property\n",
    "    def eos(self):\n",
    "         return len(self._category_to_id)+1\n",
    "    @property\n",
    "    def pad(self):\n",
    "         return len(self._category_to_id)+2\n",
    "        \n",
    "    def category_to_id(self,category):\n",
    "        if category not in self._category_to_id:\n",
    "            raise Exception(\n",
    "                    \"%s is not in our category list\" % category)\n",
    "        return self._category_to_id[category]  \n",
    "    def id_to_category(self,id):\n",
    "        if id not in self._id_to_category:\n",
    "            raise Exception(\n",
    "                    \"%s is not in our id list\" % id)\n",
    "        return self._id_to_category[id]  \n",
    "    \n",
    "    def sequence_to_id(self,sequeuce):\n",
    "        word_ids = [self.category_to_id(cur_word) for cur_word in sequeuce.split()]\n",
    "        return word_ids\n",
    "    \n",
    "    def id_to_sequence(self,ids):\n",
    "        category = ' '.join([self.id_to_category(_id) for _id in ids])\n",
    "        return category\n",
    "    \n",
    "category_base = CategoryDict(dbn_tab_file)\n",
    "test_str = '. ( )'\n",
    "test_str2 = [0,1,2,3]\n",
    "print(category_base.sequence_to_id(test_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from %s ../data/train/dbn.in.txt\n",
      "Loading data from %s ../data/train/dbn.out.txt\n",
      "done\n",
      "Loading data from %s ../data/vaild/dbn.in.txt\n",
      "Loading data from %s ../data/vaild/dbn.out.txt\n",
      "done\n",
      "Loading data from %s ../data/test/dbn.in.txt\n",
      "Loading data from %s ../data/test/dbn.out.txt\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# 构建数据库 对其，shuffle，batch\n",
    "\n",
    "num_timesteps = 300\n",
    "\n",
    "class TextDataSet:\n",
    "    def __init__(self, folder, vocab, category_vocab, num_timesteps,):\n",
    "        infile = folder + dbn_file_in\n",
    "        outfile = folder + dbn_file_out\n",
    "        self._vocab = vocab\n",
    "        self._category_vocab = category_vocab\n",
    "        self._num_timesteps = num_timesteps\n",
    "        # matrix 矩阵\n",
    "        self._inputs = []# 所有数据的集合\n",
    "        # vector 列表\n",
    "        self._targets = []#\n",
    "        self._deinputs = []\n",
    "#         self._target_out = []# \n",
    "        self._label_len = []\n",
    "        self._feature_len = []\n",
    "        self._deinput_len = []\n",
    "        self._indicator = 0# batch的位置\n",
    "        self._parse_file(infile,outfile) # 解析⽂件\n",
    "\n",
    "        \n",
    "        \n",
    "    def _parse_file(self,infile,outfile): \n",
    "        \"\"\" 解析文件 \"\"\"\n",
    "        \n",
    "        print('Loading data from %s',infile)\n",
    "        print('Loading data from %s',outfile)\n",
    "        with open(infile,'r') as fi:\n",
    "            inlines = fi.readlines()\n",
    "        with open(outfile,'r')as fo:\n",
    "            outlines = fo.readlines()\n",
    "            \n",
    "        for line in inlines:\n",
    "            line = line.strip('\\r\\n').strip()\n",
    "            id_feature = self._vocab.sequence_to_id(line)\n",
    "            id_feature = id_feature[0:self._num_timesteps] #保证不会过长\n",
    "            feature_len = len(id_feature)\n",
    "            self._feature_len.append(feature_len)\n",
    "            padding_num = self._num_timesteps - feature_len # 如果id_words⼩于这个num_timesteps\n",
    "            id_feature = [self._vocab.go]+id_feature + list(self._vocab.pad for i in range(padding_num-1))\n",
    "            self._inputs.append(id_feature)\n",
    "            \n",
    "        for line in outlines:\n",
    "            line = line.strip('\\r\\n')\n",
    "            id_label = self._category_vocab.sequence_to_id(line)\n",
    "            id_label = id_label[0:self._num_timesteps]#600\n",
    "            label_len = len(id_label)\n",
    "            self._deinput_len.append(label_len) \n",
    "            padding_num = self._num_timesteps - len(id_label) # 如果id_words⼩于这个num_timesteps\n",
    "            id_label = [self._category_vocab.go]+id_label + list(self._category_vocab.pad for i in range(padding_num-1))\n",
    "            self._deinputs.append(id_label)\n",
    "            \n",
    "        for line in outlines:\n",
    "            line = line.strip('\\r\\n')\n",
    "            id_label = self._category_vocab.sequence_to_id(line)\n",
    "            id_label = id_label[0:self._num_timesteps]#600\n",
    "            label_len = len(id_label)\n",
    "            self._label_len.append(label_len)\n",
    "            padding_num = self._num_timesteps - len(id_label) # 如果id_words⼩于这个num_timesteps\n",
    "            id_label = id_label + [self._category_vocab.eos]+ list(self._category_vocab.pad for i in range(padding_num-1))\n",
    "            self._targets.append(id_label)        \n",
    "        \n",
    "            \n",
    "            \n",
    "        self._inputs = np.asarray(self._inputs, dtype = np.int32)\n",
    "        self._target = np.asarray(self._targets, dtype = np.int32)\n",
    "        self._deinputs = np.asarray(self._deinputs, dtype = np.int32)\n",
    "#         self._target_out = np.asarray(self._target_out, dtype = np.int32)\n",
    "        self._label_len = np.asarray(self._label_len, dtype = np.int32)\n",
    "        self._feature_len = np.asarray(self._feature_len, dtype = np.int32)\n",
    "        self._random_shuffle()\n",
    "\n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(len(self._inputs))\n",
    "        self._inputs = self._inputs[p]\n",
    "        self._target = self._target[p]\n",
    "        self._label_len = self._label_len[p]\n",
    "        self._feature_len = self._feature_len[p]\n",
    "\n",
    "    def next_batch(self,batch_size):\n",
    "        end_indicator = self._indicator +batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = batch_size\n",
    "        if end_indicator > len(self._inputs):# 则说明batchsize ⽐样本空间还要⼤\n",
    "            raise Exception(\"batch_size:%d is too large\"% batch_size)\n",
    "        \n",
    "        batch_inputs = self._inputs[self._indicator:end_indicator]\n",
    "        batch_target = self._target[self._indicator:end_indicator]\n",
    "        batch_deinput = self._deinputs[self._indicator:end_indicator]\n",
    "        inputs_len = self._feature_len[self._indicator:end_indicator]\n",
    "        outputs_len =  self._label_len[self._indicator:end_indicator]\n",
    "        deinputs_len = self._deinput_len[self._indicator:end_indicator]\n",
    "        \n",
    "        self._indicator = end_indicator\n",
    "        return (batch_inputs,inputs_len),(batch_target,outputs_len),(batch_deinput,deinputs_len)\n",
    "train_dataset = TextDataSet(\n",
    "    train_folder, base, category_base, num_timesteps)\n",
    "print('done')\n",
    "val_dataset = TextDataSet(\n",
    "    vaild_folder, base, category_base, num_timesteps)\n",
    "print('done')\n",
    "test_dataset = TextDataSet(\n",
    "    test_folder, base, category_base, num_timesteps)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((array([[8, 4, 3, 3, 3, 4, 2, 5, 3, 2, 2, 4, 3, 5, 4, 4, 5, 3, 5, 4, 4, 2,\n",
      "        5, 4, 2, 2, 2, 3, 4, 4, 4, 5, 2, 3, 4, 4, 4, 5, 3, 4, 5, 3, 5, 5,\n",
      "        3, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 2, 5, 2, 2, 5, 5, 4, 4, 4, 3,\n",
      "        5, 2, 4, 2, 5, 3, 2, 5, 3, 5, 2, 2, 4, 2, 3, 5, 2, 4, 3, 5, 4, 4,\n",
      "        5, 3, 3, 3, 3, 2, 4, 3, 5, 5, 3, 3, 3, 3, 5, 5, 2, 3, 4, 4, 3, 3,\n",
      "        4, 4, 2, 4, 3, 5, 5, 4, 4, 4, 4, 5, 3, 5, 5, 5, 4, 4, 3, 2, 5, 3,\n",
      "        3, 2, 3, 4, 4, 2, 4, 5, 4, 4, 2, 3, 3, 4, 4, 2, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
      "       [8, 3, 3, 4, 4, 3, 3, 3, 3, 3, 2, 3, 2, 3, 5, 5, 2, 3, 3, 4, 3, 5,\n",
      "        2, 3, 4, 4, 5, 5, 4, 3, 4, 3, 4, 4, 5, 2, 2, 4, 5, 4, 3, 2, 2, 2,\n",
      "        3, 3, 3, 4, 2, 2, 5, 4, 5, 4, 4, 2, 2, 2, 3, 2, 3, 2, 3, 2, 5, 5,\n",
      "        2, 4, 2, 5, 2, 5, 4, 3, 3, 3, 3, 2, 4, 4, 2, 2, 5, 3, 4, 4, 3, 3,\n",
      "        5, 2, 4, 3, 2, 2, 5, 4, 4, 2, 4, 4, 5, 3, 2, 5, 5, 3, 4, 3, 5, 4,\n",
      "        5, 4, 4, 4, 3, 3, 2, 3, 2, 4, 3, 2, 4, 5, 2, 3, 2, 2, 4, 5, 3, 4,\n",
      "        2, 4, 2, 4, 4, 4, 5, 2, 5, 5, 3, 3, 5, 4, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]], dtype=int32), array([147, 145], dtype=int32)), (array([[1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 3, 3, 3, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 3, 3,\n",
      "        3, 3, 3, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 2, 2, 3, 3, 1, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 2, 2, 2, 2, 1, 1, 3, 1, 1, 1, 1, 5, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1,\n",
      "        1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1,\n",
      "        1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]], dtype=int32), array([147, 145], dtype=int32)), (array([[4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 3, 3, 1, 3, 3, 3, 3, 3,\n",
      "        1, 1, 1, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 2, 2, 1,\n",
      "        1, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2,\n",
      "        2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 2, 1, 3, 3, 1, 1, 1, 1, 1, 3, 3, 1,\n",
      "        1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 3, 1, 1, 3, 3, 1, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
      "       [4, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,\n",
      "        2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2,\n",
      "        1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 1, 1, 1, 3,\n",
      "        3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 1, 3, 3,\n",
      "        3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 1,\n",
      "        1, 1, 1, 3, 3, 1, 1, 3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]], dtype=int32), [117, 164]))\n"
     ]
    }
   ],
   "source": [
    "a = train_dataset.next_batch(2)\n",
    "print(a)\n",
    "# print(val_dataset.next_batch(2)) \n",
    "# print(test_dataset.next_batch(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写数据\n",
    "def write_log(callback, names, logs, batch_no):\n",
    "\n",
    "    for (name, value) in zip(names, logs):\n",
    "        summary = tf.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.simple_value = value\n",
    "        summary_value.tag = name\n",
    "        callback.writer.add_summary(summary, batch_no)\n",
    "        callback.writer.flush()\n",
    "        \n",
    "def write_log_batch(callback, name, log, batch_no):\n",
    "    value = log\n",
    "    summary = tf.Summary()\n",
    "    summary_value = summary.value.add()\n",
    "    summary_value.simple_value = value\n",
    "    summary_value.tag = name\n",
    "    callback.writer.add_summary(summary, batch_no)\n",
    "    callback.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_length = 10\n",
    "# input_dim = 1\n",
    "# MAX_LENGTH = 300 # 单个字符的长度\n",
    "# output_length = 10\n",
    "# output_dim = 4\n",
    "\n",
    "# samples = 100\n",
    "# hidden_dim = 3\n",
    "# hidden_size = 150 # must be a half of Max_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1003 22:28:23.994187 140630717482752 deprecation_wrapper.py:119] From /data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1003 22:28:24.004379 140630717482752 deprecation_wrapper.py:119] From /data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1003 22:28:24.008403 140630717482752 deprecation_wrapper.py:119] From /data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1003 22:28:25.888438 140630717482752 deprecation_wrapper.py:119] From /data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1003 22:28:25.894220 140630717482752 deprecation.py:506] From /data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 300, 64)\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "(?, 1, 600)\n",
      "(?, 300)\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# model\n",
    "\n",
    "encoder = EncoderRNN(embedding_size,hidden_size)\n",
    "decoder = DecoderRNN(embedding_size,hidden_size)\n",
    "atten = AttnDecoderRNN(embedding_size,hidden_size)# hidden_size = 6\n",
    "\n",
    "en_inputs = Input(shape=(300,),dtype=float)\n",
    "de_inputs = Input(shape=(300,),dtype=float)\n",
    "en_output, en_hidden = encoder(en_inputs)\n",
    "attent_output,attn_hidden = atten(de_inputs,en_hidden,en_output)\n",
    "\n",
    "#\n",
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 300, 64)      64000       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, 300, 600), ( 657000      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     [(None, 300, 600), ( 2161800     bidirectional_1[0][0]            \n",
      "                                                                 gru_2[0][0]                      \n",
      "                                                                 gru_2[1][0]                      \n",
      "                                                                 gru_2[2][0]                      \n",
      "                                                                 gru_2[3][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "atten_embed (Embedding)         (None, 300, 64)      64000       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 19200)     0           atten_embed[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 19200)        0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 600)       0           gru_2[4][1]                      \n",
      "__________________________________________________________________________________________________\n",
      "atten_dropout (Dropout)         (None, 19200)        0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 600)          0           reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "atten_concat2 (Concatenate)     (None, 19800)        0           atten_dropout[0][0]              \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 300)          5940300     atten_concat2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "softmax_1 (Softmax)             (None, 300)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1, 300)       0           softmax_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 600)       0           reshape_3[0][0]                  \n",
      "                                                                 gru_2[4][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 600)          0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 19800)        0           atten_dropout[0][0]              \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "atten_combine (Dense)           (None, 300)          5940300     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 300)          0           atten_combine[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 1, 300)       0           re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "atten_bigru (Bidirectional)     [(None, 1, 600), (No 1081800     reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "atten_deepgru (GRU)             [(None, 1, 600), (No 2161800     atten_bigru[0][0]                \n",
      "                                                                 gru_2[0][1]                      \n",
      "                                                                 atten_deepgru[0][0]              \n",
      "                                                                 gru_2[1][1]                      \n",
      "                                                                 atten_deepgru[1][0]              \n",
      "                                                                 gru_2[2][1]                      \n",
      "                                                                 atten_deepgru[2][0]              \n",
      "                                                                 gru_2[3][1]                      \n",
      "__________________________________________________________________________________________________\n",
      "atten_lastdeepgru (GRU)         [(None, 600), (None, 2161800     atten_deepgru[3][0]              \n",
      "                                                                 gru_2[3][1]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 1, 600)       0           atten_lastdeepgru[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 1, 300)       180300      reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 300)          0           time_distributed_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 20,413,100\n",
      "Trainable params: 20,413,100\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "[train]:step:0,loss:6644.711914,acc:0.000000 \n",
      "[vaild]:step:0,loss:6730.159668,acc:0.000000 \n",
      "[train]:step:1,loss:6752.369141,acc:0.000000 \n",
      "[train]:step:2,loss:6811.607910,acc:0.000000 \n",
      "[train]:step:3,loss:6773.845215,acc:0.000000 \n",
      "[train]:step:4,loss:6812.823730,acc:0.000000 \n",
      "[train]:step:5,loss:6706.283203,acc:0.000000 \n",
      "[train]:step:6,loss:6525.267578,acc:0.000000 \n",
      "[train]:step:7,loss:6675.481445,acc:0.000000 \n",
      "[train]:step:8,loss:6679.547363,acc:0.000000 \n",
      "[train]:step:9,loss:6802.491211,acc:0.000000 \n",
      "[train]:step:10,loss:6605.116699,acc:0.000000 \n",
      "[vaild]:step:10,loss:6812.042969,acc:0.000000 \n",
      "[train]:step:11,loss:6723.788574,acc:0.000000 \n",
      "[train]:step:12,loss:6831.260742,acc:0.000000 \n",
      "[train]:step:13,loss:6779.273926,acc:0.000000 \n",
      "[train]:step:14,loss:6794.666016,acc:0.000000 \n",
      "[train]:step:15,loss:6837.738281,acc:0.000000 \n",
      "[train]:step:16,loss:6634.720703,acc:0.000000 \n",
      "[train]:step:17,loss:6805.861328,acc:0.000000 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train]:step:18,loss:6831.465820,acc:0.000000 \n",
      "[train]:step:19,loss:6728.345215,acc:0.000000 \n",
      "[train]:step:20,loss:6755.117676,acc:0.000000 \n",
      "[vaild]:step:20,loss:6664.409180,acc:0.000000 \n",
      "[train]:step:21,loss:6692.052734,acc:0.000000 \n",
      "[train]:step:22,loss:6509.510742,acc:0.000000 \n",
      "[train]:step:23,loss:6717.941895,acc:0.000000 \n",
      "[train]:step:24,loss:6754.718750,acc:0.000000 \n",
      "[train]:step:25,loss:6684.495117,acc:0.000000 \n",
      "[train]:step:26,loss:6770.722656,acc:0.000000 \n",
      "[train]:step:27,loss:6787.746094,acc:0.000000 \n",
      "[train]:step:28,loss:6685.555664,acc:0.000000 \n",
      "[train]:step:29,loss:6585.392090,acc:0.000000 \n",
      "[train]:step:30,loss:6678.966309,acc:0.000000 \n",
      "[vaild]:step:30,loss:6871.312500,acc:0.000000 \n",
      "[train]:step:31,loss:6750.029785,acc:0.000000 \n",
      "[train]:step:32,loss:6762.289551,acc:0.000000 \n",
      "[train]:step:33,loss:6762.516113,acc:0.000000 \n",
      "[train]:step:34,loss:6661.260742,acc:0.000000 \n",
      "[train]:step:35,loss:6746.633789,acc:0.000000 \n",
      "[train]:step:36,loss:6698.163574,acc:0.000000 \n",
      "[train]:step:37,loss:6716.736328,acc:0.000000 \n",
      "[train]:step:38,loss:6945.772949,acc:0.000000 \n",
      "[train]:step:39,loss:6762.794922,acc:0.000000 \n",
      "[train]:step:40,loss:6813.931152,acc:0.000000 \n",
      "[vaild]:step:40,loss:6848.504883,acc:0.000000 \n",
      "[train]:step:41,loss:6787.984375,acc:0.000000 \n",
      "[train]:step:42,loss:6675.209473,acc:0.000000 \n",
      "[train]:step:43,loss:6828.285645,acc:0.000000 \n",
      "[train]:step:44,loss:6519.962402,acc:0.000000 \n",
      "[train]:step:45,loss:6649.681641,acc:0.000000 \n",
      "[train]:step:46,loss:6688.078125,acc:0.000000 \n",
      "[train]:step:47,loss:6765.654297,acc:0.000000 \n",
      "[train]:step:48,loss:6604.797363,acc:0.000000 \n",
      "[train]:step:49,loss:6710.075684,acc:0.000000 \n",
      "[train]:step:50,loss:6782.022949,acc:0.000000 \n",
      "[vaild]:step:50,loss:6653.584961,acc:0.000000 \n",
      "[train]:step:51,loss:6719.651367,acc:0.000000 \n",
      "[train]:step:52,loss:6595.060059,acc:0.000000 \n",
      "[train]:step:53,loss:6585.326660,acc:0.000000 \n",
      "[train]:step:54,loss:6687.190430,acc:0.000000 \n",
      "[train]:step:55,loss:6486.717285,acc:0.000000 \n",
      "[train]:step:56,loss:6824.147461,acc:0.000000 \n",
      "[train]:step:57,loss:6795.140137,acc:0.000000 \n",
      "[train]:step:58,loss:6820.223145,acc:0.000000 \n",
      "[train]:step:59,loss:6841.546387,acc:0.000000 \n",
      "[train]:step:60,loss:6882.786133,acc:0.000000 \n",
      "[vaild]:step:60,loss:6151.517578,acc:0.000000 \n",
      "[train]:step:61,loss:6690.249512,acc:0.000000 \n",
      "[train]:step:62,loss:6611.862305,acc:0.000000 \n",
      "[train]:step:63,loss:6765.784180,acc:0.000000 \n",
      "[train]:step:64,loss:6736.345703,acc:0.000000 \n",
      "[train]:step:65,loss:6895.171875,acc:0.000000 \n",
      "[train]:step:66,loss:6766.245117,acc:0.000000 \n",
      "[train]:step:67,loss:6668.282715,acc:0.000000 \n",
      "[train]:step:68,loss:6673.328613,acc:0.000000 \n",
      "[train]:step:69,loss:6758.381348,acc:0.000000 \n",
      "[train]:step:70,loss:6631.424316,acc:0.000000 \n",
      "[vaild]:step:70,loss:6916.311523,acc:0.000000 \n",
      "[train]:step:71,loss:6703.818359,acc:0.000000 \n",
      "[train]:step:72,loss:6709.817383,acc:0.000000 \n",
      "[train]:step:73,loss:6804.313965,acc:0.000000 \n",
      "[train]:step:74,loss:6746.208008,acc:0.000000 \n",
      "[train]:step:75,loss:6672.298828,acc:0.000000 \n",
      "[train]:step:76,loss:6821.473633,acc:0.000000 \n",
      "[train]:step:77,loss:6638.412109,acc:0.000000 \n",
      "[train]:step:78,loss:6721.336426,acc:0.000000 \n",
      "[train]:step:79,loss:6837.641113,acc:0.000000 \n",
      "[train]:step:80,loss:6478.451172,acc:0.000000 \n",
      "[vaild]:step:80,loss:6662.621094,acc:0.000000 \n",
      "[train]:step:81,loss:6587.923828,acc:0.000000 \n",
      "[train]:step:82,loss:6670.694824,acc:0.000000 \n",
      "[train]:step:83,loss:6635.017090,acc:0.000000 \n",
      "[train]:step:84,loss:6756.603516,acc:0.000000 \n",
      "[train]:step:85,loss:6752.276855,acc:0.000000 \n",
      "[train]:step:86,loss:6740.505859,acc:0.000000 \n",
      "[train]:step:87,loss:6795.465820,acc:0.000000 \n",
      "[train]:step:88,loss:6597.696289,acc:0.000000 \n",
      "[train]:step:89,loss:6672.647949,acc:0.000000 \n",
      "[train]:step:90,loss:6745.970215,acc:0.010000 \n",
      "[vaild]:step:90,loss:6413.587891,acc:0.000000 \n",
      "[train]:step:91,loss:6736.938965,acc:0.000000 \n",
      "[train]:step:92,loss:6721.084473,acc:0.000000 \n",
      "[train]:step:93,loss:6629.282715,acc:0.000000 \n",
      "[train]:step:94,loss:6683.345703,acc:0.000000 \n",
      "[train]:step:95,loss:6767.104492,acc:0.000000 \n",
      "[train]:step:96,loss:6747.077637,acc:0.000000 \n",
      "[train]:step:97,loss:6766.726074,acc:0.000000 \n",
      "[train]:step:98,loss:6715.998535,acc:0.000000 \n",
      "[train]:step:99,loss:6802.929199,acc:0.000000 \n",
      "[train]:step:100,loss:6778.346680,acc:0.000000 \n",
      "[vaild]:step:100,loss:6757.951172,acc:0.000000 \n",
      "[train]:step:101,loss:6880.109863,acc:0.000000 \n",
      "[train]:step:102,loss:6714.301758,acc:0.000000 \n",
      "[train]:step:103,loss:6778.073730,acc:0.000000 \n",
      "[train]:step:104,loss:6846.524414,acc:0.000000 \n",
      "[train]:step:105,loss:6652.784180,acc:0.000000 \n",
      "[train]:step:106,loss:6678.602539,acc:0.000000 \n",
      "[train]:step:107,loss:6860.446777,acc:0.000000 \n",
      "[train]:step:108,loss:6774.714355,acc:0.000000 \n",
      "[train]:step:109,loss:6699.599609,acc:0.000000 \n",
      "[train]:step:110,loss:6719.526367,acc:0.000000 \n",
      "[vaild]:step:110,loss:6920.947266,acc:0.000000 \n",
      "[train]:step:111,loss:6636.965820,acc:0.000000 \n",
      "[train]:step:112,loss:6829.165039,acc:0.000000 \n",
      "[train]:step:113,loss:6762.604492,acc:0.000000 \n",
      "[train]:step:114,loss:6642.785156,acc:0.000000 \n",
      "[train]:step:115,loss:6740.034180,acc:0.000000 \n",
      "[train]:step:116,loss:6743.560547,acc:0.000000 \n",
      "[train]:step:117,loss:6746.282715,acc:0.000000 \n",
      "[train]:step:118,loss:6775.028809,acc:0.000000 \n",
      "[train]:step:119,loss:6788.659180,acc:0.000000 \n",
      "[train]:step:120,loss:6657.056641,acc:0.000000 \n",
      "[vaild]:step:120,loss:6886.528320,acc:0.000000 \n",
      "[train]:step:121,loss:6856.538574,acc:0.000000 \n",
      "[train]:step:122,loss:6867.153320,acc:0.000000 \n",
      "[train]:step:123,loss:6584.680176,acc:0.000000 \n",
      "[train]:step:124,loss:6638.641113,acc:0.000000 \n",
      "[train]:step:125,loss:6697.729492,acc:0.000000 \n",
      "[train]:step:126,loss:6722.692383,acc:0.000000 \n",
      "[train]:step:127,loss:6801.310547,acc:0.000000 \n",
      "[train]:step:128,loss:6619.917480,acc:0.000000 \n",
      "[train]:step:129,loss:6611.310547,acc:0.000000 \n",
      "[train]:step:130,loss:6752.318848,acc:0.000000 \n",
      "[vaild]:step:130,loss:6698.783203,acc:0.000000 \n",
      "[train]:step:131,loss:6694.072266,acc:0.000000 \n",
      "[train]:step:132,loss:6630.547363,acc:0.000000 \n",
      "[train]:step:133,loss:6749.210449,acc:0.000000 \n",
      "[train]:step:134,loss:6669.037598,acc:0.000000 \n",
      "[train]:step:135,loss:6705.227051,acc:0.000000 \n",
      "[train]:step:136,loss:6764.360840,acc:0.000000 \n",
      "[train]:step:137,loss:6717.343750,acc:0.000000 \n",
      "[train]:step:138,loss:6842.839355,acc:0.000000 \n",
      "[train]:step:139,loss:6718.910645,acc:0.000000 \n",
      "[train]:step:140,loss:6775.075195,acc:0.000000 \n",
      "[vaild]:step:140,loss:6535.944336,acc:0.000000 \n",
      "[train]:step:141,loss:6666.857422,acc:0.000000 \n",
      "[train]:step:142,loss:6564.886230,acc:0.000000 \n",
      "[train]:step:143,loss:6607.762695,acc:0.000000 \n",
      "[train]:step:144,loss:6533.206055,acc:0.000000 \n",
      "[train]:step:145,loss:6718.260742,acc:0.000000 \n",
      "[train]:step:146,loss:6725.345215,acc:0.000000 \n",
      "[train]:step:147,loss:6714.583008,acc:0.000000 \n",
      "[train]:step:148,loss:6770.083984,acc:0.000000 \n",
      "[train]:step:149,loss:6690.322266,acc:0.000000 \n",
      "[train]:step:150,loss:6756.913086,acc:0.000000 \n",
      "[vaild]:step:150,loss:6527.962891,acc:0.000000 \n",
      "[train]:step:151,loss:6835.191406,acc:0.000000 \n",
      "[train]:step:152,loss:6689.111328,acc:0.000000 \n",
      "[train]:step:153,loss:6735.586914,acc:0.000000 \n",
      "[train]:step:154,loss:6683.665527,acc:0.000000 \n",
      "[train]:step:155,loss:6646.873535,acc:0.000000 \n",
      "[train]:step:156,loss:6811.701172,acc:0.000000 \n",
      "[train]:step:157,loss:6787.491211,acc:0.000000 \n",
      "[train]:step:158,loss:6643.247070,acc:0.000000 \n",
      "[train]:step:159,loss:6825.712402,acc:0.000000 \n",
      "[train]:step:160,loss:6767.251465,acc:0.000000 \n",
      "[vaild]:step:160,loss:6797.097656,acc:0.000000 \n",
      "[train]:step:161,loss:6813.287109,acc:0.000000 \n",
      "[train]:step:162,loss:6668.272949,acc:0.000000 \n",
      "[train]:step:163,loss:6672.841309,acc:0.000000 \n",
      "[train]:step:164,loss:6614.728516,acc:0.000000 \n",
      "[train]:step:165,loss:6745.575684,acc:0.000000 \n",
      "[train]:step:166,loss:6729.360840,acc:0.000000 \n",
      "[train]:step:167,loss:6758.839844,acc:0.000000 \n",
      "[train]:step:168,loss:6768.242676,acc:0.000000 \n",
      "[train]:step:169,loss:6642.490723,acc:0.000000 \n",
      "[train]:step:170,loss:6578.060059,acc:0.000000 \n",
      "[vaild]:step:170,loss:6604.436523,acc:0.000000 \n",
      "[train]:step:171,loss:6690.901855,acc:0.000000 \n",
      "[train]:step:172,loss:6705.188965,acc:0.000000 \n",
      "[train]:step:173,loss:6716.763672,acc:0.000000 \n",
      "[train]:step:174,loss:6795.878906,acc:0.000000 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train]:step:175,loss:6809.230469,acc:0.000000 \n",
      "[train]:step:176,loss:6805.162598,acc:0.000000 \n",
      "[train]:step:177,loss:6690.841309,acc:0.000000 \n",
      "[train]:step:178,loss:6620.423828,acc:0.000000 \n",
      "[train]:step:179,loss:6614.733887,acc:0.000000 \n",
      "[train]:step:180,loss:6609.037109,acc:0.000000 \n",
      "[vaild]:step:180,loss:6466.924316,acc:0.000000 \n",
      "[train]:step:181,loss:6620.477051,acc:0.000000 \n",
      "[train]:step:182,loss:6704.038574,acc:0.000000 \n",
      "[train]:step:183,loss:6798.423340,acc:0.000000 \n",
      "[train]:step:184,loss:6688.832520,acc:0.000000 \n",
      "[train]:step:185,loss:6732.598145,acc:0.000000 \n",
      "[train]:step:186,loss:6696.465820,acc:0.000000 \n",
      "[train]:step:187,loss:6877.761230,acc:0.000000 \n",
      "[train]:step:188,loss:6675.873535,acc:0.000000 \n",
      "[train]:step:189,loss:6735.023926,acc:0.000000 \n",
      "[train]:step:190,loss:6748.401855,acc:0.000000 \n",
      "[vaild]:step:190,loss:7114.794434,acc:0.000000 \n",
      "[train]:step:191,loss:6658.560547,acc:0.000000 \n",
      "[train]:step:192,loss:6767.740723,acc:0.000000 \n",
      "[train]:step:193,loss:6588.456055,acc:0.000000 \n",
      "[train]:step:194,loss:6839.093262,acc:0.000000 \n",
      "[train]:step:195,loss:6852.741211,acc:0.000000 \n",
      "[train]:step:196,loss:6826.375000,acc:0.000000 \n",
      "[train]:step:197,loss:6687.486328,acc:0.000000 \n",
      "[train]:step:198,loss:6871.875488,acc:0.000000 \n",
      "[train]:step:199,loss:6735.364258,acc:0.000000 \n",
      "[train]:step:200,loss:6711.410156,acc:0.000000 \n",
      "[vaild]:step:200,loss:6555.926758,acc:0.000000 \n",
      "[train]:step:201,loss:6858.279297,acc:0.000000 \n",
      "[train]:step:202,loss:6843.691406,acc:0.000000 \n",
      "[train]:step:203,loss:6722.666016,acc:0.000000 \n",
      "[train]:step:204,loss:6807.075684,acc:0.000000 \n",
      "[train]:step:205,loss:6691.124512,acc:0.000000 \n",
      "[train]:step:206,loss:6745.969238,acc:0.000000 \n",
      "[train]:step:207,loss:6822.982910,acc:0.000000 \n",
      "[train]:step:208,loss:6634.700195,acc:0.000000 \n",
      "[train]:step:209,loss:6667.550781,acc:0.000000 \n",
      "[train]:step:210,loss:6805.051758,acc:0.000000 \n",
      "[vaild]:step:210,loss:6540.629883,acc:0.000000 \n",
      "[train]:step:211,loss:6695.267578,acc:0.000000 \n",
      "[train]:step:212,loss:6823.087402,acc:0.000000 \n",
      "[train]:step:213,loss:6657.865234,acc:0.000000 \n",
      "[train]:step:214,loss:6688.853516,acc:0.000000 \n",
      "[train]:step:215,loss:6635.457520,acc:0.000000 \n",
      "[train]:step:216,loss:6779.669434,acc:0.000000 \n",
      "[train]:step:217,loss:6773.175781,acc:0.000000 \n",
      "[train]:step:218,loss:6582.075195,acc:0.000000 \n",
      "[train]:step:219,loss:6742.066895,acc:0.000000 \n",
      "[train]:step:220,loss:6671.534180,acc:0.000000 \n",
      "[vaild]:step:220,loss:6822.633789,acc:0.000000 \n",
      "[train]:step:221,loss:6770.133789,acc:0.000000 \n",
      "[train]:step:222,loss:6797.570801,acc:0.000000 \n",
      "[train]:step:223,loss:6643.290527,acc:0.000000 \n",
      "[train]:step:224,loss:6606.986328,acc:0.000000 \n",
      "[train]:step:225,loss:6616.940430,acc:0.000000 \n",
      "[train]:step:226,loss:6808.187500,acc:0.000000 \n",
      "[train]:step:227,loss:6614.319824,acc:0.000000 \n",
      "[train]:step:228,loss:6618.812012,acc:0.000000 \n",
      "[train]:step:229,loss:6679.923340,acc:0.000000 \n",
      "[train]:step:230,loss:6710.843262,acc:0.000000 \n",
      "[vaild]:step:230,loss:6633.712891,acc:0.000000 \n",
      "[train]:step:231,loss:6561.546875,acc:0.000000 \n",
      "[train]:step:232,loss:6485.813965,acc:0.000000 \n",
      "[train]:step:233,loss:6737.683594,acc:0.000000 \n",
      "[train]:step:234,loss:6776.965820,acc:0.000000 \n",
      "[train]:step:235,loss:6604.705566,acc:0.000000 \n",
      "[train]:step:236,loss:6770.429199,acc:0.000000 \n",
      "[train]:step:237,loss:6793.639160,acc:0.000000 \n",
      "[train]:step:238,loss:6660.363281,acc:0.010000 \n",
      "[train]:step:239,loss:6734.393555,acc:0.000000 \n",
      "[train]:step:240,loss:6626.727539,acc:0.000000 \n",
      "[vaild]:step:240,loss:6722.848145,acc:0.000000 \n",
      "[train]:step:241,loss:6888.184570,acc:0.000000 \n",
      "[train]:step:242,loss:6740.693848,acc:0.000000 \n",
      "[train]:step:243,loss:6542.398926,acc:0.000000 \n",
      "[train]:step:244,loss:6726.857422,acc:0.000000 \n",
      "[train]:step:245,loss:6734.305664,acc:0.000000 \n",
      "[train]:step:246,loss:6676.736816,acc:0.000000 \n",
      "[train]:step:247,loss:6691.716797,acc:0.000000 \n",
      "[train]:step:248,loss:6667.798340,acc:0.000000 \n",
      "[train]:step:249,loss:6723.515137,acc:0.000000 \n",
      "[train]:step:250,loss:6743.332031,acc:0.000000 \n",
      "[vaild]:step:250,loss:6982.579102,acc:0.000000 \n",
      "[train]:step:251,loss:6734.823730,acc:0.010000 \n",
      "[train]:step:252,loss:6576.832031,acc:0.010000 \n",
      "[train]:step:253,loss:6553.844238,acc:0.000000 \n",
      "[train]:step:254,loss:6550.908203,acc:0.000000 \n",
      "[train]:step:255,loss:6650.481934,acc:0.000000 \n",
      "[train]:step:256,loss:6594.855469,acc:0.000000 \n",
      "[train]:step:257,loss:6776.430176,acc:0.000000 \n",
      "[train]:step:258,loss:6608.705566,acc:0.000000 \n",
      "[train]:step:259,loss:6643.811035,acc:0.000000 \n",
      "[train]:step:260,loss:6748.887695,acc:0.000000 \n",
      "[vaild]:step:260,loss:6518.602051,acc:0.000000 \n",
      "[train]:step:261,loss:6756.410156,acc:0.000000 \n",
      "[train]:step:262,loss:6784.538086,acc:0.000000 \n",
      "[train]:step:263,loss:6885.665527,acc:0.000000 \n",
      "[train]:step:264,loss:6946.105469,acc:0.000000 \n",
      "[train]:step:265,loss:6684.795410,acc:0.000000 \n",
      "[train]:step:266,loss:6598.193359,acc:0.000000 \n",
      "[train]:step:267,loss:6800.299805,acc:0.000000 \n",
      "[train]:step:268,loss:6606.857910,acc:0.000000 \n",
      "[train]:step:269,loss:6630.480469,acc:0.000000 \n",
      "[train]:step:270,loss:6544.453613,acc:0.000000 \n",
      "[vaild]:step:270,loss:6660.254395,acc:0.000000 \n",
      "[train]:step:271,loss:6776.728516,acc:0.000000 \n",
      "[train]:step:272,loss:6665.859863,acc:0.000000 \n",
      "[train]:step:273,loss:6627.062012,acc:0.000000 \n",
      "[train]:step:274,loss:6816.271484,acc:0.010000 \n",
      "[train]:step:275,loss:6599.204590,acc:0.000000 \n",
      "[train]:step:276,loss:6673.268066,acc:0.000000 \n",
      "[train]:step:277,loss:6937.784180,acc:0.000000 \n",
      "[train]:step:278,loss:6792.131836,acc:0.000000 \n",
      "[train]:step:279,loss:6731.279297,acc:0.000000 \n",
      "[train]:step:280,loss:6712.050781,acc:0.000000 \n",
      "[vaild]:step:280,loss:6575.727539,acc:0.000000 \n",
      "[train]:step:281,loss:6653.297363,acc:0.000000 \n",
      "[train]:step:282,loss:6720.038086,acc:0.000000 \n",
      "[train]:step:283,loss:6592.776855,acc:0.000000 \n",
      "[train]:step:284,loss:6796.669922,acc:0.010000 \n",
      "[train]:step:285,loss:6745.915039,acc:0.000000 \n",
      "[train]:step:286,loss:6691.474609,acc:0.000000 \n",
      "[train]:step:287,loss:6771.984375,acc:0.000000 \n",
      "[train]:step:288,loss:6673.930664,acc:0.000000 \n",
      "[train]:step:289,loss:6707.841797,acc:0.000000 \n",
      "[train]:step:290,loss:6778.193359,acc:0.000000 \n",
      "[vaild]:step:290,loss:6957.830078,acc:0.000000 \n",
      "[train]:step:291,loss:6767.943848,acc:0.000000 \n",
      "[train]:step:292,loss:6759.390625,acc:0.000000 \n",
      "[train]:step:293,loss:6811.353027,acc:0.000000 \n",
      "[train]:step:294,loss:6699.997559,acc:0.000000 \n",
      "[train]:step:295,loss:6778.082031,acc:0.000000 \n",
      "[train]:step:296,loss:6786.104980,acc:0.000000 \n",
      "[train]:step:297,loss:6812.209473,acc:0.000000 \n",
      "[train]:step:298,loss:6562.444824,acc:0.000000 \n",
      "[train]:step:299,loss:6861.267090,acc:0.000000 \n",
      "[train]:step:300,loss:6784.444336,acc:0.000000 \n",
      "[vaild]:step:300,loss:7049.516113,acc:0.000000 \n",
      "[train]:step:301,loss:6654.850586,acc:0.000000 \n",
      "[train]:step:302,loss:6703.142090,acc:0.000000 \n",
      "[train]:step:303,loss:6670.053223,acc:0.000000 \n",
      "[train]:step:304,loss:6655.132324,acc:0.000000 \n",
      "[train]:step:305,loss:6757.171387,acc:0.000000 \n",
      "[train]:step:306,loss:6630.912598,acc:0.000000 \n",
      "[train]:step:307,loss:6799.805664,acc:0.000000 \n",
      "[train]:step:308,loss:6595.404785,acc:0.000000 \n",
      "[train]:step:309,loss:6560.685547,acc:0.000000 \n",
      "[train]:step:310,loss:6808.101074,acc:0.000000 \n",
      "[vaild]:step:310,loss:6775.034180,acc:0.000000 \n",
      "[train]:step:311,loss:6813.491211,acc:0.000000 \n",
      "[train]:step:312,loss:6613.877441,acc:0.000000 \n",
      "[train]:step:313,loss:6889.902344,acc:0.000000 \n",
      "[train]:step:314,loss:6755.583984,acc:0.000000 \n",
      "[train]:step:315,loss:6645.734375,acc:0.000000 \n",
      "[train]:step:316,loss:6782.058105,acc:0.000000 \n",
      "[train]:step:317,loss:6512.264160,acc:0.000000 \n",
      "[train]:step:318,loss:6626.101074,acc:0.000000 \n",
      "[train]:step:319,loss:6626.904297,acc:0.000000 \n",
      "[train]:step:320,loss:6637.475586,acc:0.000000 \n",
      "[vaild]:step:320,loss:6635.322266,acc:0.000000 \n",
      "[train]:step:321,loss:6673.929199,acc:0.000000 \n",
      "[train]:step:322,loss:6560.121094,acc:0.000000 \n",
      "[train]:step:323,loss:6761.729980,acc:0.000000 \n",
      "[train]:step:324,loss:6719.346680,acc:0.000000 \n",
      "[train]:step:325,loss:6737.917480,acc:0.000000 \n",
      "[train]:step:326,loss:6723.886719,acc:0.000000 \n",
      "[train]:step:327,loss:6679.658203,acc:0.000000 \n",
      "[train]:step:328,loss:6783.795410,acc:0.000000 \n",
      "[train]:step:329,loss:6702.657715,acc:0.000000 \n",
      "[train]:step:330,loss:6726.212402,acc:0.000000 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vaild]:step:330,loss:6733.189453,acc:0.000000 \n",
      "[train]:step:331,loss:6767.589844,acc:0.000000 \n",
      "[train]:step:332,loss:6718.222656,acc:0.000000 \n",
      "[train]:step:333,loss:6564.606934,acc:0.000000 \n",
      "[train]:step:334,loss:6698.502930,acc:0.000000 \n",
      "[train]:step:335,loss:6724.657715,acc:0.000000 \n",
      "[train]:step:336,loss:6693.106445,acc:0.000000 \n",
      "[train]:step:337,loss:6879.773926,acc:0.000000 \n",
      "[train]:step:338,loss:6714.915039,acc:0.000000 \n",
      "[train]:step:339,loss:6718.493164,acc:0.000000 \n",
      "[train]:step:340,loss:6731.926758,acc:0.000000 \n",
      "[vaild]:step:340,loss:6854.973633,acc:0.000000 \n",
      "[train]:step:341,loss:6730.921387,acc:0.000000 \n",
      "[train]:step:342,loss:6720.316406,acc:0.000000 \n",
      "[train]:step:343,loss:6705.214844,acc:0.000000 \n",
      "[train]:step:344,loss:6696.243652,acc:0.000000 \n",
      "[train]:step:345,loss:6593.520020,acc:0.000000 \n",
      "[train]:step:346,loss:6615.464355,acc:0.000000 \n",
      "[train]:step:347,loss:6777.386230,acc:0.010000 \n",
      "[train]:step:348,loss:6521.984375,acc:0.000000 \n",
      "[train]:step:349,loss:6857.241211,acc:0.000000 \n",
      "[train]:step:350,loss:6730.962891,acc:0.000000 \n",
      "[vaild]:step:350,loss:6810.540039,acc:0.000000 \n",
      "[train]:step:351,loss:6767.258789,acc:0.000000 \n",
      "[train]:step:352,loss:6638.872070,acc:0.000000 \n",
      "[train]:step:353,loss:6728.343750,acc:0.000000 \n",
      "[train]:step:354,loss:6732.829590,acc:0.000000 \n",
      "[train]:step:355,loss:6701.103027,acc:0.000000 \n",
      "[train]:step:356,loss:6773.632324,acc:0.000000 \n",
      "[train]:step:357,loss:6766.643066,acc:0.000000 \n",
      "[train]:step:358,loss:6493.133789,acc:0.000000 \n",
      "[train]:step:359,loss:6741.061035,acc:0.000000 \n",
      "[train]:step:360,loss:6658.615234,acc:0.000000 \n",
      "[vaild]:step:360,loss:6889.512695,acc:0.000000 \n",
      "[train]:step:361,loss:6674.483887,acc:0.000000 \n",
      "[train]:step:362,loss:6536.674316,acc:0.000000 \n",
      "[train]:step:363,loss:6657.951660,acc:0.000000 \n",
      "[train]:step:364,loss:6851.158203,acc:0.000000 \n",
      "[train]:step:365,loss:6784.662109,acc:0.000000 \n",
      "[train]:step:366,loss:6691.154297,acc:0.000000 \n",
      "[train]:step:367,loss:6623.368652,acc:0.000000 \n",
      "[train]:step:368,loss:6835.918945,acc:0.000000 \n",
      "[train]:step:369,loss:6684.068359,acc:0.000000 \n",
      "[train]:step:370,loss:6808.739258,acc:0.000000 \n",
      "[vaild]:step:370,loss:6821.491699,acc:0.000000 \n",
      "[train]:step:371,loss:6687.924805,acc:0.000000 \n",
      "[train]:step:372,loss:6876.910645,acc:0.000000 \n",
      "[train]:step:373,loss:6707.208008,acc:0.000000 \n",
      "[train]:step:374,loss:6893.028320,acc:0.000000 \n",
      "[train]:step:375,loss:6679.421387,acc:0.000000 \n",
      "[train]:step:376,loss:6682.167480,acc:0.000000 \n",
      "[train]:step:377,loss:6652.486328,acc:0.000000 \n",
      "[train]:step:378,loss:6749.791016,acc:0.000000 \n",
      "[train]:step:379,loss:6778.407715,acc:0.000000 \n",
      "[train]:step:380,loss:6742.060547,acc:0.000000 \n",
      "[vaild]:step:380,loss:6477.604980,acc:0.000000 \n",
      "[train]:step:381,loss:6649.375000,acc:0.000000 \n",
      "[train]:step:382,loss:6738.574219,acc:0.000000 \n",
      "[train]:step:383,loss:6728.181152,acc:0.000000 \n",
      "[train]:step:384,loss:6718.004395,acc:0.000000 \n",
      "[train]:step:385,loss:6578.964844,acc:0.000000 \n",
      "[train]:step:386,loss:6815.223145,acc:0.000000 \n",
      "[train]:step:387,loss:6659.114258,acc:0.000000 \n",
      "[train]:step:388,loss:6599.538086,acc:0.000000 \n",
      "[train]:step:389,loss:6755.258789,acc:0.000000 \n",
      "[train]:step:390,loss:6720.457031,acc:0.000000 \n",
      "[vaild]:step:390,loss:7013.336426,acc:0.000000 \n",
      "[train]:step:391,loss:6667.356445,acc:0.000000 \n",
      "[train]:step:392,loss:6720.316895,acc:0.000000 \n",
      "[train]:step:393,loss:6577.634766,acc:0.000000 \n",
      "[train]:step:394,loss:6576.495117,acc:0.000000 \n",
      "[train]:step:395,loss:6662.580566,acc:0.000000 \n",
      "[train]:step:396,loss:6685.000488,acc:0.000000 \n",
      "[train]:step:397,loss:6672.417480,acc:0.000000 \n",
      "[train]:step:398,loss:6755.703613,acc:0.000000 \n",
      "[train]:step:399,loss:6799.615234,acc:0.000000 \n",
      "[train]:step:400,loss:6671.308594,acc:0.000000 \n",
      "[vaild]:step:400,loss:6699.355469,acc:0.000000 \n",
      "[train]:step:401,loss:6775.528809,acc:0.000000 \n",
      "[train]:step:402,loss:6839.924805,acc:0.000000 \n",
      "[train]:step:403,loss:6711.453125,acc:0.000000 \n",
      "[train]:step:404,loss:6773.013672,acc:0.000000 \n",
      "[train]:step:405,loss:6848.308594,acc:0.000000 \n",
      "[train]:step:406,loss:6641.051270,acc:0.000000 \n",
      "[train]:step:407,loss:6767.296875,acc:0.000000 \n",
      "[train]:step:408,loss:6721.868164,acc:0.000000 \n",
      "[train]:step:409,loss:6626.021973,acc:0.000000 \n",
      "[train]:step:410,loss:6620.751953,acc:0.000000 \n",
      "[vaild]:step:410,loss:6787.706055,acc:0.000000 \n",
      "[train]:step:411,loss:6682.001953,acc:0.000000 \n",
      "[train]:step:412,loss:6757.944336,acc:0.000000 \n",
      "[train]:step:413,loss:6694.013672,acc:0.000000 \n",
      "[train]:step:414,loss:6768.114258,acc:0.000000 \n",
      "[train]:step:415,loss:6739.981445,acc:0.000000 \n",
      "[train]:step:416,loss:6667.740723,acc:0.000000 \n",
      "[train]:step:417,loss:6692.165039,acc:0.000000 \n",
      "[train]:step:418,loss:6837.745605,acc:0.000000 \n",
      "[train]:step:419,loss:6816.463867,acc:0.000000 \n",
      "[train]:step:420,loss:6764.780762,acc:0.000000 \n",
      "[vaild]:step:420,loss:6851.591309,acc:0.000000 \n",
      "[train]:step:421,loss:6561.417969,acc:0.000000 \n",
      "[train]:step:422,loss:6702.977051,acc:0.000000 \n",
      "[train]:step:423,loss:6640.839844,acc:0.000000 \n",
      "[train]:step:424,loss:6812.460449,acc:0.000000 \n",
      "[train]:step:425,loss:6739.673828,acc:0.000000 \n",
      "[train]:step:426,loss:6779.245117,acc:0.000000 \n",
      "[train]:step:427,loss:6777.684570,acc:0.000000 \n",
      "[train]:step:428,loss:6782.849609,acc:0.000000 \n",
      "[train]:step:429,loss:6683.427734,acc:0.000000 \n",
      "[train]:step:430,loss:6630.782715,acc:0.000000 \n",
      "[vaild]:step:430,loss:6784.188477,acc:0.000000 \n",
      "[train]:step:431,loss:6812.325195,acc:0.000000 \n",
      "[train]:step:432,loss:6764.071777,acc:0.000000 \n",
      "[train]:step:433,loss:6752.333984,acc:0.000000 \n",
      "[train]:step:434,loss:6549.234863,acc:0.000000 \n",
      "[train]:step:435,loss:6732.308594,acc:0.000000 \n",
      "[train]:step:436,loss:6561.121094,acc:0.000000 \n",
      "[train]:step:437,loss:6795.622559,acc:0.000000 \n",
      "[train]:step:438,loss:6753.509766,acc:0.000000 \n",
      "[train]:step:439,loss:6813.369141,acc:0.000000 \n",
      "[train]:step:440,loss:6610.423340,acc:0.000000 \n",
      "[vaild]:step:440,loss:6902.304688,acc:0.000000 \n",
      "[train]:step:441,loss:6787.146973,acc:0.000000 \n",
      "[train]:step:442,loss:6693.713867,acc:0.000000 \n",
      "[train]:step:443,loss:6712.098145,acc:0.000000 \n",
      "[train]:step:444,loss:6815.373535,acc:0.000000 \n",
      "[train]:step:445,loss:6626.542969,acc:0.000000 \n",
      "[train]:step:446,loss:6704.413574,acc:0.000000 \n",
      "[train]:step:447,loss:6565.700684,acc:0.000000 \n",
      "[train]:step:448,loss:6767.526367,acc:0.000000 \n",
      "[train]:step:449,loss:6676.247070,acc:0.000000 \n",
      "[train]:step:450,loss:6665.965820,acc:0.000000 \n",
      "[vaild]:step:450,loss:6839.791016,acc:0.000000 \n",
      "[train]:step:451,loss:6639.668945,acc:0.000000 \n",
      "[train]:step:452,loss:6652.569336,acc:0.000000 \n",
      "[train]:step:453,loss:6782.543945,acc:0.000000 \n",
      "[train]:step:454,loss:6703.376465,acc:0.000000 \n",
      "[train]:step:455,loss:6712.405762,acc:0.000000 \n",
      "[train]:step:456,loss:6651.906738,acc:0.000000 \n",
      "[train]:step:457,loss:6672.781738,acc:0.000000 \n",
      "[train]:step:458,loss:6680.818848,acc:0.000000 \n",
      "[train]:step:459,loss:6838.662109,acc:0.000000 \n",
      "[train]:step:460,loss:6716.455566,acc:0.000000 \n",
      "[vaild]:step:460,loss:6799.580566,acc:0.000000 \n",
      "[train]:step:461,loss:6679.517090,acc:0.000000 \n",
      "[train]:step:462,loss:6746.527344,acc:0.000000 \n",
      "[train]:step:463,loss:6592.687500,acc:0.000000 \n",
      "[train]:step:464,loss:6723.953613,acc:0.000000 \n",
      "[train]:step:465,loss:6737.193848,acc:0.000000 \n",
      "[train]:step:466,loss:6856.209473,acc:0.000000 \n",
      "[train]:step:467,loss:6629.590820,acc:0.000000 \n",
      "[train]:step:468,loss:6755.075195,acc:0.000000 \n",
      "[train]:step:469,loss:6754.883789,acc:0.010000 \n",
      "[train]:step:470,loss:6598.288574,acc:0.010000 \n",
      "[vaild]:step:470,loss:6884.669922,acc:0.000000 \n",
      "[train]:step:471,loss:6786.142578,acc:0.000000 \n",
      "[train]:step:472,loss:6597.774414,acc:0.000000 \n",
      "[train]:step:473,loss:6746.168945,acc:0.000000 \n",
      "[train]:step:474,loss:6660.424805,acc:0.000000 \n",
      "[train]:step:475,loss:6590.965820,acc:0.000000 \n",
      "[train]:step:476,loss:6729.078613,acc:0.000000 \n",
      "[train]:step:477,loss:6767.188965,acc:0.000000 \n",
      "[train]:step:478,loss:6632.650391,acc:0.000000 \n",
      "[train]:step:479,loss:6752.281250,acc:0.000000 \n",
      "[train]:step:480,loss:6581.064453,acc:0.000000 \n",
      "[vaild]:step:480,loss:6905.636719,acc:0.000000 \n",
      "[train]:step:481,loss:6756.489258,acc:0.000000 \n",
      "[train]:step:482,loss:6668.049805,acc:0.000000 \n",
      "[train]:step:483,loss:6748.185059,acc:0.000000 \n",
      "[train]:step:484,loss:6767.113770,acc:0.000000 \n",
      "[train]:step:485,loss:6516.336426,acc:0.000000 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train]:step:486,loss:6836.688965,acc:0.000000 \n",
      "[train]:step:487,loss:6832.850098,acc:0.000000 \n",
      "[train]:step:488,loss:6775.202637,acc:0.000000 \n",
      "[train]:step:489,loss:6764.574219,acc:0.000000 \n",
      "[train]:step:490,loss:6630.829590,acc:0.000000 \n",
      "[vaild]:step:490,loss:6468.269043,acc:0.000000 \n",
      "[train]:step:491,loss:6902.495605,acc:0.000000 \n",
      "[train]:step:492,loss:6828.623535,acc:0.000000 \n",
      "[train]:step:493,loss:6805.800781,acc:0.010000 \n",
      "[train]:step:494,loss:6659.970215,acc:0.000000 \n",
      "[train]:step:495,loss:6672.785156,acc:0.000000 \n",
      "[train]:step:496,loss:6826.250000,acc:0.000000 \n",
      "[train]:step:497,loss:6629.796875,acc:0.000000 \n",
      "[train]:step:498,loss:6827.892090,acc:0.000000 \n",
      "[train]:step:499,loss:6611.465820,acc:0.000000 \n",
      "[train]:step:500,loss:6790.372559,acc:0.000000 \n",
      "[vaild]:step:500,loss:7139.785645,acc:0.000000 \n",
      "[train]:step:501,loss:6732.848633,acc:0.000000 \n",
      "[train]:step:502,loss:6718.850586,acc:0.000000 \n",
      "[train]:step:503,loss:6729.469238,acc:0.000000 \n",
      "[train]:step:504,loss:6746.062500,acc:0.000000 \n",
      "[train]:step:505,loss:6665.678711,acc:0.000000 \n",
      "[train]:step:506,loss:6751.035645,acc:0.000000 \n",
      "[train]:step:507,loss:6762.951660,acc:0.000000 \n",
      "[train]:step:508,loss:6717.531738,acc:0.000000 \n",
      "[train]:step:509,loss:6919.136719,acc:0.000000 \n",
      "[train]:step:510,loss:6666.194824,acc:0.000000 \n",
      "[vaild]:step:510,loss:6684.052246,acc:0.000000 \n",
      "[train]:step:511,loss:6780.733887,acc:0.000000 \n",
      "[train]:step:512,loss:6748.252930,acc:0.000000 \n",
      "[train]:step:513,loss:6748.697266,acc:0.000000 \n",
      "[train]:step:514,loss:6767.643066,acc:0.000000 \n",
      "[train]:step:515,loss:6866.810547,acc:0.000000 \n",
      "[train]:step:516,loss:6701.870117,acc:0.000000 \n",
      "[train]:step:517,loss:6700.290039,acc:0.000000 \n",
      "[train]:step:518,loss:6589.889160,acc:0.010000 \n",
      "[train]:step:519,loss:6717.345215,acc:0.000000 \n",
      "[train]:step:520,loss:6755.359375,acc:0.000000 \n",
      "[vaild]:step:520,loss:6678.018555,acc:0.000000 \n",
      "[train]:step:521,loss:6730.323242,acc:0.000000 \n",
      "[train]:step:522,loss:6718.228516,acc:0.000000 \n",
      "[train]:step:523,loss:6518.366211,acc:0.000000 \n",
      "[train]:step:524,loss:6784.925781,acc:0.000000 \n",
      "[train]:step:525,loss:6726.431641,acc:0.000000 \n",
      "[train]:step:526,loss:6673.861816,acc:0.000000 \n",
      "[train]:step:527,loss:6788.061035,acc:0.000000 \n",
      "[train]:step:528,loss:6743.841309,acc:0.000000 \n",
      "[train]:step:529,loss:6689.500488,acc:0.000000 \n",
      "[train]:step:530,loss:6691.252441,acc:0.000000 \n",
      "[vaild]:step:530,loss:6719.526855,acc:0.000000 \n",
      "[train]:step:531,loss:6805.553223,acc:0.000000 \n",
      "[train]:step:532,loss:6732.178223,acc:0.000000 \n",
      "[train]:step:533,loss:6568.358887,acc:0.000000 \n",
      "[train]:step:534,loss:6843.687500,acc:0.000000 \n",
      "[train]:step:535,loss:6698.204590,acc:0.000000 \n",
      "[train]:step:536,loss:6633.935059,acc:0.000000 \n",
      "[train]:step:537,loss:6566.810059,acc:0.000000 \n",
      "[train]:step:538,loss:6708.189941,acc:0.000000 \n",
      "[train]:step:539,loss:6667.276855,acc:0.000000 \n",
      "[train]:step:540,loss:6728.431152,acc:0.000000 \n",
      "[vaild]:step:540,loss:6286.231934,acc:0.000000 \n",
      "[train]:step:541,loss:6650.369141,acc:0.000000 \n",
      "[train]:step:542,loss:6718.110840,acc:0.000000 \n",
      "[train]:step:543,loss:6796.686035,acc:0.000000 \n",
      "[train]:step:544,loss:6735.887695,acc:0.000000 \n",
      "[train]:step:545,loss:6785.674316,acc:0.000000 \n",
      "[train]:step:546,loss:6564.937500,acc:0.000000 \n",
      "[train]:step:547,loss:6646.721680,acc:0.000000 \n",
      "[train]:step:548,loss:6743.093262,acc:0.000000 \n",
      "[train]:step:549,loss:6693.421387,acc:0.000000 \n",
      "[train]:step:550,loss:6619.049316,acc:0.000000 \n",
      "[vaild]:step:550,loss:6795.437988,acc:0.000000 \n",
      "[train]:step:551,loss:6728.865723,acc:0.000000 \n",
      "[train]:step:552,loss:6671.979980,acc:0.000000 \n",
      "[train]:step:553,loss:6771.153320,acc:0.000000 \n",
      "[train]:step:554,loss:6732.740234,acc:0.000000 \n",
      "[train]:step:555,loss:6603.453125,acc:0.000000 \n",
      "[train]:step:556,loss:6694.214355,acc:0.000000 \n",
      "[train]:step:557,loss:6755.181641,acc:0.000000 \n",
      "[train]:step:558,loss:6608.019531,acc:0.000000 \n",
      "[train]:step:559,loss:6622.104980,acc:0.000000 \n",
      "[train]:step:560,loss:6710.443848,acc:0.000000 \n",
      "[vaild]:step:560,loss:7056.298340,acc:0.000000 \n",
      "[train]:step:561,loss:6719.377441,acc:0.000000 \n",
      "[train]:step:562,loss:6650.917969,acc:0.000000 \n",
      "[train]:step:563,loss:6664.731445,acc:0.000000 \n",
      "[train]:step:564,loss:6718.296875,acc:0.000000 \n",
      "[train]:step:565,loss:6784.180176,acc:0.000000 \n",
      "[train]:step:566,loss:6606.598145,acc:0.000000 \n",
      "[train]:step:567,loss:6706.283203,acc:0.000000 \n",
      "[train]:step:568,loss:6703.316406,acc:0.000000 \n",
      "[train]:step:569,loss:6706.384766,acc:0.000000 \n",
      "[train]:step:570,loss:6669.494141,acc:0.000000 \n",
      "[vaild]:step:570,loss:6605.523438,acc:0.000000 \n",
      "[train]:step:571,loss:6780.353027,acc:0.000000 \n",
      "[train]:step:572,loss:6691.838867,acc:0.000000 \n",
      "[train]:step:573,loss:6643.676758,acc:0.000000 \n",
      "[train]:step:574,loss:6759.470703,acc:0.000000 \n",
      "[train]:step:575,loss:6744.974609,acc:0.000000 \n",
      "[train]:step:576,loss:6829.578125,acc:0.000000 \n",
      "[train]:step:577,loss:6635.787109,acc:0.000000 \n",
      "[train]:step:578,loss:6791.762695,acc:0.000000 \n",
      "[train]:step:579,loss:6657.998535,acc:0.000000 \n",
      "[train]:step:580,loss:6817.496094,acc:0.000000 \n",
      "[vaild]:step:580,loss:6935.467773,acc:0.000000 \n",
      "[train]:step:581,loss:6596.561035,acc:0.000000 \n",
      "[train]:step:582,loss:6683.560059,acc:0.000000 \n",
      "[train]:step:583,loss:6758.267090,acc:0.000000 \n",
      "[train]:step:584,loss:6624.390625,acc:0.000000 \n",
      "[train]:step:585,loss:6667.293945,acc:0.000000 \n",
      "[train]:step:586,loss:6774.437012,acc:0.000000 \n",
      "[train]:step:587,loss:6665.198242,acc:0.000000 \n",
      "[train]:step:588,loss:6688.180176,acc:0.000000 \n",
      "[train]:step:589,loss:6746.349609,acc:0.000000 \n",
      "[train]:step:590,loss:6593.629395,acc:0.000000 \n",
      "[vaild]:step:590,loss:6874.179688,acc:0.000000 \n",
      "[train]:step:591,loss:6750.693359,acc:0.000000 \n",
      "[train]:step:592,loss:6646.562988,acc:0.000000 \n",
      "[train]:step:593,loss:6620.982910,acc:0.000000 \n",
      "[train]:step:594,loss:6734.033203,acc:0.000000 \n",
      "[train]:step:595,loss:6736.775391,acc:0.000000 \n",
      "[train]:step:596,loss:6825.270020,acc:0.000000 \n",
      "[train]:step:597,loss:6496.247070,acc:0.000000 \n",
      "[train]:step:598,loss:6759.350586,acc:0.000000 \n",
      "[train]:step:599,loss:6684.292969,acc:0.000000 \n",
      "[train]:step:600,loss:6760.933105,acc:0.000000 \n",
      "[vaild]:step:600,loss:6605.413086,acc:0.000000 \n",
      "[train]:step:601,loss:6533.709473,acc:0.000000 \n",
      "[train]:step:602,loss:6706.676270,acc:0.000000 \n",
      "[train]:step:603,loss:6581.075684,acc:0.000000 \n",
      "[train]:step:604,loss:6751.129395,acc:0.000000 \n",
      "[train]:step:605,loss:6763.877930,acc:0.000000 \n",
      "[train]:step:606,loss:6711.287598,acc:0.000000 \n",
      "[train]:step:607,loss:6685.491211,acc:0.000000 \n",
      "[train]:step:608,loss:6574.143066,acc:0.000000 \n",
      "[train]:step:609,loss:6790.059570,acc:0.000000 \n",
      "[train]:step:610,loss:6785.555176,acc:0.000000 \n",
      "[vaild]:step:610,loss:6545.913086,acc:0.000000 \n",
      "[train]:step:611,loss:6672.823730,acc:0.000000 \n",
      "[train]:step:612,loss:6707.750488,acc:0.000000 \n",
      "[train]:step:613,loss:6702.033691,acc:0.000000 \n",
      "[train]:step:614,loss:6698.976074,acc:0.000000 \n",
      "[train]:step:615,loss:6679.808105,acc:0.000000 \n",
      "[train]:step:616,loss:6513.400391,acc:0.000000 \n",
      "[train]:step:617,loss:6738.155762,acc:0.000000 \n",
      "[train]:step:618,loss:6701.675781,acc:0.000000 \n",
      "[train]:step:619,loss:6701.868652,acc:0.000000 \n",
      "[train]:step:620,loss:6813.176270,acc:0.000000 \n",
      "[vaild]:step:620,loss:6824.153320,acc:0.000000 \n",
      "[train]:step:621,loss:6639.693848,acc:0.000000 \n",
      "[train]:step:622,loss:6594.837402,acc:0.000000 \n",
      "[train]:step:623,loss:6540.420410,acc:0.000000 \n",
      "[train]:step:624,loss:6691.747559,acc:0.000000 \n",
      "[train]:step:625,loss:6649.168945,acc:0.000000 \n",
      "[train]:step:626,loss:6781.471191,acc:0.000000 \n",
      "[train]:step:627,loss:6710.028320,acc:0.000000 \n",
      "[train]:step:628,loss:6810.393066,acc:0.000000 \n",
      "[train]:step:629,loss:6741.281738,acc:0.000000 \n",
      "[train]:step:630,loss:6804.263184,acc:0.000000 \n",
      "[vaild]:step:630,loss:6422.433594,acc:0.000000 \n",
      "[train]:step:631,loss:6626.910156,acc:0.000000 \n",
      "[train]:step:632,loss:6614.134277,acc:0.000000 \n",
      "[train]:step:633,loss:6661.321777,acc:0.000000 \n",
      "[train]:step:634,loss:6764.128906,acc:0.000000 \n",
      "[train]:step:635,loss:6678.156738,acc:0.000000 \n",
      "[train]:step:636,loss:6680.068359,acc:0.000000 \n",
      "[train]:step:637,loss:6661.093262,acc:0.000000 \n",
      "[train]:step:638,loss:6713.440430,acc:0.000000 \n",
      "[train]:step:639,loss:6835.181641,acc:0.000000 \n",
      "[train]:step:640,loss:6795.215820,acc:0.000000 \n",
      "[vaild]:step:640,loss:7028.696777,acc:0.000000 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train]:step:641,loss:6693.801270,acc:0.000000 \n",
      "[train]:step:642,loss:6714.169922,acc:0.000000 \n",
      "[train]:step:643,loss:6779.265625,acc:0.000000 \n",
      "[train]:step:644,loss:6633.039551,acc:0.000000 \n",
      "[train]:step:645,loss:6755.427734,acc:0.000000 \n",
      "[train]:step:646,loss:6817.754883,acc:0.000000 \n",
      "[train]:step:647,loss:6694.216309,acc:0.000000 \n",
      "[train]:step:648,loss:6778.402344,acc:0.000000 \n",
      "[train]:step:649,loss:6704.412598,acc:0.000000 \n",
      "[train]:step:650,loss:6746.037598,acc:0.000000 \n",
      "[vaild]:step:650,loss:6806.652344,acc:0.000000 \n",
      "[train]:step:651,loss:6740.677734,acc:0.000000 \n",
      "[train]:step:652,loss:6671.732422,acc:0.000000 \n",
      "[train]:step:653,loss:6792.058105,acc:0.000000 \n",
      "[train]:step:654,loss:6882.046875,acc:0.000000 \n",
      "[train]:step:655,loss:6707.075195,acc:0.000000 \n",
      "[train]:step:656,loss:6800.083008,acc:0.000000 \n",
      "[train]:step:657,loss:6668.575684,acc:0.000000 \n",
      "[train]:step:658,loss:6957.103027,acc:0.000000 \n",
      "[train]:step:659,loss:6813.150391,acc:0.000000 \n",
      "[train]:step:660,loss:6815.755859,acc:0.000000 \n",
      "[vaild]:step:660,loss:6465.642578,acc:0.000000 \n",
      "[train]:step:661,loss:6616.691406,acc:0.000000 \n",
      "[train]:step:662,loss:6626.065430,acc:0.000000 \n",
      "[train]:step:663,loss:6659.933594,acc:0.000000 \n",
      "[train]:step:664,loss:6807.945801,acc:0.000000 \n",
      "[train]:step:665,loss:6794.792969,acc:0.000000 \n",
      "[train]:step:666,loss:6656.012695,acc:0.000000 \n",
      "[train]:step:667,loss:6811.282715,acc:0.000000 \n",
      "[train]:step:668,loss:6612.559570,acc:0.000000 \n",
      "[train]:step:669,loss:6712.952637,acc:0.000000 \n",
      "[train]:step:670,loss:6779.077637,acc:0.000000 \n",
      "[vaild]:step:670,loss:7021.086914,acc:0.000000 \n",
      "[train]:step:671,loss:6609.285156,acc:0.000000 \n",
      "[train]:step:672,loss:6793.621094,acc:0.000000 \n",
      "[train]:step:673,loss:6789.107422,acc:0.000000 \n",
      "[train]:step:674,loss:6689.908691,acc:0.000000 \n",
      "[train]:step:675,loss:6666.207031,acc:0.000000 \n",
      "[train]:step:676,loss:6766.887695,acc:0.000000 \n",
      "[train]:step:677,loss:6588.926758,acc:0.000000 \n",
      "[train]:step:678,loss:6709.224609,acc:0.000000 \n",
      "[train]:step:679,loss:6823.621094,acc:0.000000 \n",
      "[train]:step:680,loss:6728.207520,acc:0.000000 \n",
      "[vaild]:step:680,loss:6821.616699,acc:0.000000 \n",
      "[train]:step:681,loss:6727.944824,acc:0.000000 \n",
      "[train]:step:682,loss:6576.991699,acc:0.000000 \n",
      "[train]:step:683,loss:6629.006348,acc:0.000000 \n",
      "[train]:step:684,loss:6850.568848,acc:0.000000 \n",
      "[train]:step:685,loss:6756.234375,acc:0.000000 \n",
      "[train]:step:686,loss:6843.877930,acc:0.000000 \n",
      "[train]:step:687,loss:6742.237305,acc:0.000000 \n",
      "[train]:step:688,loss:6633.521973,acc:0.000000 \n",
      "[train]:step:689,loss:6819.893555,acc:0.000000 \n",
      "[train]:step:690,loss:6725.021484,acc:0.000000 \n",
      "[vaild]:step:690,loss:6814.145996,acc:0.000000 \n",
      "[train]:step:691,loss:6566.784180,acc:0.000000 \n",
      "[train]:step:692,loss:6723.088867,acc:0.010000 \n",
      "[train]:step:693,loss:6648.573730,acc:0.000000 \n",
      "[train]:step:694,loss:6656.806152,acc:0.000000 \n",
      "[train]:step:695,loss:6760.818848,acc:0.000000 \n",
      "[train]:step:696,loss:6786.710449,acc:0.000000 \n",
      "[train]:step:697,loss:6808.078125,acc:0.000000 \n",
      "[train]:step:698,loss:6771.149902,acc:0.000000 \n",
      "[train]:step:699,loss:6582.211426,acc:0.000000 \n",
      "[train]:step:700,loss:6836.748047,acc:0.000000 \n",
      "[vaild]:step:700,loss:6608.661133,acc:0.000000 \n",
      "[train]:step:701,loss:6708.600098,acc:0.000000 \n",
      "[train]:step:702,loss:6580.695801,acc:0.000000 \n",
      "[train]:step:703,loss:6714.170410,acc:0.000000 \n",
      "[train]:step:704,loss:6851.306152,acc:0.000000 \n",
      "[train]:step:705,loss:6784.897949,acc:0.000000 \n",
      "[train]:step:706,loss:6662.852539,acc:0.000000 \n",
      "[train]:step:707,loss:6892.756348,acc:0.010000 \n",
      "[train]:step:708,loss:6726.782715,acc:0.000000 \n",
      "[train]:step:709,loss:6769.064941,acc:0.000000 \n",
      "[train]:step:710,loss:6741.232422,acc:0.000000 \n",
      "[vaild]:step:710,loss:6754.206055,acc:0.000000 \n",
      "[train]:step:711,loss:6794.611816,acc:0.000000 \n",
      "[train]:step:712,loss:6802.477539,acc:0.000000 \n",
      "[train]:step:713,loss:6763.292969,acc:0.000000 \n",
      "[train]:step:714,loss:6619.917480,acc:0.000000 \n",
      "[train]:step:715,loss:6947.252441,acc:0.000000 \n",
      "[train]:step:716,loss:6643.760742,acc:0.000000 \n",
      "[train]:step:717,loss:6763.368164,acc:0.000000 \n",
      "[train]:step:718,loss:6730.336914,acc:0.000000 \n",
      "[train]:step:719,loss:6667.710449,acc:0.000000 \n",
      "[train]:step:720,loss:6747.127441,acc:0.000000 \n",
      "[vaild]:step:720,loss:6633.937988,acc:0.000000 \n",
      "[train]:step:721,loss:6552.721191,acc:0.000000 \n",
      "[train]:step:722,loss:6816.304199,acc:0.000000 \n",
      "[train]:step:723,loss:6744.624512,acc:0.000000 \n",
      "[train]:step:724,loss:6627.496094,acc:0.000000 \n",
      "[train]:step:725,loss:6659.830566,acc:0.000000 \n",
      "[train]:step:726,loss:6757.770020,acc:0.000000 \n",
      "[train]:step:727,loss:6679.054199,acc:0.000000 \n",
      "[train]:step:728,loss:6778.044922,acc:0.000000 \n",
      "[train]:step:729,loss:6772.347656,acc:0.000000 \n",
      "[train]:step:730,loss:6770.963867,acc:0.000000 \n",
      "[vaild]:step:730,loss:6797.974609,acc:0.062500 \n",
      "[train]:step:731,loss:6703.632324,acc:0.000000 \n",
      "[train]:step:732,loss:6823.796387,acc:0.000000 \n",
      "[train]:step:733,loss:6598.312988,acc:0.000000 \n",
      "[train]:step:734,loss:6660.608887,acc:0.000000 \n",
      "[train]:step:735,loss:6533.950195,acc:0.000000 \n",
      "[train]:step:736,loss:6725.862305,acc:0.000000 \n",
      "[train]:step:737,loss:6780.029785,acc:0.000000 \n",
      "[train]:step:738,loss:6782.020508,acc:0.000000 \n",
      "[train]:step:739,loss:6741.407715,acc:0.000000 \n",
      "[train]:step:740,loss:6605.907715,acc:0.000000 \n",
      "[vaild]:step:740,loss:6851.476562,acc:0.000000 \n",
      "[train]:step:741,loss:6642.493652,acc:0.000000 \n",
      "[train]:step:742,loss:6770.832520,acc:0.000000 \n",
      "[train]:step:743,loss:6604.801270,acc:0.000000 \n",
      "[train]:step:744,loss:6725.812500,acc:0.000000 \n",
      "[train]:step:745,loss:6738.406250,acc:0.000000 \n",
      "[train]:step:746,loss:6730.892578,acc:0.000000 \n",
      "[train]:step:747,loss:6699.447266,acc:0.000000 \n",
      "[train]:step:748,loss:6827.364258,acc:0.000000 \n",
      "[train]:step:749,loss:6765.594238,acc:0.000000 \n",
      "[train]:step:750,loss:6471.375488,acc:0.000000 \n",
      "[vaild]:step:750,loss:6939.260742,acc:0.000000 \n",
      "[train]:step:751,loss:6675.040527,acc:0.000000 \n",
      "[train]:step:752,loss:6725.193359,acc:0.000000 \n",
      "[train]:step:753,loss:6772.973633,acc:0.000000 \n",
      "[train]:step:754,loss:6709.806641,acc:0.000000 \n",
      "[train]:step:755,loss:6750.277344,acc:0.010000 \n",
      "[train]:step:756,loss:6815.555664,acc:0.000000 \n",
      "[train]:step:757,loss:6706.756348,acc:0.000000 \n",
      "[train]:step:758,loss:6840.464355,acc:0.000000 \n",
      "[train]:step:759,loss:6521.254883,acc:0.000000 \n",
      "[train]:step:760,loss:6718.329590,acc:0.000000 \n",
      "[vaild]:step:760,loss:6640.820801,acc:0.000000 \n",
      "[train]:step:761,loss:6605.221191,acc:0.000000 \n",
      "[train]:step:762,loss:6767.428711,acc:0.000000 \n",
      "[train]:step:763,loss:6832.154785,acc:0.010000 \n",
      "[train]:step:764,loss:6820.279297,acc:0.000000 \n",
      "[train]:step:765,loss:6644.452637,acc:0.000000 \n",
      "[train]:step:766,loss:6763.136719,acc:0.000000 \n",
      "[train]:step:767,loss:6498.863281,acc:0.000000 \n",
      "[train]:step:768,loss:6835.396973,acc:0.000000 \n",
      "[train]:step:769,loss:6683.934570,acc:0.000000 \n",
      "[train]:step:770,loss:6673.641113,acc:0.000000 \n",
      "[vaild]:step:770,loss:6331.029785,acc:0.000000 \n",
      "[train]:step:771,loss:6731.065430,acc:0.000000 \n",
      "[train]:step:772,loss:6697.542480,acc:0.000000 \n",
      "[train]:step:773,loss:6764.753906,acc:0.000000 \n",
      "[train]:step:774,loss:6728.919922,acc:0.000000 \n",
      "[train]:step:775,loss:6647.427734,acc:0.000000 \n",
      "[train]:step:776,loss:6769.883789,acc:0.000000 \n",
      "[train]:step:777,loss:6584.506348,acc:0.000000 \n",
      "[train]:step:778,loss:6745.887695,acc:0.000000 \n",
      "[train]:step:779,loss:6610.324219,acc:0.000000 \n",
      "[train]:step:780,loss:6635.841309,acc:0.000000 \n",
      "[vaild]:step:780,loss:6861.375000,acc:0.000000 \n",
      "[train]:step:781,loss:6830.697266,acc:0.000000 \n",
      "[train]:step:782,loss:6730.529297,acc:0.000000 \n",
      "[train]:step:783,loss:6635.812500,acc:0.000000 \n",
      "[train]:step:784,loss:6617.732910,acc:0.000000 \n",
      "[train]:step:785,loss:6738.113770,acc:0.000000 \n",
      "[train]:step:786,loss:6774.074219,acc:0.000000 \n",
      "[train]:step:787,loss:6678.273926,acc:0.000000 \n",
      "[train]:step:788,loss:6726.626953,acc:0.000000 \n",
      "[train]:step:789,loss:6745.177734,acc:0.000000 \n",
      "[train]:step:790,loss:6752.857910,acc:0.000000 \n",
      "[vaild]:step:790,loss:6841.500977,acc:0.000000 \n",
      "[train]:step:791,loss:6621.524414,acc:0.000000 \n",
      "[train]:step:792,loss:6664.393066,acc:0.000000 \n",
      "[train]:step:793,loss:6703.174805,acc:0.000000 \n",
      "[train]:step:794,loss:6713.969238,acc:0.000000 \n",
      "[train]:step:795,loss:6861.333984,acc:0.000000 \n",
      "[train]:step:796,loss:6547.732422,acc:0.000000 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train]:step:797,loss:6705.833008,acc:0.000000 \n",
      "[train]:step:798,loss:6858.696289,acc:0.000000 \n",
      "[train]:step:799,loss:6688.880859,acc:0.000000 \n",
      "[train]:step:800,loss:6541.016113,acc:0.000000 \n",
      "[vaild]:step:800,loss:6623.995117,acc:0.000000 \n",
      "[train]:step:801,loss:6775.274414,acc:0.020000 \n",
      "[train]:step:802,loss:6659.578125,acc:0.000000 \n",
      "[train]:step:803,loss:6614.756348,acc:0.000000 \n",
      "[train]:step:804,loss:6789.956055,acc:0.000000 \n",
      "[train]:step:805,loss:6688.852539,acc:0.000000 \n",
      "[train]:step:806,loss:6665.744141,acc:0.010000 \n",
      "[train]:step:807,loss:6650.307617,acc:0.000000 \n",
      "[train]:step:808,loss:6785.137695,acc:0.000000 \n",
      "[train]:step:809,loss:6765.645020,acc:0.000000 \n",
      "[train]:step:810,loss:6550.605469,acc:0.000000 \n",
      "[vaild]:step:810,loss:6292.386719,acc:0.000000 \n",
      "[train]:step:811,loss:6776.199219,acc:0.000000 \n",
      "[train]:step:812,loss:6622.141113,acc:0.000000 \n",
      "[train]:step:813,loss:6749.992676,acc:0.000000 \n",
      "[train]:step:814,loss:6708.589844,acc:0.000000 \n",
      "[train]:step:815,loss:6845.058594,acc:0.000000 \n",
      "[train]:step:816,loss:6782.750000,acc:0.000000 \n",
      "[train]:step:817,loss:6892.350586,acc:0.000000 \n",
      "[train]:step:818,loss:6680.157715,acc:0.000000 \n",
      "[train]:step:819,loss:6725.312500,acc:0.000000 \n",
      "[train]:step:820,loss:6728.151855,acc:0.000000 \n",
      "[vaild]:step:820,loss:6772.791992,acc:0.000000 \n",
      "[train]:step:821,loss:6725.234863,acc:0.000000 \n",
      "[train]:step:822,loss:6780.403809,acc:0.000000 \n",
      "[train]:step:823,loss:6686.847656,acc:0.000000 \n",
      "[train]:step:824,loss:6775.691895,acc:0.000000 \n",
      "[train]:step:825,loss:6616.332031,acc:0.000000 \n",
      "[train]:step:826,loss:6514.017578,acc:0.000000 \n",
      "[train]:step:827,loss:6660.544922,acc:0.000000 \n",
      "[train]:step:828,loss:6769.904297,acc:0.000000 \n",
      "[train]:step:829,loss:6849.027344,acc:0.000000 \n",
      "[train]:step:830,loss:6683.466797,acc:0.000000 \n",
      "[vaild]:step:830,loss:6817.852539,acc:0.000000 \n",
      "[train]:step:831,loss:6656.743164,acc:0.000000 \n",
      "[train]:step:832,loss:6738.271484,acc:0.000000 \n",
      "[train]:step:833,loss:6674.765137,acc:0.000000 \n",
      "[train]:step:834,loss:6801.898926,acc:0.000000 \n",
      "[train]:step:835,loss:6698.498047,acc:0.000000 \n",
      "[train]:step:836,loss:6618.462891,acc:0.000000 \n",
      "[train]:step:837,loss:6765.778320,acc:0.000000 \n",
      "[train]:step:838,loss:6812.841797,acc:0.000000 \n",
      "[train]:step:839,loss:6763.781738,acc:0.000000 \n",
      "[train]:step:840,loss:6733.587891,acc:0.000000 \n",
      "[vaild]:step:840,loss:6854.259277,acc:0.000000 \n",
      "[train]:step:841,loss:6652.334473,acc:0.000000 \n",
      "[train]:step:842,loss:6747.321777,acc:0.000000 \n",
      "[train]:step:843,loss:6833.785156,acc:0.000000 \n",
      "[train]:step:844,loss:6752.497559,acc:0.000000 \n",
      "[train]:step:845,loss:6751.523926,acc:0.000000 \n",
      "[train]:step:846,loss:6657.229980,acc:0.000000 \n",
      "[train]:step:847,loss:6901.542480,acc:0.000000 \n",
      "[train]:step:848,loss:6718.842285,acc:0.000000 \n",
      "[train]:step:849,loss:6722.693359,acc:0.000000 \n",
      "[train]:step:850,loss:6661.566406,acc:0.000000 \n",
      "[vaild]:step:850,loss:6794.186523,acc:0.000000 \n",
      "[train]:step:851,loss:6862.283691,acc:0.000000 \n",
      "[train]:step:852,loss:6672.201660,acc:0.000000 \n",
      "[train]:step:853,loss:6632.611816,acc:0.000000 \n",
      "[train]:step:854,loss:6688.072266,acc:0.000000 \n",
      "[train]:step:855,loss:6590.833984,acc:0.000000 \n",
      "[train]:step:856,loss:6641.923340,acc:0.000000 \n",
      "[train]:step:857,loss:6785.096191,acc:0.000000 \n",
      "[train]:step:858,loss:6796.225098,acc:0.000000 \n",
      "[train]:step:859,loss:6685.523926,acc:0.000000 \n",
      "[train]:step:860,loss:6672.778320,acc:0.000000 \n",
      "[vaild]:step:860,loss:6541.249023,acc:0.000000 \n",
      "[train]:step:861,loss:6557.512695,acc:0.000000 \n",
      "[train]:step:862,loss:6785.569824,acc:0.000000 \n",
      "[train]:step:863,loss:6911.986328,acc:0.000000 \n",
      "[train]:step:864,loss:6799.799805,acc:0.000000 \n",
      "[train]:step:865,loss:6678.318848,acc:0.000000 \n",
      "[train]:step:866,loss:6752.627930,acc:0.000000 \n",
      "[train]:step:867,loss:6737.473145,acc:0.000000 \n",
      "[train]:step:868,loss:6554.802734,acc:0.000000 \n",
      "[train]:step:869,loss:6783.690430,acc:0.000000 \n",
      "[train]:step:870,loss:6595.032715,acc:0.000000 \n",
      "[vaild]:step:870,loss:7204.887695,acc:0.000000 \n",
      "[train]:step:871,loss:6753.933594,acc:0.000000 \n",
      "[train]:step:872,loss:6853.232422,acc:0.000000 \n",
      "[train]:step:873,loss:6772.557617,acc:0.000000 \n",
      "[train]:step:874,loss:6788.988770,acc:0.000000 \n",
      "[train]:step:875,loss:6664.212891,acc:0.000000 \n",
      "[train]:step:876,loss:6757.267090,acc:0.000000 \n",
      "[train]:step:877,loss:6711.498047,acc:0.000000 \n",
      "[train]:step:878,loss:6706.195801,acc:0.000000 \n",
      "[train]:step:879,loss:6701.816895,acc:0.000000 \n",
      "[train]:step:880,loss:6782.484863,acc:0.000000 \n",
      "[vaild]:step:880,loss:6894.505371,acc:0.000000 \n",
      "[train]:step:881,loss:6673.402344,acc:0.000000 \n",
      "[train]:step:882,loss:6794.330566,acc:0.000000 \n",
      "[train]:step:883,loss:6826.889160,acc:0.000000 \n",
      "[train]:step:884,loss:6785.579590,acc:0.000000 \n",
      "[train]:step:885,loss:6607.924316,acc:0.000000 \n",
      "[train]:step:886,loss:6546.832031,acc:0.000000 \n",
      "[train]:step:887,loss:6778.711426,acc:0.000000 \n",
      "[train]:step:888,loss:6740.818848,acc:0.000000 \n",
      "[train]:step:889,loss:6667.736816,acc:0.000000 \n",
      "[train]:step:890,loss:6753.935059,acc:0.000000 \n",
      "[vaild]:step:890,loss:6620.917480,acc:0.000000 \n",
      "[train]:step:891,loss:6677.357422,acc:0.000000 \n",
      "[train]:step:892,loss:6766.890625,acc:0.000000 \n",
      "[train]:step:893,loss:6743.453613,acc:0.000000 \n",
      "[train]:step:894,loss:6803.260742,acc:0.000000 \n",
      "[train]:step:895,loss:6679.193359,acc:0.000000 \n",
      "[train]:step:896,loss:6727.397949,acc:0.000000 \n",
      "[train]:step:897,loss:6826.993164,acc:0.000000 \n",
      "[train]:step:898,loss:6706.575195,acc:0.000000 \n",
      "[train]:step:899,loss:6644.062500,acc:0.000000 \n",
      "[train]:step:900,loss:6657.222656,acc:0.000000 \n",
      "[vaild]:step:900,loss:6915.944824,acc:0.000000 \n",
      "[train]:step:901,loss:6776.036133,acc:0.000000 \n",
      "[train]:step:902,loss:6585.012695,acc:0.000000 \n",
      "[train]:step:903,loss:6800.416016,acc:0.000000 \n",
      "[train]:step:904,loss:6688.549805,acc:0.000000 \n",
      "[train]:step:905,loss:6754.546875,acc:0.000000 \n",
      "[train]:step:906,loss:6664.189453,acc:0.000000 \n",
      "[train]:step:907,loss:6743.624512,acc:0.000000 \n",
      "[train]:step:908,loss:6657.001465,acc:0.000000 \n",
      "[train]:step:909,loss:6702.342285,acc:0.000000 \n",
      "[train]:step:910,loss:6813.531738,acc:0.000000 \n",
      "[vaild]:step:910,loss:6831.070801,acc:0.000000 \n",
      "[train]:step:911,loss:6807.993652,acc:0.000000 \n",
      "[train]:step:912,loss:6641.775391,acc:0.000000 \n",
      "[train]:step:913,loss:6744.078125,acc:0.000000 \n",
      "[train]:step:914,loss:6594.628906,acc:0.000000 \n",
      "[train]:step:915,loss:6700.706055,acc:0.000000 \n",
      "[train]:step:916,loss:6638.958984,acc:0.000000 \n",
      "[train]:step:917,loss:6806.456055,acc:0.000000 \n",
      "[train]:step:918,loss:6765.205078,acc:0.000000 \n",
      "[train]:step:919,loss:6662.406738,acc:0.000000 \n",
      "[train]:step:920,loss:6712.770020,acc:0.000000 \n",
      "[vaild]:step:920,loss:6726.307617,acc:0.000000 \n",
      "[train]:step:921,loss:6630.005859,acc:0.000000 \n",
      "[train]:step:922,loss:6558.719238,acc:0.000000 \n",
      "[train]:step:923,loss:6750.326172,acc:0.000000 \n",
      "[train]:step:924,loss:6700.035645,acc:0.000000 \n",
      "[train]:step:925,loss:6829.752441,acc:0.000000 \n",
      "[train]:step:926,loss:6507.629395,acc:0.000000 \n",
      "[train]:step:927,loss:6766.766113,acc:0.000000 \n",
      "[train]:step:928,loss:6723.503906,acc:0.000000 \n",
      "[train]:step:929,loss:6773.287109,acc:0.000000 \n",
      "[train]:step:930,loss:6850.059570,acc:0.000000 \n",
      "[vaild]:step:930,loss:6650.527832,acc:0.000000 \n",
      "[train]:step:931,loss:6591.310547,acc:0.000000 \n",
      "[train]:step:932,loss:6575.363281,acc:0.000000 \n",
      "[train]:step:933,loss:6589.977539,acc:0.000000 \n",
      "[train]:step:934,loss:6803.884277,acc:0.000000 \n",
      "[train]:step:935,loss:6710.944824,acc:0.000000 \n",
      "[train]:step:936,loss:6663.098145,acc:0.000000 \n",
      "[train]:step:937,loss:6725.993164,acc:0.000000 \n",
      "[train]:step:938,loss:6664.946289,acc:0.000000 \n",
      "[train]:step:939,loss:6793.987305,acc:0.000000 \n",
      "[train]:step:940,loss:6719.767578,acc:0.000000 \n",
      "[vaild]:step:940,loss:6854.438965,acc:0.000000 \n",
      "[train]:step:941,loss:6738.996094,acc:0.000000 \n",
      "[train]:step:942,loss:6772.569336,acc:0.000000 \n",
      "[train]:step:943,loss:6879.395508,acc:0.000000 \n",
      "[train]:step:944,loss:6825.486328,acc:0.000000 \n",
      "[train]:step:945,loss:6718.787109,acc:0.000000 \n",
      "[train]:step:946,loss:6710.852539,acc:0.000000 \n",
      "[train]:step:947,loss:6747.984863,acc:0.000000 \n",
      "[train]:step:948,loss:6664.196289,acc:0.000000 \n",
      "[train]:step:949,loss:6729.177734,acc:0.000000 \n",
      "[train]:step:950,loss:6757.846680,acc:0.000000 \n",
      "[vaild]:step:950,loss:6919.724609,acc:0.000000 \n",
      "[train]:step:951,loss:6664.631348,acc:0.000000 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train]:step:952,loss:6759.145508,acc:0.000000 \n",
      "[train]:step:953,loss:6777.806641,acc:0.000000 \n",
      "[train]:step:954,loss:6735.985840,acc:0.000000 \n",
      "[train]:step:955,loss:6640.732910,acc:0.000000 \n",
      "[train]:step:956,loss:6616.945801,acc:0.000000 \n",
      "[train]:step:957,loss:6839.763672,acc:0.000000 \n",
      "[train]:step:958,loss:6583.359375,acc:0.000000 \n",
      "[train]:step:959,loss:6685.481445,acc:0.000000 \n",
      "[train]:step:960,loss:6583.162109,acc:0.000000 \n",
      "[vaild]:step:960,loss:6772.495117,acc:0.000000 \n",
      "[train]:step:961,loss:6562.363770,acc:0.000000 \n",
      "[train]:step:962,loss:6770.750000,acc:0.000000 \n",
      "[train]:step:963,loss:6695.131836,acc:0.000000 \n",
      "[train]:step:964,loss:6926.333008,acc:0.000000 \n",
      "[train]:step:965,loss:6670.440430,acc:0.000000 \n",
      "[train]:step:966,loss:6676.659180,acc:0.000000 \n",
      "[train]:step:967,loss:6681.212402,acc:0.000000 \n",
      "[train]:step:968,loss:6713.924316,acc:0.000000 \n",
      "[train]:step:969,loss:6849.804199,acc:0.000000 \n",
      "[train]:step:970,loss:6633.055176,acc:0.000000 \n",
      "[vaild]:step:970,loss:6666.415039,acc:0.000000 \n",
      "[train]:step:971,loss:6941.666992,acc:0.000000 \n",
      "[train]:step:972,loss:6522.630859,acc:0.000000 \n",
      "[train]:step:973,loss:6676.131836,acc:0.000000 \n",
      "[train]:step:974,loss:6660.822266,acc:0.000000 \n",
      "[train]:step:975,loss:6624.042969,acc:0.000000 \n",
      "[train]:step:976,loss:6737.715820,acc:0.000000 \n",
      "[train]:step:977,loss:6749.122559,acc:0.000000 \n",
      "[train]:step:978,loss:6793.417969,acc:0.000000 \n",
      "[train]:step:979,loss:6687.640137,acc:0.000000 \n",
      "[train]:step:980,loss:6671.238770,acc:0.000000 \n",
      "[vaild]:step:980,loss:6836.960938,acc:0.000000 \n",
      "[train]:step:981,loss:6647.413574,acc:0.000000 \n",
      "[train]:step:982,loss:6651.633301,acc:0.000000 \n",
      "[train]:step:983,loss:6751.975586,acc:0.000000 \n",
      "[train]:step:984,loss:6765.729980,acc:0.000000 \n",
      "[train]:step:985,loss:6597.339844,acc:0.000000 \n",
      "[train]:step:986,loss:6698.061035,acc:0.000000 \n",
      "[train]:step:987,loss:6567.852051,acc:0.000000 \n",
      "[train]:step:988,loss:6801.670410,acc:0.000000 \n",
      "[train]:step:989,loss:6736.195801,acc:0.000000 \n",
      "[train]:step:990,loss:6739.214355,acc:0.000000 \n",
      "[vaild]:step:990,loss:6904.428711,acc:0.000000 \n",
      "[train]:step:991,loss:6605.464355,acc:0.000000 \n",
      "[train]:step:992,loss:6697.390625,acc:0.000000 \n",
      "[train]:step:993,loss:6648.372070,acc:0.000000 \n",
      "[train]:step:994,loss:6845.569336,acc:0.000000 \n",
      "[train]:step:995,loss:6634.290039,acc:0.000000 \n",
      "[train]:step:996,loss:6573.365234,acc:0.000000 \n",
      "[train]:step:997,loss:6688.810059,acc:0.000000 \n",
      "[train]:step:998,loss:6740.184570,acc:0.000000 \n",
      "[train]:step:999,loss:6661.938965,acc:0.000000 \n",
      "[train]:step:1000,loss:6685.928711,acc:0.000000 \n",
      "[vaild]:step:1000,loss:6660.009766,acc:0.000000 \n",
      "[train]:step:1001,loss:6769.691406,acc:0.000000 \n",
      "[train]:step:1002,loss:6775.037109,acc:0.000000 \n",
      "[train]:step:1003,loss:6757.382324,acc:0.000000 \n",
      "[train]:step:1004,loss:6634.758301,acc:0.000000 \n",
      "[train]:step:1005,loss:6612.191895,acc:0.000000 \n",
      "[train]:step:1006,loss:6744.041016,acc:0.000000 \n",
      "[train]:step:1007,loss:6724.461426,acc:0.000000 \n",
      "[train]:step:1008,loss:6651.707031,acc:0.000000 \n",
      "[train]:step:1009,loss:6761.607910,acc:0.000000 \n",
      "[train]:step:1010,loss:6772.602051,acc:0.000000 \n",
      "[vaild]:step:1010,loss:6792.268555,acc:0.000000 \n",
      "[train]:step:1011,loss:6730.041992,acc:0.000000 \n",
      "[train]:step:1012,loss:6647.138672,acc:0.000000 \n",
      "[train]:step:1013,loss:6766.048828,acc:0.000000 \n",
      "[train]:step:1014,loss:6589.188965,acc:0.000000 \n",
      "[train]:step:1015,loss:6652.671875,acc:0.000000 \n",
      "[train]:step:1016,loss:6669.740723,acc:0.000000 \n",
      "[train]:step:1017,loss:6631.714355,acc:0.000000 \n",
      "[train]:step:1018,loss:6504.648926,acc:0.000000 \n",
      "[train]:step:1019,loss:6643.330078,acc:0.000000 \n",
      "[train]:step:1020,loss:6750.725098,acc:0.000000 \n",
      "[vaild]:step:1020,loss:6998.675781,acc:0.000000 \n",
      "[train]:step:1021,loss:6725.534180,acc:0.000000 \n",
      "[train]:step:1022,loss:6836.107910,acc:0.000000 \n",
      "[train]:step:1023,loss:6570.117676,acc:0.000000 \n",
      "[train]:step:1024,loss:6822.041016,acc:0.000000 \n",
      "[train]:step:1025,loss:6777.179199,acc:0.000000 \n",
      "[train]:step:1026,loss:6683.759766,acc:0.000000 \n",
      "[train]:step:1027,loss:6759.613281,acc:0.000000 \n",
      "[train]:step:1028,loss:6731.366211,acc:0.000000 \n",
      "[train]:step:1029,loss:6719.051758,acc:0.000000 \n",
      "[train]:step:1030,loss:6853.667480,acc:0.000000 \n",
      "[vaild]:step:1030,loss:6568.438965,acc:0.000000 \n",
      "[train]:step:1031,loss:6629.458008,acc:0.000000 \n",
      "[train]:step:1032,loss:6682.433594,acc:0.000000 \n",
      "[train]:step:1033,loss:6744.979980,acc:0.000000 \n",
      "[train]:step:1034,loss:6757.384277,acc:0.000000 \n",
      "[train]:step:1035,loss:6864.176270,acc:0.000000 \n",
      "[train]:step:1036,loss:6671.278320,acc:0.000000 \n",
      "[train]:step:1037,loss:6763.861816,acc:0.000000 \n",
      "[train]:step:1038,loss:6762.306152,acc:0.000000 \n",
      "[train]:step:1039,loss:6606.260742,acc:0.000000 \n",
      "[train]:step:1040,loss:6687.426270,acc:0.000000 \n",
      "[vaild]:step:1040,loss:6671.658203,acc:0.000000 \n",
      "[train]:step:1041,loss:6700.558105,acc:0.000000 \n",
      "[train]:step:1042,loss:6618.790527,acc:0.000000 \n",
      "[train]:step:1043,loss:6628.120605,acc:0.000000 \n",
      "[train]:step:1044,loss:6747.200195,acc:0.000000 \n",
      "[train]:step:1045,loss:6768.504395,acc:0.000000 \n",
      "[train]:step:1046,loss:6900.363281,acc:0.000000 \n",
      "[train]:step:1047,loss:6600.488281,acc:0.000000 \n",
      "[train]:step:1048,loss:6791.304199,acc:0.000000 \n",
      "[train]:step:1049,loss:6681.135742,acc:0.000000 \n",
      "[train]:step:1050,loss:6758.167969,acc:0.000000 \n",
      "[vaild]:step:1050,loss:6366.798828,acc:0.000000 \n",
      "[train]:step:1051,loss:6644.106934,acc:0.000000 \n",
      "[train]:step:1052,loss:6640.270508,acc:0.000000 \n",
      "[train]:step:1053,loss:6689.867676,acc:0.000000 \n",
      "[train]:step:1054,loss:6745.406738,acc:0.000000 \n",
      "[train]:step:1055,loss:6784.040527,acc:0.000000 \n",
      "[train]:step:1056,loss:6679.204590,acc:0.000000 \n",
      "[train]:step:1057,loss:6798.606934,acc:0.000000 \n",
      "[train]:step:1058,loss:6721.476074,acc:0.000000 \n",
      "[train]:step:1059,loss:6723.478027,acc:0.000000 \n",
      "[train]:step:1060,loss:6741.099609,acc:0.000000 \n",
      "[vaild]:step:1060,loss:7180.452148,acc:0.000000 \n",
      "[train]:step:1061,loss:6756.100586,acc:0.000000 \n",
      "[train]:step:1062,loss:6660.494141,acc:0.000000 \n",
      "[train]:step:1063,loss:6689.448242,acc:0.000000 \n",
      "[train]:step:1064,loss:6649.053223,acc:0.000000 \n",
      "[train]:step:1065,loss:6781.946777,acc:0.000000 \n",
      "[train]:step:1066,loss:6596.153809,acc:0.000000 \n",
      "[train]:step:1067,loss:6776.377930,acc:0.000000 \n",
      "[train]:step:1068,loss:6635.864258,acc:0.000000 \n",
      "[train]:step:1069,loss:6871.868164,acc:0.000000 \n",
      "[train]:step:1070,loss:6642.602539,acc:0.000000 \n",
      "[vaild]:step:1070,loss:6801.413086,acc:0.000000 \n",
      "[train]:step:1071,loss:6634.575195,acc:0.000000 \n",
      "[train]:step:1072,loss:6666.348145,acc:0.000000 \n",
      "[train]:step:1073,loss:6695.596191,acc:0.000000 \n",
      "[train]:step:1074,loss:6865.334961,acc:0.000000 \n",
      "[train]:step:1075,loss:6651.660156,acc:0.000000 \n",
      "[train]:step:1076,loss:6763.708984,acc:0.000000 \n",
      "[train]:step:1077,loss:6795.079590,acc:0.000000 \n",
      "[train]:step:1078,loss:6586.277344,acc:0.000000 \n",
      "[train]:step:1079,loss:6720.162109,acc:0.000000 \n",
      "[train]:step:1080,loss:6598.014160,acc:0.000000 \n",
      "[vaild]:step:1080,loss:6547.326172,acc:0.000000 \n",
      "[train]:step:1081,loss:6794.354492,acc:0.000000 \n",
      "[train]:step:1082,loss:6790.188965,acc:0.000000 \n",
      "[train]:step:1083,loss:6547.614258,acc:0.000000 \n",
      "[train]:step:1084,loss:6707.187988,acc:0.000000 \n",
      "[train]:step:1085,loss:6768.355469,acc:0.000000 \n",
      "[train]:step:1086,loss:6650.881348,acc:0.000000 \n",
      "[train]:step:1087,loss:6807.647461,acc:0.000000 \n",
      "[train]:step:1088,loss:6631.934570,acc:0.000000 \n",
      "[train]:step:1089,loss:6592.582031,acc:0.000000 \n",
      "[train]:step:1090,loss:6855.425781,acc:0.000000 \n",
      "[vaild]:step:1090,loss:6534.979492,acc:0.000000 \n",
      "[train]:step:1091,loss:6706.638672,acc:0.000000 \n",
      "[train]:step:1092,loss:6660.881348,acc:0.000000 \n",
      "[train]:step:1093,loss:6755.685059,acc:0.000000 \n",
      "[train]:step:1094,loss:6797.693359,acc:0.000000 \n",
      "[train]:step:1095,loss:6734.776367,acc:0.000000 \n",
      "[train]:step:1096,loss:6793.955078,acc:0.000000 \n",
      "[train]:step:1097,loss:6676.243652,acc:0.000000 \n",
      "[train]:step:1098,loss:6664.521973,acc:0.000000 \n",
      "[train]:step:1099,loss:6759.593262,acc:0.000000 \n",
      "[train]:step:1100,loss:6713.423828,acc:0.000000 \n",
      "[vaild]:step:1100,loss:6992.652344,acc:0.000000 \n",
      "[train]:step:1101,loss:6690.126465,acc:0.000000 \n",
      "[train]:step:1102,loss:6701.669434,acc:0.000000 \n",
      "[train]:step:1103,loss:6664.523926,acc:0.000000 \n",
      "[train]:step:1104,loss:6660.126953,acc:0.000000 \n",
      "[train]:step:1105,loss:6643.473633,acc:0.000000 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train]:step:1106,loss:6796.032715,acc:0.000000 \n",
      "[train]:step:1107,loss:6717.923340,acc:0.000000 \n",
      "[train]:step:1108,loss:6642.806641,acc:0.000000 \n",
      "[train]:step:1109,loss:6665.986816,acc:0.000000 \n",
      "[train]:step:1110,loss:6669.342285,acc:0.000000 \n",
      "[vaild]:step:1110,loss:6837.288086,acc:0.000000 \n",
      "[train]:step:1111,loss:6619.461914,acc:0.000000 \n",
      "[train]:step:1112,loss:6742.971680,acc:0.000000 \n",
      "[train]:step:1113,loss:6615.973145,acc:0.000000 \n",
      "[train]:step:1114,loss:6760.841309,acc:0.000000 \n",
      "[train]:step:1115,loss:6669.127441,acc:0.000000 \n",
      "[train]:step:1116,loss:6653.422363,acc:0.000000 \n",
      "[train]:step:1117,loss:6499.453125,acc:0.000000 \n",
      "[train]:step:1118,loss:6745.298828,acc:0.000000 \n",
      "[train]:step:1119,loss:6643.191895,acc:0.000000 \n",
      "[train]:step:1120,loss:6665.521973,acc:0.000000 \n",
      "[vaild]:step:1120,loss:6585.693359,acc:0.000000 \n",
      "[train]:step:1121,loss:6735.638672,acc:0.000000 \n",
      "[train]:step:1122,loss:6857.164551,acc:0.000000 \n",
      "[train]:step:1123,loss:6638.628906,acc:0.000000 \n",
      "[train]:step:1124,loss:6681.229492,acc:0.000000 \n",
      "[train]:step:1125,loss:6728.507324,acc:0.000000 \n",
      "[train]:step:1126,loss:6623.136719,acc:0.000000 \n",
      "[train]:step:1127,loss:6666.065430,acc:0.000000 \n",
      "[train]:step:1128,loss:6655.443359,acc:0.000000 \n",
      "[train]:step:1129,loss:6659.291992,acc:0.000000 \n",
      "[train]:step:1130,loss:6861.281738,acc:0.000000 \n",
      "[vaild]:step:1130,loss:6774.084961,acc:0.000000 \n",
      "[train]:step:1131,loss:6751.309570,acc:0.000000 \n",
      "[train]:step:1132,loss:6790.758301,acc:0.000000 \n",
      "[train]:step:1133,loss:6728.002441,acc:0.000000 \n",
      "[train]:step:1134,loss:6676.194336,acc:0.000000 \n",
      "[train]:step:1135,loss:6864.146973,acc:0.000000 \n",
      "[train]:step:1136,loss:6747.775391,acc:0.000000 \n",
      "[train]:step:1137,loss:6727.757324,acc:0.000000 \n",
      "[train]:step:1138,loss:6850.731934,acc:0.000000 \n",
      "[train]:step:1139,loss:6674.214844,acc:0.000000 \n",
      "[train]:step:1140,loss:6926.068848,acc:0.000000 \n",
      "[vaild]:step:1140,loss:6587.836914,acc:0.000000 \n",
      "[train]:step:1141,loss:6702.762695,acc:0.000000 \n",
      "[train]:step:1142,loss:6720.700195,acc:0.000000 \n",
      "[train]:step:1143,loss:6821.784180,acc:0.000000 \n",
      "[train]:step:1144,loss:6819.177734,acc:0.000000 \n",
      "[train]:step:1145,loss:6707.845703,acc:0.000000 \n",
      "[train]:step:1146,loss:6744.492676,acc:0.000000 \n",
      "[train]:step:1147,loss:6745.603516,acc:0.000000 \n",
      "[train]:step:1148,loss:6728.037598,acc:0.000000 \n",
      "[train]:step:1149,loss:6768.023926,acc:0.000000 \n",
      "[train]:step:1150,loss:6730.778809,acc:0.000000 \n",
      "[vaild]:step:1150,loss:6781.627930,acc:0.000000 \n",
      "[train]:step:1151,loss:6743.403320,acc:0.000000 \n",
      "[train]:step:1152,loss:6560.986816,acc:0.000000 \n",
      "[train]:step:1153,loss:6707.732422,acc:0.000000 \n",
      "[train]:step:1154,loss:6784.240234,acc:0.000000 \n",
      "[train]:step:1155,loss:6723.542969,acc:0.000000 \n",
      "[train]:step:1156,loss:6649.000488,acc:0.000000 \n",
      "[train]:step:1157,loss:6686.897949,acc:0.000000 \n",
      "[train]:step:1158,loss:6787.571777,acc:0.000000 \n",
      "[train]:step:1159,loss:6810.014160,acc:0.000000 \n",
      "[train]:step:1160,loss:6723.852539,acc:0.000000 \n",
      "[vaild]:step:1160,loss:6425.243652,acc:0.000000 \n",
      "[train]:step:1161,loss:6639.028320,acc:0.000000 \n",
      "[train]:step:1162,loss:6742.554199,acc:0.000000 \n",
      "[train]:step:1163,loss:6632.319336,acc:0.000000 \n",
      "[train]:step:1164,loss:6705.042969,acc:0.000000 \n",
      "[train]:step:1165,loss:6960.444824,acc:0.000000 \n",
      "[train]:step:1166,loss:6812.407715,acc:0.000000 \n",
      "[train]:step:1167,loss:6678.957520,acc:0.000000 \n",
      "[train]:step:1168,loss:6747.790039,acc:0.000000 \n",
      "[train]:step:1169,loss:6610.268066,acc:0.000000 \n",
      "[train]:step:1170,loss:6681.968750,acc:0.000000 \n",
      "[vaild]:step:1170,loss:6910.771484,acc:0.000000 \n",
      "[train]:step:1171,loss:6545.874512,acc:0.000000 \n",
      "[train]:step:1172,loss:6633.758789,acc:0.000000 \n",
      "[train]:step:1173,loss:6795.037598,acc:0.000000 \n",
      "[train]:step:1174,loss:6762.415039,acc:0.000000 \n",
      "[train]:step:1175,loss:6664.901367,acc:0.000000 \n",
      "[train]:step:1176,loss:6756.991211,acc:0.000000 \n",
      "[train]:step:1177,loss:6659.774902,acc:0.000000 \n",
      "[train]:step:1178,loss:6800.409180,acc:0.000000 \n",
      "[train]:step:1179,loss:6673.483887,acc:0.000000 \n",
      "[train]:step:1180,loss:6571.551270,acc:0.000000 \n",
      "[vaild]:step:1180,loss:6867.485352,acc:0.000000 \n",
      "[train]:step:1181,loss:6553.298828,acc:0.000000 \n",
      "[train]:step:1182,loss:6753.732910,acc:0.000000 \n",
      "[train]:step:1183,loss:6782.587891,acc:0.000000 \n",
      "[train]:step:1184,loss:6866.808105,acc:0.000000 \n",
      "[train]:step:1185,loss:6682.437988,acc:0.000000 \n",
      "[train]:step:1186,loss:6743.251953,acc:0.000000 \n",
      "[train]:step:1187,loss:6704.394531,acc:0.000000 \n",
      "[train]:step:1188,loss:6749.430176,acc:0.000000 \n",
      "[train]:step:1189,loss:6838.513672,acc:0.000000 \n",
      "[train]:step:1190,loss:6752.383301,acc:0.000000 \n",
      "[vaild]:step:1190,loss:6836.979492,acc:0.000000 \n",
      "[train]:step:1191,loss:6611.128906,acc:0.000000 \n",
      "[train]:step:1192,loss:6800.917480,acc:0.000000 \n",
      "[train]:step:1193,loss:6623.759766,acc:0.000000 \n",
      "[train]:step:1194,loss:6766.595215,acc:0.000000 \n",
      "[train]:step:1195,loss:6649.103516,acc:0.000000 \n",
      "[train]:step:1196,loss:6638.975586,acc:0.000000 \n",
      "[train]:step:1197,loss:6744.602539,acc:0.000000 \n",
      "[train]:step:1198,loss:6595.475586,acc:0.000000 \n",
      "[train]:step:1199,loss:6734.797363,acc:0.000000 \n",
      "[train]:step:1200,loss:6796.975098,acc:0.000000 \n",
      "[vaild]:step:1200,loss:6582.093262,acc:0.000000 \n",
      "[train]:step:1201,loss:6617.977051,acc:0.000000 \n",
      "[train]:step:1202,loss:6646.069824,acc:0.000000 \n",
      "[train]:step:1203,loss:6718.752930,acc:0.000000 \n",
      "[train]:step:1204,loss:6864.667969,acc:0.000000 \n",
      "[train]:step:1205,loss:6702.318848,acc:0.000000 \n",
      "[train]:step:1206,loss:6642.373535,acc:0.000000 \n",
      "[train]:step:1207,loss:6621.680664,acc:0.000000 \n",
      "[train]:step:1208,loss:6749.537598,acc:0.000000 \n",
      "[train]:step:1209,loss:6800.507324,acc:0.000000 \n",
      "[train]:step:1210,loss:6528.020020,acc:0.000000 \n",
      "[vaild]:step:1210,loss:6593.382812,acc:0.000000 \n",
      "[train]:step:1211,loss:6716.002441,acc:0.000000 \n",
      "[train]:step:1212,loss:6803.695801,acc:0.000000 \n",
      "[train]:step:1213,loss:6837.400391,acc:0.000000 \n",
      "[train]:step:1214,loss:6721.135742,acc:0.000000 \n",
      "[train]:step:1215,loss:6524.307617,acc:0.000000 \n",
      "[train]:step:1216,loss:6737.126953,acc:0.000000 \n",
      "[train]:step:1217,loss:6766.368652,acc:0.000000 \n",
      "[train]:step:1218,loss:6683.843262,acc:0.000000 \n",
      "[train]:step:1219,loss:6625.220215,acc:0.000000 \n",
      "[train]:step:1220,loss:6780.274902,acc:0.000000 \n",
      "[vaild]:step:1220,loss:6533.842285,acc:0.000000 \n",
      "[train]:step:1221,loss:6673.192383,acc:0.000000 \n",
      "[train]:step:1222,loss:6664.271484,acc:0.000000 \n",
      "[train]:step:1223,loss:6705.962402,acc:0.000000 \n",
      "[train]:step:1224,loss:6862.642090,acc:0.000000 \n",
      "[train]:step:1225,loss:6786.211914,acc:0.000000 \n",
      "[train]:step:1226,loss:6623.868652,acc:0.000000 \n",
      "[train]:step:1227,loss:6712.466797,acc:0.000000 \n",
      "[train]:step:1228,loss:6609.200684,acc:0.000000 \n",
      "[train]:step:1229,loss:6688.471680,acc:0.000000 \n",
      "[train]:step:1230,loss:6651.090820,acc:0.000000 \n",
      "[vaild]:step:1230,loss:6683.194336,acc:0.000000 \n",
      "[train]:step:1231,loss:6843.705078,acc:0.000000 \n",
      "[train]:step:1232,loss:6889.026855,acc:0.000000 \n",
      "[train]:step:1233,loss:6677.078125,acc:0.000000 \n",
      "[train]:step:1234,loss:6699.200195,acc:0.000000 \n",
      "[train]:step:1235,loss:6571.634766,acc:0.000000 \n",
      "[train]:step:1236,loss:6926.669434,acc:0.000000 \n",
      "[train]:step:1237,loss:6706.839355,acc:0.000000 \n",
      "[train]:step:1238,loss:6971.163086,acc:0.000000 \n",
      "[train]:step:1239,loss:6596.354492,acc:0.000000 \n",
      "[train]:step:1240,loss:6713.713867,acc:0.000000 \n",
      "[vaild]:step:1240,loss:7140.797852,acc:0.000000 \n",
      "[train]:step:1241,loss:6670.417969,acc:0.000000 \n",
      "[train]:step:1242,loss:6713.288086,acc:0.000000 \n",
      "[train]:step:1243,loss:6744.793945,acc:0.000000 \n",
      "[train]:step:1244,loss:6826.062988,acc:0.000000 \n",
      "[train]:step:1245,loss:6719.102539,acc:0.000000 \n",
      "[train]:step:1246,loss:6674.616699,acc:0.000000 \n",
      "[train]:step:1247,loss:6620.775391,acc:0.000000 \n",
      "[train]:step:1248,loss:6821.573730,acc:0.000000 \n",
      "[train]:step:1249,loss:6719.958008,acc:0.000000 \n",
      "[train]:step:1250,loss:6514.907715,acc:0.000000 \n",
      "[vaild]:step:1250,loss:6958.119141,acc:0.000000 \n",
      "[train]:step:1251,loss:6665.439453,acc:0.000000 \n",
      "[train]:step:1252,loss:6525.501465,acc:0.000000 \n",
      "[train]:step:1253,loss:6556.371094,acc:0.000000 \n",
      "[train]:step:1254,loss:6574.292480,acc:0.000000 \n",
      "[train]:step:1255,loss:6732.187500,acc:0.000000 \n",
      "[train]:step:1256,loss:6791.222656,acc:0.000000 \n",
      "[train]:step:1257,loss:6666.623535,acc:0.000000 \n",
      "[train]:step:1258,loss:6748.441406,acc:0.000000 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train]:step:1259,loss:6715.054199,acc:0.000000 \n",
      "[train]:step:1260,loss:6642.930664,acc:0.000000 \n",
      "[vaild]:step:1260,loss:6701.936035,acc:0.000000 \n",
      "[train]:step:1261,loss:6584.604492,acc:0.000000 \n",
      "[train]:step:1262,loss:6730.554199,acc:0.000000 \n",
      "[train]:step:1263,loss:6708.845703,acc:0.000000 \n",
      "[train]:step:1264,loss:6747.136719,acc:0.000000 \n",
      "[train]:step:1265,loss:6801.995117,acc:0.000000 \n",
      "[train]:step:1266,loss:6784.716797,acc:0.000000 \n",
      "[train]:step:1267,loss:6614.091309,acc:0.000000 \n",
      "[train]:step:1268,loss:6720.965820,acc:0.000000 \n",
      "[train]:step:1269,loss:6728.810059,acc:0.000000 \n",
      "[train]:step:1270,loss:6704.147461,acc:0.000000 \n",
      "[vaild]:step:1270,loss:6986.259766,acc:0.000000 \n",
      "[train]:step:1271,loss:6711.755859,acc:0.000000 \n",
      "[train]:step:1272,loss:6883.037109,acc:0.000000 \n",
      "[train]:step:1273,loss:6823.922363,acc:0.000000 \n",
      "[train]:step:1274,loss:6795.012695,acc:0.000000 \n",
      "[train]:step:1275,loss:6689.082520,acc:0.000000 \n",
      "[train]:step:1276,loss:6767.732422,acc:0.000000 \n",
      "[train]:step:1277,loss:6770.968750,acc:0.000000 \n",
      "[train]:step:1278,loss:6638.826172,acc:0.000000 \n",
      "[train]:step:1279,loss:6683.687012,acc:0.000000 \n",
      "[train]:step:1280,loss:6792.659180,acc:0.000000 \n",
      "[vaild]:step:1280,loss:6315.604980,acc:0.000000 \n",
      "[train]:step:1281,loss:6808.958984,acc:0.000000 \n",
      "[train]:step:1282,loss:6679.850098,acc:0.000000 \n",
      "[train]:step:1283,loss:6667.116211,acc:0.000000 \n",
      "[train]:step:1284,loss:6741.296387,acc:0.010000 \n",
      "[train]:step:1285,loss:6622.068359,acc:0.010000 \n",
      "[train]:step:1286,loss:6790.722656,acc:0.000000 \n",
      "[train]:step:1287,loss:6667.369141,acc:0.000000 \n",
      "[train]:step:1288,loss:6566.782715,acc:0.000000 \n",
      "[train]:step:1289,loss:6718.216309,acc:0.000000 \n",
      "[train]:step:1290,loss:6649.687988,acc:0.000000 \n",
      "[vaild]:step:1290,loss:6557.589844,acc:0.000000 \n",
      "[train]:step:1291,loss:6575.727539,acc:0.000000 \n",
      "[train]:step:1292,loss:6813.986816,acc:0.000000 \n",
      "[train]:step:1293,loss:6756.327637,acc:0.000000 \n",
      "[train]:step:1294,loss:6606.748535,acc:0.000000 \n",
      "[train]:step:1295,loss:6775.196777,acc:0.000000 \n",
      "[tr