{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.activations import relu\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解决keras 显存问题\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.25\n",
    "set_session(tf.Session(config=config)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = 10\n",
    "input_dim = 1\n",
    "MAX_LENGTH = 300 # 单个字符的长度\n",
    "output_length = 10\n",
    "output_dim = 6\n",
    "\n",
    "samples = 100\n",
    "hidden_dim = 6\n",
    "hidden_size = 300 # a half of Max_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_size = [embedding_length,embedding_dim,input_length]\n",
    "embedding_size = [1000,64,300]\n",
    "# hidden_size is the length of the kernel utils  = enc_units\n",
    "class EncoderRNN(Model):\n",
    "    def __init__(self, embedding_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = Embedding(embedding_size[0], embedding_size[1],input_length=embedding_size[2])\n",
    "        self.gru = GRU(hidden_size, return_sequences=True, return_state=True)\n",
    "        self.deepgru = GRU(hidden_size*2, return_sequences=True, return_state=True)\n",
    "        self.lastdeepgru = GRU(hidden_size*2, return_sequences=False, return_state=True)\n",
    "        self.bigru = Bidirectional(self.gru)\n",
    "\n",
    "    def __call__(self, en_input):\n",
    "        state_h = []\n",
    "        emb = self.embedding(en_inputs)\n",
    "        encoder_out, fwd_h1, bck_h1 = self.bigru(emb)\n",
    "        state_h.append(concatenate([fwd_h1, bck_h1]))\n",
    "        if hidden_dim>1:\n",
    "            for i in range(1,hidden_dim):\n",
    "                encoder_out, en_hidden = self.deepgru(encoder_out)\n",
    "                state_h.append(en_hidden)\n",
    "        output = encoder_out\n",
    "        hidden = state_h\n",
    "        return output, hidden\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        init_state = [tf.zeros((1, self.hidden_size)) for i in range(2)]\n",
    "        return init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(Model):\n",
    "    def __init__(self, embedding_size, hidden_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = Embedding(embedding_size[0], embedding_size[1],input_length=embedding_size[2])\n",
    "        self.gru = GRU(hidden_size, return_sequences=True, return_state=True)\n",
    "        self.deepgru = GRU(hidden_size*2, return_sequences=True, return_state=True)\n",
    "        self.lastdeepgru = GRU(hidden_size*2, return_sequences=False, return_state=True)\n",
    "        self.bigru = Bidirectional(self.gru)\n",
    "        \n",
    "        self.out = Dense(output_length)\n",
    "        self.softmax = Activation('softmax')\n",
    "\n",
    "    def __call__(self, de_input, hiddens):\n",
    "        state_h = []\n",
    "        emb = self.embedding(en_inputs)\n",
    "        emb = Activation('relu')(emb)\n",
    "        # !挖个坑，这里没有初始化hidden_state\n",
    "        output, fwd_h, bck_h = self.bigru(emb)\n",
    "        state_h.append(concatenate([fwd_h, bck_h]))\n",
    "        if len(hiddens)>2:\n",
    "            for hidden in hiddens[1:-1]:\n",
    "                output, de_hidden = self.deepgru(output, hidden)\n",
    "                state_h.append(de_hidden)\n",
    "        if len(hiddens)>1:\n",
    "            output, de_hidden = self.lastdeepgru(output, hidden)\n",
    "            state_h.append(de_hidden)\n",
    "        hidden = state_h\n",
    "        output = self.softmax(output)\n",
    "        print(type(output))\n",
    "        return output, hidden\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        init_state = tf.zeros((1, self.hidden_size))\n",
    "        return init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttnDecoderRNN(Model):\n",
    "    def __init__(self, embedding_size ,hidden_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = Embedding(embedding_size[0], embedding_size[1],input_length=embedding_size[2],name ='atten_embed')\n",
    "        self.attn = Dense(self.max_length,name = 'atten_attn')\n",
    "        self.attn_combine = Dense(self.hidden_size,name = 'atten_combine')\n",
    "        self.dropout = Dropout(self.dropout_p,name ='atten_dropout')\n",
    "        \n",
    "        self.gru = GRU(hidden_size, return_sequences=True, return_state=True,name = 'atten_gru')\n",
    "        self.deepgru = GRU(hidden_size*2, return_sequences=True, return_state=True,name = 'atten_deepgru')\n",
    "        self.lastdeepgru = GRU(hidden_size*2, return_sequences=False, return_state=True,name='atten_lastdeepgru')\n",
    "        self.bigru = Bidirectional(self.gru,name='atten_bigru')\n",
    "        self.batch_dot = Lambda(lambda layers:K.batch_dot(layers[0],layers[1]))\n",
    "        self.out = Dense(output_length)\n",
    "        self.softmax =Softmax(axis=-1)\n",
    "        #包装层\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def __call__(self, de_input, hiddens, encoder_outputs):\n",
    "        state_h = []\n",
    "        embedded = self.embedding(de_input)\n",
    "        print(embedded.shape)\n",
    "        # TODO ; use lambda https://www.cnblogs.com/jqpy1994/p/11433746.html or  https://keras.io/zh/layers/core/\n",
    "        #embedded = K.reshape(embedded,[samples,embedding_size[1]*embedding_size[2]])\n",
    "        embedded = Reshape((1,embedding_size[1]*embedding_size[2]))(embedded)\n",
    "        print(type(embedded))\n",
    "        embedded = Lambda(lambda x:K.squeeze(x,1))(embedded)\n",
    "    \n",
    "        embedded = self.dropout(embedded) # Dim:(Batch Size , Decoder Hidden Size + Embedding Size)\n",
    "        if type(hiddens) == type(list()):\n",
    "            hidden = hiddens[-1]\n",
    "        \n",
    "        # hidden = K.reshape(hidden,[samples,self.hidden_size*2]) \n",
    "        hidden = Reshape((1,self.hidden_size*2))(hidden)\n",
    "        hidden = Lambda(lambda x:K.squeeze(x,1))(hidden)\n",
    "        concat = Concatenate(1,name='atten_concat2')([embedded, hidden])\n",
    "        # note: 从这里开始，把两个向量拼接起来    \n",
    "        attn_weights =self.softmax(\n",
    "            Dense(self.max_length)(concat))\n",
    "        atten_weights = Reshape((1,-1))(attn_weights)\n",
    "        attn_applied = self.batch_dot([atten_weights,encoder_outputs])\n",
    "        print(attn_applied.shape)\n",
    "        attn_applied = Lambda(lambda x:K.squeeze(x,1))(attn_applied)\n",
    "\n",
    "        \n",
    "        output = Concatenate(1)([embedded, attn_applied])\n",
    "\n",
    "        output = self.attn_combine(output)\n",
    "        \n",
    "        output = ReLU()(output)\n",
    "        output = Reshape((1,-1))(output)\n",
    "        output, fwd_h, bck_h = self.bigru(output)\n",
    "        state_h.append(concatenate([fwd_h, bck_h]))\n",
    "        if len(hiddens)>2:\n",
    "            for hidden in hiddens[1:-1]:\n",
    "                output, de_hidden = self.deepgru(output, hidden)\n",
    "                state_h.append(de_hidden)\n",
    "        if len(hiddens)>1:\n",
    "            output, de_hidden = self.lastdeepgru(output, hidden)\n",
    "            state_h.append(de_hidden)\n",
    "        hidden = state_h\n",
    "#         output = self.softmax(output)\n",
    "        output = Reshape((1,-1))(output)\n",
    "        output = TimeDistributed(Dense(MAX_LENGTH, activation='softmax'))(output)\n",
    "        output = Lambda(lambda x:K.squeeze(x,1))(output)\n",
    "        print(output.shape)\n",
    "        return output, hidden\n",
    "    \n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Model,load_model\n",
    "#############\n",
    "# 写表文件\n",
    "stem_tab_file = '../table/stem.txt'\n",
    "dbn_tab_file = '../table/dbn.txt'\n",
    "flag_tab_file = '../table/flag.txt'\n",
    "#\n",
    "#############\n",
    "# 训练文件和测试文件\n",
    "train_folder = '../data/train/'\n",
    "test_folder = '../data/test/'\n",
    "vaild_folder = '../data/vaild/'\n",
    "\n",
    "dbn_file_in ='dbn.in.txt'\n",
    "dbn_file_out ='dbn.out.txt'\n",
    "\n",
    "flag_file_in ='flag.in.txt'\n",
    "flag_file_out = 'flag.out.txt'\n",
    "#\n",
    "#############\n",
    "# 保存模型\n",
    "model_file ='./save_model/keras_seq2seq_embedding.h5'\n",
    "if not os.path.exists('./save_model'):\n",
    "    os.makedirs('./save_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解决keras 显存问题\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "set_session(tf.Session(config=config)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Base:\n",
    "    def __init__(self,filename,num_word_threshold):\n",
    "        self._bash_to_id = {}\n",
    "        self._num_word_threshold = num_word_threshold\n",
    "        self._read_dict(filename)\n",
    "    \n",
    "    def _read_dict(self,filename):\n",
    "        with open(filename,'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            # print(line)\n",
    "            word,frequency = line.strip('\\r\\n').split('\\t')\n",
    "            frequency = int(frequency)\n",
    "            if frequency < self._num_word_threshold:\n",
    "                continue\n",
    "            idx = len(self._bash_to_id)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            elif word == '<PAD>':\n",
    "                self._pad = idx\n",
    "            elif word == '<GO>':\n",
    "                self._go = idx\n",
    "            elif word == '<EOS>':\n",
    "                self._eos = idx\n",
    "            self._bash_to_id[word] = idx\n",
    "            \n",
    "    @property\n",
    "    def unk(self):\n",
    "         return len(self._bash_to_id)\n",
    "    @property\n",
    "    def pad(self):\n",
    "         return len(self._bash_to_id)+1 \n",
    "    @property\n",
    "    def go(self):\n",
    "         return len(self._bash_to_id)+2\n",
    "    @property\n",
    "    def eos(self):\n",
    "         return len(self._bash_to_id)+3\n",
    "        \n",
    "    def size(self):\n",
    "         return len(self._bash_to_id)\n",
    "    def base_to_id(self,word):\n",
    "        #完成\n",
    "         return self._bash_to_id.get(word,self._unk)\n",
    "    def sequence_to_id(self,sequeuce):\n",
    "        word_ids = [self.base_to_id(cur_word) for cur_word in sequeuce.split()]\n",
    "        return word_ids\n",
    "    \n",
    "    ### \n",
    "    # 不需要id2base 所以没写\n",
    "    ###\n",
    "num_word_threshold = 5000 # 频率低于5000的不要\n",
    "base =Base(stem_tab_file,num_word_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base.sequence_to_id(\"A G C U A C G G C C A U A C A U A G A U G A A A A U A C C G G A U C C C G U C C G A U C U C C G A A G U C A A G C A U C U A A U G G C G A C G U C A G U A C U G U G A U G G G G G A C C G C A C G G G A A U A C G U C G U G C U G U A G U\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "class CategoryDict:\n",
    "    def __init__(self,filename):\n",
    "        self._category_to_id = {}\n",
    "        self._id_to_category = {}\n",
    "        with open(filename , 'r') as f: \n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            category,_ = line.strip('\\r\\n').split('\\t')\n",
    "            idx = len(self._category_to_id)\n",
    "            self._category_to_id[category] = idx\n",
    "            self._id_to_category[idx] = category\n",
    "    def size(self):\n",
    "        return len(self._category_to_id)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def go(self):\n",
    "         return len(self._category_to_id)\n",
    "    @property\n",
    "    def eos(self):\n",
    "         return len(self._category_to_id)+1\n",
    "    @property\n",
    "    def pad(self):\n",
    "         return len(self._category_to_id)+2\n",
    "        \n",
    "    def category_to_id(self,category):\n",
    "        if category not in self._category_to_id:\n",
    "            raise Exception(\n",
    "                    \"%s is not in our category list\" % category)\n",
    "        return self._category_to_id[category]  \n",
    "    def id_to_category(self,id):\n",
    "        if id not in self._id_to_category:\n",
    "            raise Exception(\n",
    "                    \"%s is not in our id list\" % id)\n",
    "        return self._id_to_category[id]  \n",
    "    \n",
    "    def sequence_to_id(self,sequeuce):\n",
    "        word_ids = [self.category_to_id(cur_word) for cur_word in sequeuce.split()]\n",
    "        return word_ids\n",
    "    \n",
    "    def id_to_sequence(self,ids):\n",
    "        category = ' '.join([self.id_to_category(_id) for _id in ids])\n",
    "        return category\n",
    "    \n",
    "category_base = CategoryDict(dbn_tab_file)\n",
    "test_str = '. ( )'\n",
    "test_str2 = [0,1,2,3]\n",
    "print(category_base.sequence_to_id(test_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from %s ../data/train/dbn.in.txt\n",
      "Loading data from %s ../data/train/dbn.out.txt\n",
      "done\n",
      "Loading data from %s ../data/vaild/dbn.in.txt\n",
      "Loading data from %s ../data/vaild/dbn.out.txt\n",
      "done\n",
      "Loading data from %s ../data/test/dbn.in.txt\n",
      "Loading data from %s ../data/test/dbn.out.txt\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# 构建数据库 对其，shuffle，batch\n",
    "\n",
    "num_timesteps = 300\n",
    "\n",
    "class TextDataSet:\n",
    "    def __init__(self, folder, vocab, category_vocab, num_timesteps,):\n",
    "        infile = folder + dbn_file_in\n",
    "        outfile = folder + dbn_file_out\n",
    "        self._vocab = vocab\n",
    "        self._category_vocab = category_vocab\n",
    "        self._num_timesteps = num_timesteps\n",
    "        # matrix 矩阵\n",
    "        self._inputs = []# 所有数据的集合\n",
    "        # vector 列表\n",
    "        self._targets = []#\n",
    "        self._deinputs = []\n",
    "#         self._target_out = []# \n",
    "        self._label_len = []\n",
    "        self._feature_len = []\n",
    "        self._deinput_len = []\n",
    "        self._indicator = 0# batch的位置\n",
    "        self._parse_file(infile,outfile) # 解析⽂件\n",
    "\n",
    "        \n",
    "        \n",
    "    def _parse_file(self,infile,outfile): \n",
    "        \"\"\" 解析文件 \"\"\"\n",
    "        \n",
    "        print('Loading data from %s',infile)\n",
    "        print('Loading data from %s',outfile)\n",
    "        with open(infile,'r') as fi:\n",
    "            inlines = fi.readlines()\n",
    "        with open(outfile,'r')as fo:\n",
    "            outlines = fo.readlines()\n",
    "            \n",
    "        for line in inlines:\n",
    "            line = line.strip('\\r\\n').strip()\n",
    "            id_feature = self._vocab.sequence_to_id(line)\n",
    "            id_feature = id_feature[0:self._num_timesteps] #保证不会过长\n",
    "            feature_len = len(id_feature)\n",
    "            self._feature_len.append(feature_len)\n",
    "            padding_num = self._num_timesteps - feature_len # 如果id_words⼩于这个num_timesteps\n",
    "            id_feature = [self._vocab.go]+id_feature + list(self._vocab.pad for i in range(padding_num-1))\n",
    "            self._inputs.append(id_feature)\n",
    "            \n",
    "        for line in outlines:\n",
    "            line = line.strip('\\r\\n')\n",
    "            id_label = self._category_vocab.sequence_to_id(line)\n",
    "            id_label = id_label[0:self._num_timesteps]#600\n",
    "            label_len = len(id_label)\n",
    "            self._deinput_len.append(label_len) \n",
    "            padding_num = self._num_timesteps - len(id_label) # 如果id_words⼩于这个num_timesteps\n",
    "            id_label = [self._category_vocab.go]+id_label + list(self._category_vocab.pad for i in range(padding_num-1))\n",
    "            self._deinputs.append(id_label)\n",
    "            \n",
    "        for line in outlines:\n",
    "            line = line.strip('\\r\\n')\n",
    "            id_label = self._category_vocab.sequence_to_id(line)\n",
    "            id_label = id_label[0:self._num_timesteps]#600\n",
    "            label_len = len(id_label)\n",
    "            self._label_len.append(label_len)\n",
    "            padding_num = self._num_timesteps - len(id_label) # 如果id_words⼩于这个num_timesteps\n",
    "            id_label = id_label + [self._category_vocab.eos]+ list(self._category_vocab.pad for i in range(padding_num-1))\n",
    "            self._targets.append(id_label)        \n",
    "        \n",
    "            \n",
    "            \n",
    "        self._inputs = np.asarray(self._inputs, dtype = np.int32)\n",
    "        self._target = np.asarray(self._targets, dtype = np.int32)\n",
    "        self._deinputs = np.asarray(self._deinputs, dtype = np.int32)\n",
    "#         self._target_out = np.asarray(self._target_out, dtype = np.int32)\n",
    "        self._label_len = np.asarray(self._label_len, dtype = np.int32)\n",
    "        self._feature_len = np.asarray(self._feature_len, dtype = np.int32)\n",
    "        self._random_shuffle()\n",
    "\n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(len(self._inputs))\n",
    "        self._inputs = self._inputs[p]\n",
    "        self._target = self._target[p]\n",
    "        self._label_len = self._label_len[p]\n",
    "        self._feature_len = self._feature_len[p]\n",
    "\n",
    "    def next_batch(self,batch_size):\n",
    "        end_indicator = self._indicator +batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = batch_size\n",
    "        if end_indicator > len(self._inputs):# 则说明batchsize ⽐样本空间还要⼤\n",
    "            raise Exception(\"batch_size:%d is too large\"% batch_size)\n",
    "        \n",
    "        batch_inputs = self._inputs[self._indicator:end_indicator]\n",
    "        batch_target = self._target[self._indicator:end_indicator]\n",
    "        batch_deinput = self._deinputs[self._indicator:end_indicator]\n",
    "        inputs_len = self._feature_len[self._indicator:end_indicator]\n",
    "        outputs_len =  self._label_len[self._indicator:end_indicator]\n",
    "        deinputs_len = self._deinput_len[self._indicator:end_indicator]\n",
    "        \n",
    "        self._indicator = end_indicator\n",
    "        return (batch_inputs,inputs_len),(batch_target,outputs_len),(batch_deinput,deinputs_len)\n",
    "train_dataset = TextDataSet(\n",
    "    train_folder, base, category_base, num_timesteps)\n",
    "print('done')\n",
    "val_dataset = TextDataSet(\n",
    "    vaild_folder, base, category_base, num_timesteps)\n",
    "print('done')\n",
    "test_dataset = TextDataSet(\n",
    "    test_folder, base, category_base, num_timesteps)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((array([[8, 4, 3, 3, 3, 4, 2, 5, 3, 2, 2, 4, 3, 5, 4, 4, 5, 3, 5, 4, 4, 2,\n",
      "        5, 4, 2, 2, 2, 3, 4, 4, 4, 5, 2, 3, 4, 4, 4, 5, 3, 4, 5, 3, 5, 5,\n",
      "        3, 2, 4, 4, 4, 2, 4, 2, 2, 2, 4, 4, 2, 5, 2, 2, 5, 5, 4, 4, 4, 3,\n",
      "        5, 2, 4, 2, 5, 3, 2, 5, 3, 5, 2, 2, 4, 2, 3, 5, 2, 4, 3, 5, 4, 4,\n",
      "        5, 3, 3, 3, 3, 2, 4, 3, 5, 5, 3, 3, 3, 3, 5, 5, 2, 3, 4, 4, 3, 3,\n",
      "        4, 4, 2, 4, 3, 5, 5, 4, 4, 4, 4, 5, 3, 5, 5, 5, 4, 4, 3, 2, 5, 3,\n",
      "        3, 2, 3, 4, 4, 2, 4, 5, 4, 4, 2, 3, 3, 4, 4, 2, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
      "       [8, 3, 3, 4, 4, 3, 3, 3, 3, 3, 2, 3, 2, 3, 5, 5, 2, 3, 3, 4, 3, 5,\n",
      "        2, 3, 4, 4, 5, 5, 4, 3, 4, 3, 4, 4, 5, 2, 2, 4, 5, 4, 3, 2, 2, 2,\n",
      "        3, 3, 3, 4, 2, 2, 5, 4, 5, 4, 4, 2, 2, 2, 3, 2, 3, 2, 3, 2, 5, 5,\n",
      "        2, 4, 2, 5, 2, 5, 4, 3, 3, 3, 3, 2, 4, 4, 2, 2, 5, 3, 4, 4, 3, 3,\n",
      "        5, 2, 4, 3, 2, 2, 5, 4, 4, 2, 4, 4, 5, 3, 2, 5, 5, 3, 4, 3, 5, 4,\n",
      "        5, 4, 4, 4, 3, 3, 2, 3, 2, 4, 3, 2, 4, 5, 2, 3, 2, 2, 4, 5, 3, 4,\n",
      "        2, 4, 2, 4, 4, 4, 5, 2, 5, 5, 3, 3, 5, 4, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]], dtype=int32), array([147, 145], dtype=int32)), (array([[1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 3, 3, 3, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 3, 3,\n",
      "        3, 3, 3, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 2, 2, 3, 3, 1, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 2, 2, 2, 2, 1, 1, 3, 1, 1, 1, 1, 5, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1,\n",
      "        1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1,\n",
      "        1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]], dtype=int32), array([147, 145], dtype=int32)), (array([[4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 3, 3, 1, 3, 3, 3, 3, 3,\n",
      "        1, 1, 1, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 2, 2, 1,\n",
      "        1, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2,\n",
      "        2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 2, 1, 3, 3, 1, 1, 1, 1, 1, 3, 3, 1,\n",
      "        1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 3, 1, 1, 3, 3, 1, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
      "       [4, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,\n",
      "        2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2,\n",
      "        1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 1, 1, 1, 3,\n",
      "        3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 1, 3, 3,\n",
      "        3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 1,\n",
      "        1, 1, 1, 3, 3, 1, 1, 3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]], dtype=int32), [117, 164]))\n"
     ]
    }
   ],
   "source": [
    "a = train_dataset.next_batch(2)\n",
    "print(a)\n",
    "# print(val_dataset.next_batch(2)) \n",
    "# print(test_dataset.next_batch(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写数据\n",
    "def write_log(callback, names, logs, batch_no):\n",
    "\n",
    "    for (name, value) in zip(names, logs):\n",
    "        summary = tf.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.simple_value = value\n",
    "        summary_value.tag = name\n",
    "        callback.writer.add_summary(summary, batch_no)\n",
    "        callback.writer.flush()\n",
    "        \n",
    "def write_log_batch(callback, name, log, batch_no):\n",
    "    value = log\n",
    "    summary = tf.Summary()\n",
    "    summary_value = summary.value.add()\n",
    "    summary_value.simple_value = value\n",
    "    summary_value.tag = name\n",
    "    callback.writer.add_summary(summary, batch_no)\n",
    "    callback.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_length = 10\n",
    "# input_dim = 1\n",
    "# MAX_LENGTH = 300 # 单个字符的长度\n",
    "# output_length = 10\n",
    "# output_dim = 4\n",
    "\n",
    "# samples = 100\n",
    "# hidden_dim = 3\n",
    "# hidden_size = 150 # must be a half of Max_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1003 22:28:23.994187 140630717482752 deprecation_wrapper.py:119] From /data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1003 22:28:24.004379 140630717482752 deprecation_wrapper.py:119] From /data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1003 22:28:24.008403 140630717482752 deprecation_wrapper.py:119] From /data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1003 22:28:25.888438 140630717482752 deprecation_wrapper.py:119] From /data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1003 22:28:25.894220 140630717482752 deprecation.py:506] From /data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 300, 64)\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "(?, 1, 600)\n",
      "(?, 300)\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# model\n",
    "\n",
    "encoder = EncoderRNN(embedding_size,hidden_size)\n",
    "decoder = DecoderRNN(embedding_size,hidden_size)\n",
    "atten = AttnDecoderRNN(embedding_size,hidden_size)# hidden_size = 6\n",
    "\n",
    "en_inputs = Input(shape=(300,),dtype=float)\n",
    "de_inputs = Input(shape=(300,),dtype=float)\n",
    "en_output, en_hidden = encoder(en_inputs)\n",
    "attent_output,attn_hidden = atten(de_inputs,en_hidden,en_output)\n",
    "\n",
    "#\n",
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1003 22:30:48.031798 140630717482752 deprecation_wrapper.py:119] From /data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1003 22:30:48.311457 140630717482752 deprecation_wrapper.py:119] From /data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 300, 64)      64000       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, 300, 600), ( 657000      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     [(None, 300, 600), ( 2161800     bidirectional_1[0][0]            \n",
      "                                                                 gru_2[0][0]                      \n",
      "                                                                 gru_2[1][0]                      \n",
      "                                                                 gru_2[2][0]                      \n",
      "                                                                 gru_2[3][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "atten_embed (Embedding)         (None, 300, 64)      64000       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 19200)     0           atten_embed[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 19200)        0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 600)       0           gru_2[4][1]                      \n",
      "__________________________________________________________________________________________________\n",
      "atten_dropout (Dropout)         (None, 19200)        0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 600)          0           reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "atten_concat2 (Concatenate)     (None, 19800)        0           atten_dropout[0][0]              \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 300)          5940300     atten_concat2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "softmax_1 (Softmax)             (None, 300)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1, 300)       0           softmax_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 600)       0           reshape_3[0][0]                  \n",
      "                                                                 gru_2[4][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 600)          0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 19800)        0           atten_dropout[0][0]              \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "atten_combine (Dense)           (None, 300)          5940300     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 300)          0           atten_combine[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 1, 300)       0           re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "atten_bigru (Bidirectional)     [(None, 1, 600), (No 1081800     reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "atten_deepgru (GRU)             [(None, 1, 600), (No 2161800     atten_bigru[0][0]                \n",
      "                                                                 gru_2[0][1]                      \n",
      "                                                                 atten_deepgru[0][0]              \n",
      "                                                                 gru_2[1][1]                      \n",
      "                                                                 atten_deepgru[1][0]              \n",
      "                                                                 gru_2[2][1]                      \n",
      "                                                                 atten_deepgru[2][0]              \n",
      "                                                                 gru_2[3][1]                      \n",
      "__________________________________________________________________________________________________\n",
      "atten_lastdeepgru (GRU)         [(None, 600), (None, 2161800     atten_deepgru[3][0]              \n",
      "                                                                 gru_2[3][1]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 1, 600)       0           atten_lastdeepgru[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 1, 300)       180300      reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 300)          0           time_distributed_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 20,413,100\n",
      "Trainable params: 20,413,100\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1003 22:30:50.729676 140630717482752 deprecation_wrapper.py:119] From /data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "W1003 22:30:50.731938 140630717482752 deprecation_wrapper.py:119] From /data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "W1003 22:30:51.890234 140630717482752 deprecation.py:323] From /data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train]:step:0,loss:6741.301270,acc:0.000000 \n",
      "[vaild]:step:0,loss:7146.382812,acc:0.000000 \n",
      "[train]:step:1,loss:6815.166016,acc:0.000000 \n",
      "[train]:step:2,loss:6908.877930,acc:0.000000 \n",
      "[train]:step:3,loss:6862.089844,acc:0.000000 \n",
      "[train]:step:4,loss:6827.649902,acc:0.000000 \n",
      "[train]:step:5,loss:6817.248047,acc:0.000000 \n",
      "[train]:step:6,loss:6715.569824,acc:0.000000 \n",
      "[train]:step:7,loss:6752.843262,acc:0.000000 \n",
      "[train]:step:8,loss:6817.259277,acc:0.000000 \n",
      "[train]:step:9,loss:6746.078613,acc:0.000000 \n",
      "[train]:step:10,loss:6784.441895,acc:0.000000 \n",
      "[vaild]:step:10,loss:6889.036133,acc:0.000000 \n",
      "[train]:step:11,loss:6841.211914,acc:0.000000 \n",
      "[train]:step:12,loss:6738.381836,acc:0.000000 \n",
      "[train]:step:13,loss:6809.899902,acc:0.010000 \n",
      "[train]:step:14,loss:6712.031738,acc:0.000000 \n",
      "[train]:step:15,loss:6638.580078,acc:0.000000 \n",
      "[train]:step:16,loss:6759.935547,acc:0.000000 \n",
      "[train]:step:17,loss:6877.816406,acc:0.000000 \n",
      "[train]:step:18,loss:6648.244141,acc:0.000000 \n",
      "[train]:step:19,loss:6798.321289,acc:0.000000 \n",
      "[train]:step:20,loss:6689.352539,acc:0.000000 \n",
      "[vaild]:step:20,loss:7126.021973,acc:0.000000 \n",
      "[train]:step:21,loss:6778.127930,acc:0.000000 \n",
      "[train]:step:22,loss:6643.569336,acc:0.000000 \n",
      "[train]:step:23,loss:6782.108887,acc:0.000000 \n",
      "[train]:step:24,loss:6787.177734,acc:0.000000 \n",
      "[train]:step:25,loss:6897.185059,acc:0.000000 \n",
      "[train]:step:26,loss:6920.455078,acc:0.000000 \n",
      "[train]:step:27,loss:6566.326172,acc:0.000000 \n",
      "[train]:step:28,loss:6774.883789,acc:0.000000 \n",
      "[train]:step:29,loss:6758.274414,acc:0.000000 \n",
      "[train]:step:30,loss:6707.694336,acc:0.000000 \n",
      "[vaild]:step:30,loss:6414.726562,acc:0.000000 \n",
      "[train]:step:31,loss:6741.576660,acc:0.000000 \n",
      "[train]:step:32,loss:6692.531250,acc:0.000000 \n",
      "[train]:step:33,loss:6555.479980,acc:0.000000 \n",
      "[train]:step:34,loss:6682.913574,acc:0.000000 \n",
      "[train]:step:35,loss:6753.166992,acc:0.000000 \n",
      "[train]:step:36,loss:6738.693359,acc:0.000000 \n",
      "[train]:step:37,loss:6758.533203,acc:0.000000 \n",
      "[train]:step:38,loss:6772.524902,acc:0.000000 \n",
      "[train]:step:39,loss:6788.151367,acc:0.000000 \n",
      "[train]:step:40,loss:6653.958008,acc:0.000000 \n",
      "[vaild]:step:40,loss:6258.200195,acc:0.000000 \n",
      "[train]:step:41,loss:6665.009277,acc:0.000000 \n",
      "[train]:step:42,loss:6589.721680,acc:0.000000 \n",
      "[train]:step:43,loss:6692.243164,acc:0.000000 \n",
      "[train]:step:44,loss:6653.416992,acc:0.000000 \n",
      "[train]:step:45,loss:6808.675781,acc:0.000000 \n",
      "[train]:step:46,loss:6888.913574,acc:0.000000 \n",
      "[train]:step:47,loss:6812.165039,acc:0.000000 \n",
      "[train]:step:48,loss:6727.599609,acc:0.000000 \n",
      "[train]:step:49,loss:6627.846191,acc:0.000000 \n",
      "[train]:step:50,loss:6696.745117,acc:0.000000 \n",
      "[vaild]:step:50,loss:6531.262207,acc:0.000000 \n",
      "[train]:step:51,loss:6744.843262,acc:0.000000 \n",
      "[train]:step:52,loss:6658.843750,acc:0.000000 \n",
      "[train]:step:53,loss:6720.981934,acc:0.000000 \n",
      "[train]:step:54,loss:6682.129395,acc:0.000000 \n",
      "[train]:step:55,loss:6623.597656,acc:0.000000 \n",
      "[train]:step:56,loss:6816.113770,acc:0.000000 \n",
      "[train]:step:57,loss:6742.312500,acc:0.000000 \n",
      "[train]:step:58,loss:6699.792480,acc:0.000000 \n",
      "[train]:step:59,loss:6584.715820,acc:0.000000 \n",
      "[train]:step:60,loss:6691.356934,acc:0.000000 \n",
      "[vaild]:step:60,loss:6680.805664,acc:0.000000 \n",
      "[train]:step:61,loss:6626.399902,acc:0.000000 \n",
      "[train]:step:62,loss:6701.683105,acc:0.010000 \n",
      "[train]:step:63,loss:6698.874512,acc:0.000000 \n",
      "[train]:step:64,loss:6785.243652,acc:0.000000 \n",
      "[train]:step:65,loss:6698.279785,acc:0.000000 \n",
      "[train]:step:66,loss:6598.502930,acc:0.000000 \n",
      "[train]:step:67,loss:6587.640137,acc:0.000000 \n",
      "[train]:step:68,loss:6732.248535,acc:0.000000 \n",
      "[train]:step:69,loss:6733.966797,acc:0.000000 \n",
      "[train]:step:70,loss:6865.598145,acc:0.000000 \n",
      "[vaild]:step:70,loss:6742.950195,acc:0.000000 \n",
      "[train]:step:71,loss:6654.312500,acc:0.000000 \n",
      "[train]:step:72,loss:6721.009277,acc:0.000000 \n",
      "[train]:step:73,loss:6680.718750,acc:0.000000 \n",
      "[train]:step:74,loss:6710.649902,acc:0.000000 \n",
      "[train]:step:75,loss:6579.694824,acc:0.000000 \n",
      "[train]:step:76,loss:6770.559570,acc:0.000000 \n",
      "[train]:step:77,loss:6590.575195,acc:0.000000 \n",
      "[train]:step:78,loss:6809.856934,acc:0.000000 \n",
      "[train]:step:79,loss:6644.489258,acc:0.000000 \n",
      "[train]:step:80,loss:6732.283203,acc:0.000000 \n",
      "[vaild]:step:80,loss:6770.254395,acc:0.000000 \n",
      "[train]:step:81,loss:6841.056641,acc:0.000000 \n",
      "[train]:step:82,loss:6776.691895,acc:0.010000 \n",
      "[train]:step:83,loss:6746.947266,acc:0.000000 \n",
      "[train]:step:84,loss:6699.203125,acc:0.000000 \n",
      "[train]:step:85,loss:6753.345703,acc:0.000000 \n",
      "[train]:step:86,loss:6853.344238,acc:0.000000 \n",
      "[train]:step:87,loss:6799.680176,acc:0.000000 \n",
      "[train]:step:88,loss:6715.529297,acc:0.000000 \n",
      "[train]:step:89,loss:6712.826172,acc:0.000000 \n",
      "[train]:step:90,loss:6713.480469,acc:0.000000 \n",
      "[vaild]:step:90,loss:6997.699219,acc:0.000000 \n",
      "[train]:step:91,loss:6655.649902,acc:0.000000 \n",
      "[train]:step:92,loss:6734.421387,acc:0.000000 \n",
      "[train]:step:93,loss:6602.738770,acc:0.000000 \n",
      "[train]:step:94,loss:6768.160156,acc:0.000000 \n",
      "[train]:step:95,loss:6767.549316,acc:0.000000 \n",
      "[train]:step:96,loss:6872.350586,acc:0.000000 \n",
      "[train]:step:97,loss:6647.301270,acc:0.000000 \n",
      "[train]:step:98,loss:6788.996094,acc:0.000000 \n",
      "[train]:step:99,loss:6669.568359,acc:0.000000 \n",
      "[train]:step:100,loss:6473.455078,acc:0.000000 \n",
      "[vaild]:step:100,loss:6323.654785,acc:0.000000 \n",
      "[train]:step:101,loss:6806.113281,acc:0.000000 \n",
      "[train]:step:102,loss:6833.444824,acc:0.000000 \n",
      "[train]:step:103,loss:6663.122559,acc:0.000000 \n",
      "[train]:step:104,loss:6623.695801,acc:0.000000 \n",
      "[train]:step:105,loss:6763.121094,acc:0.000000 \n",
      "[train]:step:106,loss:6736.595215,acc:0.000000 \n",
      "[train]:step:107,loss:6705.495605,acc:0.000000 \n",
      "[train]:step:108,loss:6729.317383,acc:0.000000 \n",
      "[train]:step:109,loss:6789.018555,acc:0.000000 \n",
      "[train]:step:110,loss:6658.783203,acc:0.000000 \n",
      "[vaild]:step:110,loss:6955.919434,acc:0.000000 \n",
      "[train]:step:111,loss:6730.623047,acc:0.000000 \n",
      "[train]:step:112,loss:6673.287109,acc:0.000000 \n",
      "[train]:step:113,loss:6812.471191,acc:0.000000 \n",
      "[train]:step:114,loss:6650.969238,acc:0.000000 \n",
      "[train]:step:115,loss:6568.263184,acc:0.000000 \n",
      "[train]:step:116,loss:6715.123535,acc:0.000000 \n",
      "[train]:step:117,loss:6770.424316,acc:0.000000 \n",
      "[train]:step:118,loss:6691.690430,acc:0.000000 \n",
      "[train]:step:119,loss:6571.236816,acc:0.000000 \n",
      "[train]:step:120,loss:6824.958984,acc:0.000000 \n",
      "[vaild]:step:120,loss:6893.366699,acc:0.000000 \n",
      "[train]:step:121,loss:6658.665527,acc:0.000000 \n",
      "[train]:step:122,loss:6712.337402,acc:0.000000 \n",
      "[train]:step:123,loss:6714.529297,acc:0.000000 \n",
      "[train]:step:124,loss:6621.990723,acc:0.000000 \n",
      "[train]:step:125,loss:6799.478516,acc:0.000000 \n",
      "[train]:step:126,loss:6813.458008,acc:0.000000 \n",
      "[train]:step:127,loss:6618.038574,acc:0.000000 \n",
      "[train]:step:128,loss:6724.335449,acc:0.000000 \n",
      "[train]:step:129,loss:6730.613281,acc:0.000000 \n",
      "[train]:step:130,loss:6497.501953,acc:0.000000 \n",
      "[vaild]:step:130,loss:6890.804199,acc:0.000000 \n",
      "[train]:step:131,loss:6681.334473,acc:0.000000 \n",
      "[train]:step:132,loss:6836.154785,acc:0.000000 \n",
      "[train]:step:133,loss:6683.230469,acc:0.000000 \n",
      "[train]:step:134,loss:6535.658203,acc:0.000000 \n",
      "[train]:step:135,loss:6705.561035,acc:0.000000 \n",
      "[train]:step:136,loss:6761.015137,acc:0.000000 \n",
      "[train]:step:137,loss:6638.831055,acc:0.000000 \n",
      "[train]:step:138,loss:6638.632324,acc:0.000000 \n",
      "[train]:step:139,loss:6751.570801,acc:0.000000 \n",
      "[train]:step:140,loss:6747.890137,acc:0.000000 \n",
      "[vaild]:step:140,loss:7189.744141,acc:0.000000 \n",
      "[train]:step:141,loss:6720.430176,acc:0.000000 \n",
      "[train]:step:142,loss:6721.700195,acc:0.000000 \n",
      "[train]:step:143,loss:6789.373535,acc:0.000000 \n",
      "[train]:step:144,loss:6624.711914,acc:0.010000 \n",
      "[train]:step:145,loss:6694.118652,acc:0.000000 \n",
      "[train]:step:146,loss:6654.031738,acc:0.000000 \n",
      "[train]:step:147,loss:6484.495605,acc:0.000000 \n",
      "[train]:step:148,loss:6810.754395,acc:0.000000 \n",
      "[train]:step:149,loss:6800.193848,acc:0.000000 \n",
      "[train]:step:150,loss:6696.660645,acc:0.000000 \n",
      "[vaild]:step:150,loss:6730.992188,acc:0.000000 \n",
      "[train]:step:151,loss:6737.431152,acc:0.000000 \n",
      "[train]:step:152,loss:6747.310059,acc:0.000000 \n",
      "[train]:step:153,loss:6746.153809,acc:0.000000 \n",
      "[train]:step:154,loss:6848.462891,acc:0.000000 \n",
      "[train]:step:155,loss:6676.331055,acc:0.000000 \n",
      "[train]:step:156,loss:6721.741211,acc:0.000000 \n",
      "[train]:step:157,loss:6725.330566,acc:0.000000 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train]:step:158,loss:6675.917969,acc:0.000000 \n",
      "[train]:step:159,loss:6628.243164,acc:0.000000 \n",
      "[train]:step:160,loss:6700.006348,acc:0.000000 \n",
      "[vaild]:step:160,loss:6610.482422,acc:0.000000 \n",
      "[train]:step:161,loss:6773.794434,acc:0.000000 \n",
      "[train]:step:162,loss:6734.754883,acc:0.000000 \n",
      "[train]:step:163,loss:6700.100586,acc:0.000000 \n",
      "[train]:step:164,loss:6785.564941,acc:0.000000 \n",
      "[train]:step:165,loss:6704.711426,acc:0.000000 \n",
      "[train]:step:166,loss:6657.947266,acc:0.000000 \n",
      "[train]:step:167,loss:6559.028320,acc:0.000000 \n",
      "[train]:step:168,loss:6677.753906,acc:0.000000 \n",
      "[train]:step:169,loss:6752.916016,acc:0.000000 \n",
      "[train]:step:170,loss:6734.998047,acc:0.000000 \n",
      "[vaild]:step:170,loss:7002.645508,acc:0.000000 \n",
      "[train]:step:171,loss:6694.432617,acc:0.000000 \n",
      "[train]:step:172,loss:6652.352051,acc:0.000000 \n",
      "[train]:step:173,loss:6669.058594,acc:0.000000 \n",
      "[train]:step:174,loss:6749.088867,acc:0.000000 \n",
      "[train]:step:175,loss:6670.196289,acc:0.000000 \n",
      "[train]:step:176,loss:6637.508789,acc:0.000000 \n",
      "[train]:step:177,loss:6702.060547,acc:0.000000 \n",
      "[train]:step:178,loss:6616.401855,acc:0.000000 \n",
      "[train]:step:179,loss:6587.805176,acc:0.000000 \n",
      "[train]:step:180,loss:6744.867676,acc:0.000000 \n",
      "[vaild]:step:180,loss:6545.255859,acc:0.000000 \n",
      "[train]:step:181,loss:6586.308594,acc:0.000000 \n",
      "[train]:step:182,loss:6607.062500,acc:0.000000 \n",
      "[train]:step:183,loss:6764.662598,acc:0.000000 \n",
      "[train]:step:184,loss:6652.933594,acc:0.000000 \n",
      "[train]:step:185,loss:6642.590820,acc:0.000000 \n",
      "[train]:step:186,loss:6705.738770,acc:0.000000 \n",
      "[train]:step:187,loss:6715.424316,acc:0.000000 \n",
      "[train]:step:188,loss:6752.397461,acc:0.000000 \n",
      "[train]:step:189,loss:6604.018555,acc:0.000000 \n",
      "[train]:step:190,loss:6636.628906,acc:0.000000 \n",
      "[vaild]:step:190,loss:6826.989258,acc:0.000000 \n",
      "[train]:step:191,loss:6766.840820,acc:0.000000 \n",
      "[train]:step:192,loss:6710.444336,acc:0.000000 \n",
      "[train]:step:193,loss:6617.239258,acc:0.000000 \n"
     ]
    }
   ],
   "source": [
    "model = Model([en_inputs,de_inputs], attent_output)\n",
    "if os.path.exists(model_file):\n",
    "    print(\"load weight from \",model_file)\n",
    "    model.load_weights(model_file)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "log_path = './graph'\n",
    "callback = TensorBoard(log_path)\n",
    "callback.set_model(model)\n",
    "train_names = 'train_loss'\n",
    "val_names = 'val_loss'\n",
    "for batch_no in range(100000):\n",
    "    #####\n",
    "    # 处理数据\n",
    "    X_train_all, Y_train_all,Y_deinput_all = train_dataset.next_batch(samples)\n",
    "    X_train,length = X_train_all\n",
    "    Y_train,length = Y_train_all\n",
    "    Y_deinput,length = Y_deinput_all\n",
    "    # 处理数据\n",
    "    #######\n",
    "#   (samples, input_length, input_dim) 16,300,1\n",
    "    logs = model.train_on_batch([X_train, Y_deinput],Y_train )\n",
    "\n",
    "    print(\"[train]:step:%d,loss:%f,acc:%f \"%(batch_no,logs[0],logs[1]))\n",
    "    write_log_batch(callback, train_names, logs[0], batch_no)\n",
    "    if batch_no % 10 == 0:\n",
    "        model.save_weights(model_file)\n",
    "        #####\n",
    "        # 处理数据\n",
    "        X_val_all, Y_val_all , Y_de_all= val_dataset.next_batch(16)\n",
    "        X_val,length = X_val_all\n",
    "        Y_val,length = Y_val_all\n",
    "        Y_de,length = Y_de_all\n",
    "        # 处理数据\n",
    "        #######\n",
    "#         logs = model.test_on_batch(X_val, Y_val)\n",
    "        score = model.evaluate([X_val,Y_de], Y_val, verbose=0)\n",
    "\n",
    "        print(\"[vaild]:step:%d,loss:%f,acc:%f \"%(batch_no,score[0],score[1]))\n",
    "        \n",
    "        write_log_batch(callback, val_names, logs[0], batch_no//10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "X_test_all,Y_test_all = test_dataset.next_batch(1)\n",
    "X_test,length = X_test_all\n",
    "Y_test,length = Y_test_all\n",
    "X_test = X_test[:,:,np.newaxis]\n",
    "Y_test = Y_test[:,:,np.newaxis]\n",
    "Y_hat = model.predict(X_test)\n",
    "Y_hat = np.squeeze(Y_hat)\n",
    "print(Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow -GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
