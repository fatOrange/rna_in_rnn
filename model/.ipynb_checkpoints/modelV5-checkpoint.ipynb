{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/data/home/chenzhiyuan/anaconda3/envs/tf/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.activations import relu\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解决keras 显存问题\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.25\n",
    "set_session(tf.Session(config=config)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_size = [embedding_length,embedding_dim,input_length]\n",
    "embedding_size = [1000,64,300]\n",
    "# hidden_size is the length of the kernel utils  = enc_units\n",
    "class EncoderRNN(Model):\n",
    "    def __init__(self, embedding_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = Embedding(embedding_size[0], embedding_size[1],input_length=embedding_size[2])\n",
    "        self.gru = GRU(hidden_size, return_sequences=True, return_state=True)\n",
    "        self.deepgru = GRU(hidden_size*2, return_sequences=True, return_state=True)\n",
    "        self.lastdeepgru = GRU(hidden_size*2, return_sequences=False, return_state=True)\n",
    "        self.bigru = Bidirectional(self.gru)\n",
    "\n",
    "    def __call__(self, en_input):\n",
    "        state_h = []\n",
    "        emb = self.embedding(en_inputs)\n",
    "        encoder_out, fwd_h1, bck_h1 = self.bigru(emb)\n",
    "        state_h.append(concatenate([fwd_h1, bck_h1]))\n",
    "        if hidden_dim>1:\n",
    "            for i in range(1,hidden_dim):\n",
    "                encoder_out, en_hidden = self.deepgru(encoder_out)\n",
    "                state_h.append(en_hidden)\n",
    "        output = encoder_out\n",
    "        hidden = state_h\n",
    "        return output, hidden\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        init_state = [tf.zeros((1, self.hidden_size)) for i in range(2)]\n",
    "        return init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(Model):\n",
    "    def __init__(self, embedding_size, hidden_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = Embedding(embedding_size[0], embedding_size[1],input_length=embedding_size[2])\n",
    "        self.gru = GRU(hidden_size, return_sequences=True, return_state=True)\n",
    "        self.deepgru = GRU(hidden_size*2, return_sequences=True, return_state=True)\n",
    "        self.lastdeepgru = GRU(hidden_size*2, return_sequences=False, return_state=True)\n",
    "        self.bigru = Bidirectional(self.gru)\n",
    "        \n",
    "        self.out = Dense(output_length)\n",
    "        self.softmax = Activation('softmax')\n",
    "\n",
    "    def __call__(self, de_input, hiddens):\n",
    "        state_h = []\n",
    "        emb = self.embedding(en_inputs)\n",
    "        emb = Activation('relu')(emb)\n",
    "        # !挖个坑，这里没有初始化hidden_state\n",
    "        output, fwd_h, bck_h = self.bigru(emb)\n",
    "        state_h.append(concatenate([fwd_h, bck_h]))\n",
    "        if len(hiddens)>2:\n",
    "            for hidden in hiddens[1:-1]:\n",
    "                output, de_hidden = self.deepgru(output, hidden)\n",
    "                state_h.append(de_hidden)\n",
    "        if len(hiddens)>1:\n",
    "            output, de_hidden = self.lastdeepgru(output, hidden)\n",
    "            state_h.append(de_hidden)\n",
    "        hidden = state_h\n",
    "        output = self.softmax(output)\n",
    "        print(type(output))\n",
    "        return output, hidden\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        init_state = tf.zeros((1, self.hidden_size))\n",
    "        return init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttnDecoderRNN(Model):\n",
    "    def __init__(self, embedding_size ,hidden_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = Embedding(embedding_size[0], embedding_size[1],input_length=embedding_size[2],name ='atten_embed')\n",
    "        self.attn = Dense(self.max_length,name = 'atten_attn')\n",
    "        self.attn_combine = Dense(self.hidden_size,name = 'atten_combine')\n",
    "        self.dropout = Dropout(self.dropout_p,name ='atten_dropout')\n",
    "        \n",
    "        self.gru = GRU(hidden_size, return_sequences=True, return_state=True,name = 'atten_gru')\n",
    "        self.deepgru = GRU(hidden_size*2, return_sequences=True, return_state=True,name = 'atten_deepgru')\n",
    "        self.lastdeepgru = GRU(hidden_size*2, return_sequences=False, return_state=True,name='atten_lastdeepgru')\n",
    "        self.bigru = Bidirectional(self.gru,name='atten_bigru')\n",
    "        self.batch_dot = Lambda(lambda layers:K.batch_dot(layers[0],layers[1]))\n",
    "        self.out = Dense(output_length)\n",
    "        self.softmax =Softmax(axis=-1)\n",
    "        #包装层\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def __call__(self, de_input, hiddens, encoder_outputs):\n",
    "        state_h = []\n",
    "        embedded = self.embedding(de_input)\n",
    "        print(embedded.shape)\n",
    "        # TODO ; use lambda https://www.cnblogs.com/jqpy1994/p/11433746.html or  https://keras.io/zh/layers/core/\n",
    "        #embedded = K.reshape(embedded,[samples,embedding_size[1]*embedding_size[2]])\n",
    "        embedded = Reshape((1,embedding_size[1]*embedding_size[2]))(embedded)\n",
    "        print(type(embedded))\n",
    "        embedded = Lambda(lambda x:K.squeeze(x,1))(embedded)\n",
    "    \n",
    "        embedded = self.dropout(embedded) # Dim:(Batch Size , Decoder Hidden Size + Embedding Size)\n",
    "        if type(hiddens) == type(list()):\n",
    "            hidden = hiddens[-1]\n",
    "        \n",
    "        # hidden = K.reshape(hidden,[samples,self.hidden_size*2]) \n",
    "        hidden = Reshape((1,self.hidden_size*2))(hidden)\n",
    "        hidden = Lambda(lambda x:K.squeeze(x,1))(hidden)\n",
    "        concat = Concatenate(1,name='atten_concat2')([embedded, hidden])\n",
    "        # note: 从这里开始，把两个向量拼接起来    \n",
    "        attn_weights =self.softmax(\n",
    "            Dense(self.max_length)(concat))\n",
    "        atten_weights = Reshape((1,-1))(attn_weights)\n",
    "        attn_applied = self.batch_dot([atten_weights,encoder_outputs])\n",
    "        print(attn_applied.shape)\n",
    "        attn_applied = Lambda(lambda x:K.squeeze(x,1))(attn_applied)\n",
    "\n",
    "        \n",
    "        output = Concatenate(1)([embedded, attn_applied])\n",
    "\n",
    "        output = self.attn_combine(output)\n",
    "        \n",
    "        output = ReLU()(output)\n",
    "        output = Reshape((1,-1))(output)\n",
    "        output, fwd_h, bck_h = self.bigru(output)\n",
    "        state_h.append(concatenate([fwd_h, bck_h]))\n",
    "        if len(hiddens)>2:\n",
    "            for hidden in hiddens[1:-1]:\n",
    "                output, de_hidden = self.deepgru(output, hidden)\n",
    "                state_h.append(de_hidden)\n",
    "        if len(hiddens)>1:\n",
    "            output, de_hidden = self.lastdeepgru(output, hidden)\n",
    "            state_h.append(de_hidden)\n",
    "        hidden = state_h\n",
    "        output = self.softmax(output)\n",
    "        print(output.shape)\n",
    "        return output, hidden\n",
    "    \n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Model,load_model\n",
    "#############\n",
    "# 写表文件\n",
    "stem_tab_file = '../table/stem.txt'\n",
    "dbn_tab_file = '../table/dbn.txt'\n",
    "flag_tab_file = '../table/flag.txt'\n",
    "#\n",
    "#############\n",
    "# 训练文件和测试文件\n",
    "train_folder = '../data/train/'\n",
    "test_folder = '../data/test/'\n",
    "vaild_folder = '../data/vaild/'\n",
    "\n",
    "dbn_file_in ='dbn.in.txt'\n",
    "dbn_file_out ='dbn.out.txt'\n",
    "\n",
    "flag_file_in ='flag.in.txt'\n",
    "flag_file_out = 'flag.out.txt'\n",
    "#\n",
    "#############\n",
    "# 保存模型\n",
    "model_file ='./save_model/keras_seq2seq_embedding.h5'\n",
    "if not os.path.exists('./save_model'):\n",
    "    os.makedirs('./save_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解决keras 显存问题\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "set_session(tf.Session(config=config)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Base:\n",
    "    def __init__(self,filename,num_word_threshold):\n",
    "        self._bash_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._pad = -1\n",
    "        self._go   = -1\n",
    "        self._eos  = -1\n",
    "        self._num_word_threshold = num_word_threshold\n",
    "        self._read_dict(filename)\n",
    "    \n",
    "    def _read_dict(self,filename):\n",
    "        with open(filename,'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            # print(line)\n",
    "            word,frequency = line.strip('\\r\\n').split('\\t')\n",
    "            frequency = int(frequency)\n",
    "            if frequency < self._num_word_threshold:\n",
    "                continue\n",
    "            idx = len(self._bash_to_id)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            elif word == '<PAD>':\n",
    "                self._pad = idx\n",
    "#             elif word == '<GO>':\n",
    "#                 self._go = idx\n",
    "#             elif word == '<EOS>':\n",
    "#                 self._eos = idx\n",
    "            self._bash_to_id[word] = idx\n",
    "            \n",
    "    @property\n",
    "    def unk(self):\n",
    "         return self._unk\n",
    "    @property\n",
    "    def pad(self):\n",
    "         return self._pad    \n",
    "    @property\n",
    "    def go(self):\n",
    "         return self._go\n",
    "    @property\n",
    "    def eos(self):\n",
    "         return self._eos \n",
    "        \n",
    "    def size(self):\n",
    "         return len(self._bash_to_id)\n",
    "    def base_to_id(self,word):\n",
    "        #完成\n",
    "         return self._bash_to_id.get(word,self._unk)\n",
    "    def sequence_to_id(self,sequeuce):\n",
    "        word_ids = [self.base_to_id(cur_word) for cur_word in sequeuce.split()]\n",
    "        return word_ids\n",
    "    \n",
    "    ### \n",
    "    # 不需要id2base 所以没写\n",
    "    ###\n",
    "num_word_threshold = 5000 # 频率低于5000的不要\n",
    "base =Base(stem_tab_file,num_word_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base.sequence_to_id(\"A G C U A C G G C C A U A C A U A G A U G A A A A U A C C G G A U C C C G U C C G A U C U C C G A A G U C A A G C A U C U A A U G G C G A C G U C A G U A C U G U G A U G G G G G A C C G C A C G G G A A U A C G U C G U G C U G U A G U\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<PAD>', 1: '.', 2: ')', 3: '('}\n",
      "<PAD> . ) (\n"
     ]
    }
   ],
   "source": [
    "class CategoryDict:\n",
    "    def __init__(self,filename):\n",
    "        self._category_to_id = {}\n",
    "        self._id_to_category = {}\n",
    "        with open(filename , 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            category,_ = line.strip('\\r\\n').split('\\t')\n",
    "            idx = len(self._category_to_id)\n",
    "            self._category_to_id[category] = idx\n",
    "            self._id_to_category[idx] = category\n",
    "    def size(self):\n",
    "        return len(self._category_to_id)\n",
    "    def category_to_id(self,category):\n",
    "        if category not in self._category_to_id:\n",
    "            raise Exception(\n",
    "                    \"%s is not in our category list\" % category)\n",
    "        return self._category_to_id[category]  \n",
    "    def id_to_category(self,id):\n",
    "        if id not in self._id_to_category:\n",
    "            raise Exception(\n",
    "                    \"%s is not in our id list\" % id)\n",
    "        return self._id_to_category[id]  \n",
    "    \n",
    "    def sequence_to_id(self,sequeuce):\n",
    "        \n",
    "        word_ids = [self.category_to_id(cur_word) for cur_word in sequeuce.split()]\n",
    "        return word_ids\n",
    "    \n",
    "    def id_to_sequence(self,ids):\n",
    "        print(self._id_to_category)\n",
    "        category = ' '.join([self.id_to_category(_id) for _id in ids])\n",
    "        return category\n",
    "    \n",
    "category_base = CategoryDict(dbn_tab_file)\n",
    "test_str = '. ( )'\n",
    "test_str2 = [0,1,2,3]\n",
    "print(category_base.id_to_sequence(test_str2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from %s ../data/train/dbn.in.txt\n",
      "Loading data from %s ../data/train/dbn.out.txt\n",
      "done\n",
      "Loading data from %s ../data/vaild/dbn.in.txt\n",
      "Loading data from %s ../data/vaild/dbn.out.txt\n",
      "done\n",
      "Loading data from %s ../data/test/dbn.in.txt\n",
      "Loading data from %s ../data/test/dbn.out.txt\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# 构建数据库 对其，shuffle，batch\n",
    "\n",
    "num_timesteps = 300\n",
    "\n",
    "class TextDataSet:\n",
    "    def __init__(self, folder, vocab, category_vocab, num_timesteps,):\n",
    "        infile = folder + dbn_file_in\n",
    "        outfile = folder + dbn_file_out\n",
    "        self._vocab = vocab\n",
    "        self._category_vocab = category_vocab\n",
    "        self._num_timesteps = num_timesteps\n",
    "        # matrix 矩阵\n",
    "        self._inputs = []# 所有数据的集合\n",
    "        # vector 列表\n",
    "        self._target = []# \n",
    "#         self._target_out = []# \n",
    "        self._label_len = []\n",
    "        self._feature_len = []\n",
    "        self._indicator = 0# batch的位置\n",
    "        \n",
    "        self._parse_file(infile,outfile) # 解析⽂件\n",
    "\n",
    "        \n",
    "        \n",
    "    def _parse_file(self,infile,outfile): \n",
    "        \"\"\" 解析文件 \"\"\"\n",
    "        \n",
    "        print('Loading data from %s',infile)\n",
    "        print('Loading data from %s',outfile)\n",
    "        with open(infile,'r') as fi:\n",
    "            inlines = fi.readlines()\n",
    "        with open(outfile,'r')as fo:\n",
    "            outlines = fo.readlines()\n",
    "            \n",
    "        for line in inlines:\n",
    "            line = line.strip('\\r\\n').strip()\n",
    "            id_feature = self._vocab.sequence_to_id(line)\n",
    "            id_feature = id_feature[0:self._num_timesteps] #保证不会过长\n",
    "            feature_len = len(id_feature)\n",
    "            self._feature_len.append(feature_len)\n",
    "            padding_num = self._num_timesteps - feature_len # 如果id_words⼩于这个num_timesteps\n",
    "            id_feature = id_feature + list(self._vocab.pad for i in range(padding_num))\n",
    "            self._inputs.append(id_feature)\n",
    "            \n",
    "        for line in outlines:\n",
    "            line = line.strip('\\r\\n')\n",
    "            id_label = self._vocab.sequence_to_id(line)\n",
    "            id_label = id_label[0:self._num_timesteps]#600\n",
    "            label_len = len(id_label)\n",
    "            self._label_len.append(label_len)\n",
    "            padding_num = self._num_timesteps - len(id_label) # 如果id_words⼩于这个num_timesteps\n",
    "            id_label = id_label + list(self._vocab.pad for i in range(padding_num))\n",
    "            self._target.append(id_label)\n",
    "            \n",
    "        self._inputs = np.asarray(self._inputs, dtype = np.int32)\n",
    "        self._target = np.asarray(self._target, dtype = np.int32)\n",
    "#         self._target_out = np.asarray(self._target_out, dtype = np.int32)\n",
    "        self._label_len = np.asarray(self._label_len, dtype = np.int32)\n",
    "        self._feature_len = np.asarray(self._feature_len, dtype = np.int32)\n",
    "        self._random_shuffle()\n",
    "\n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(len(self._inputs))\n",
    "        self._inputs = self._inputs[p]\n",
    "        self._target = self._target[p]\n",
    "        self._label_len = self._label_len[p]\n",
    "        self._feature_len = self._feature_len[p]\n",
    "\n",
    "    def next_batch(self,batch_size):\n",
    "        end_indicator = self._indicator +batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = batch_size\n",
    "        if end_indicator > len(self._inputs):# 则说明batchsize ⽐样本空间还要⼤\n",
    "            raise Exception(\"batch_size:%d is too large\"% batch_size)\n",
    "        \n",
    "        batch_inputs = self._inputs[self._indicator:end_indicator]\n",
    "        batch_target = self._target[self._indicator:end_indicator]\n",
    "        inputs_len = self._feature_len[self._indicator:end_indicator]\n",
    "        outputs_len =  self._label_len[self._indicator:end_indicator]\n",
    "        \n",
    "        \n",
    "        self._indicator = end_indicator\n",
    "        return (batch_inputs,inputs_len),(batch_target,outputs_len)\n",
    "train_dataset = TextDataSet(\n",
    "    train_folder, base, category_base, num_timesteps)\n",
    "print('done')\n",
    "val_dataset = TextDataSet(\n",
    "    vaild_folder, base, category_base, num_timesteps)\n",
    "print('done')\n",
    "test_dataset = TextDataSet(\n",
    "    test_folder, base, category_base, num_timesteps)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = train_dataset.next_batch(2)\n",
    "type(a)\n",
    "# print(val_dataset.next_batch(2)) \n",
    "# print(test_dataset.next_batch(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写数据\n",
    "def write_log(callback, names, logs, batch_no):\n",
    "\n",
    "    for (name, value) in zip(names, logs):\n",
    "        summary = tf.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.simple_value = value\n",
    "        summary_value.tag = name\n",
    "        callback.writer.add_summary(summary, batch_no)\n",
    "        callback.writer.flush()\n",
    "        \n",
    "def write_log_batch(callback, name, log, batch_no):\n",
    "    value = log\n",
    "    summary = tf.Summary()\n",
    "    summary_value = summary.value.add()\n",
    "    summary_value.simple_value = value\n",
    "    summary_value.tag = name\n",
    "    callback.writer.add_summary(summary, batch_no)\n",
    "    callback.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = 10\n",
    "input_dim = 1\n",
    "MAX_LENGTH = 300 # 单个字符的长度\n",
    "output_length = 10\n",
    "output_dim = 4\n",
    "\n",
    "samples = 100\n",
    "hidden_dim = 3\n",
    "hidden_size = 150 # must be a half of Max_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# model\n",
    "\n",
    "encoder = EncoderRNN(embedding_size,hidden_size)\n",
    "decoder = DecoderRNN(embedding_size,hidden_size)\n",
    "atten = AttnDecoderRNN(embedding_size,hidden_size)# hidden_size = 6\n",
    "\n",
    "en_inputs = Input(shape=(300,),dtype=float)\n",
    "de_inputs = Input(shape=(300,),dtype=float)\n",
    "en_output, en_hidden = encoder(en_inputs)\n",
    "attent_output,attn_hidden = atten(de_inputs,en_hidden,en_output)\n",
    "\n",
    "#\n",
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer dropout_39 was called with an input that isn't a symbolic tensor. Received type: <class 'keras.layers.embeddings.Embedding'>. Full input: [<keras.layers.embeddings.Embedding object at 0x7f1becdff400>]. All inputs to the layer should be tensors.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m                 \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_keras_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mis_keras_tensor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    473\u001b[0m         raise ValueError('Unexpectedly found an instance of type `' +\n\u001b[0;32m--> 474\u001b[0;31m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m                          'Expected a symbolic tensor instance.')\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpectedly found an instance of type `<class 'keras.layers.embeddings.Embedding'>`. Expected a symbolic tensor instance.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-d1f9147851db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m output = AttentionSeq2Seq(output_dim=output_dim, hidden_dim=hidden_dim, output_length=output_length,\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    283\u001b[0m                                  \u001b[0;34m'Received type: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                                  \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full input: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m                                  \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. All inputs to the layer '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m                                  'should be tensors.')\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Layer dropout_39 was called with an input that isn't a symbolic tensor. Received type: <class 'keras.layers.embeddings.Embedding'>. Full input: [<keras.layers.embeddings.Embedding object at 0x7f1becdff400>]. All inputs to the layer should be tensors."
     ]
    }
   ],
   "source": [
    "model = Model([en_inputs,de_inputs], attent_output)\n",
    "if os.path.exists(model_file):\n",
    "    model.load_weights(model_file)\n",
    "    print(\"load weight from \",model_file)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "log_path = './graph'\n",
    "callback = TensorBoard(log_path)\n",
    "callback.set_model(model)\n",
    "train_names = 'train_loss'\n",
    "val_names = 'val_loss'\n",
    "for batch_no in range(100000):\n",
    "    #####\n",
    "    # 处理数据\n",
    "    X_train_all, Y_train_all = train_dataset.next_batch(16)\n",
    "    X_train,length = X_train_all\n",
    "    Y_train,length = Y_train_all\n",
    "    X_train = X_train[:,:,np.newaxis]\n",
    "    Y_train = Y_train[:,:,np.newaxis]\n",
    "    # 处理数据\n",
    "    #######\n",
    "#   (samples, input_length, input_dim) 16,300,1\n",
    "    logs = model.train_on_batch(X_train, Y_train)\n",
    "\n",
    "    print(\"[train]:step:%d,loss:%f,acc:%f \"%(batch_no,logs[0],logs[1]))\n",
    "    write_log_batch(callback, train_names, logs[0], batch_no)\n",
    "    if batch_no % 10 == 0:\n",
    "        model.save_weights(model_file)\n",
    "        #####\n",
    "        # 处理数据\n",
    "        X_val_all, Y_val_all = val_dataset.next_batch(16)\n",
    "        X_val,length = X_val_all\n",
    "        Y_val,length = Y_val_all\n",
    "        X_val = X_val[:,:,np.newaxis]\n",
    "        Y_val = Y_val[:,:,np.newaxis]\n",
    "        # 处理数据\n",
    "        #######\n",
    "#         logs = model.test_on_batch(X_val, Y_val)\n",
    "        score = model.evaluate(X_val, Y_val, verbose=0)\n",
    "\n",
    "        print(\"[vaild]:step:%d,loss:%f,acc:%f \"%(batch_no,score[0],score[1]))\n",
    "        \n",
    "        write_log_batch(callback, val_names, logs[0], batch_no//10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.39549881e-01  7.17206597e-02  1.91469491e-02 -1.15999440e-02\n",
      " -2.58289631e-02 -2.94791106e-02 -2.69863177e-02 -2.13196594e-02\n",
      " -1.43572958e-02 -7.24183768e-03 -6.45056251e-04  5.05234487e-03\n",
      "  9.64918546e-03  1.30563406e-02  1.52599439e-02  1.63010228e-02\n",
      "  1.62643008e-02  1.52714597e-02  1.34751387e-02  1.10527994e-02\n",
      "  8.19986593e-03  5.12337685e-03  2.03495938e-03 -8.53746897e-04\n",
      " -3.33608943e-03 -5.21383947e-03 -6.30143890e-03 -6.43619942e-03\n",
      " -5.49441995e-03 -3.42649361e-03 -3.06189060e-04  3.60551407e-03\n",
      "  7.81962741e-03  1.16519425e-02  1.43788233e-02  1.54796988e-02\n",
      "  1.48286978e-02  1.27052087e-02  9.63252783e-03  6.17038831e-03\n",
      "  2.77342671e-03 -2.59965658e-04 -2.77598971e-03 -4.72863950e-03\n",
      " -6.14094455e-03 -7.07413396e-03 -7.60536129e-03 -7.81414378e-03\n",
      " -7.77498633e-03 -7.55256973e-03 -7.20157707e-03 -6.76606689e-03\n",
      " -6.28032489e-03 -5.77112567e-03 -5.25746867e-03 -4.75304713e-03\n",
      " -4.26736847e-03 -3.80561897e-03 -3.37125571e-03 -2.96511385e-03\n",
      " -2.58701458e-03 -2.23581120e-03 -1.90946227e-03 -1.60623947e-03\n",
      " -1.32389285e-03 -1.06058980e-03 -8.14363186e-04 -5.84095658e-04\n",
      " -3.68311972e-04 -1.66043639e-04  2.32756138e-05  1.99928880e-04\n",
      "  3.64452571e-04  5.16489090e-04  6.56559947e-04  7.84337346e-04\n",
      "  8.99851089e-04  1.00290740e-03  1.09367026e-03  1.17210986e-03\n",
      "  1.23821141e-03  1.29221310e-03  1.33483030e-03  1.36588432e-03\n",
      "  1.38574746e-03  1.39538851e-03  1.39523949e-03  1.38626900e-03\n",
      "  1.36957981e-03  1.34698965e-03  1.32186629e-03  1.29993190e-03\n",
      "  1.29142334e-03  1.31493725e-03  1.40376296e-03  1.61383906e-03\n",
      "  2.03935523e-03  2.83017009e-03  4.21771873e-03  6.54545007e-03\n",
      "  1.03035942e-02  1.61614642e-02  2.49949768e-02  3.78939919e-02\n",
      "  5.61441444e-02  8.11646134e-02  1.14395268e-01  1.57122120e-01\n",
      "  2.10234851e-01  2.73930699e-01  3.47397387e-01  4.28564638e-01\n",
      "  5.14060378e-01  5.99514246e-01  6.80255413e-01  7.52236903e-01\n",
      "  8.12837780e-01  8.61205220e-01  8.98045123e-01  9.25054967e-01\n",
      "  9.44300950e-01  9.57757652e-01  9.67070282e-01  9.73494291e-01\n",
      "  9.77933764e-01  9.81016815e-01  9.83171225e-01  9.84682620e-01\n",
      "  9.85746861e-01  9.86500978e-01  9.87038374e-01  9.87423062e-01\n",
      "  9.87699330e-01  9.87898171e-01  9.88041461e-01  9.88144815e-01\n",
      "  9.88219321e-01  9.88273025e-01  9.88311708e-01  9.88339543e-01\n",
      "  9.88359571e-01  9.88373935e-01  9.88384247e-01  9.88391638e-01\n",
      "  9.88396883e-01  9.88400698e-01  9.88403380e-01  9.88405287e-01\n",
      "  9.88406658e-01  9.88407612e-01  9.88408267e-01  9.88408804e-01\n",
      "  9.88409102e-01  9.88409340e-01  9.88409579e-01  9.88409698e-01\n",
      "  9.88409758e-01  9.88409817e-01  9.88409877e-01  9.88409936e-01\n",
      "  9.88409936e-01  9.88409996e-01  9.88409996e-01  9.88409996e-01\n",
      "  9.88410056e-01  9.88410056e-01  9.88410056e-01  9.88410056e-01\n",
      "  9.88410115e-01  9.88410115e-01  9.88410115e-01  9.88410115e-01\n",
      "  9.88410115e-01  9.88410175e-01  9.88410175e-01  9.88410175e-01\n",
      "  9.88410175e-01  9.88410175e-01  9.88410175e-01  9.88410234e-01\n",
      "  9.88410234e-01  9.88410234e-01  9.88410234e-01  9.88410234e-01\n",
      "  9.88410234e-01  9.88410234e-01  9.88410234e-01  9.88410234e-01\n",
      "  9.88410234e-01  9.88410234e-01  9.88410234e-01  9.88410294e-01\n",
      "  9.88410294e-01  9.88410294e-01  9.88410294e-01  9.88410294e-01\n",
      "  9.88410294e-01  9.88410294e-01  9.88410294e-01  9.88410294e-01\n",
      "  9.88410294e-01  9.88410294e-01  9.88410294e-01  9.88410294e-01\n",
      "  9.88410294e-01  9.88410294e-01  9.88410294e-01  9.88410294e-01\n",
      "  9.88410354e-01  9.88410294e-01  9.88410354e-01  9.88410354e-01\n",
      "  9.88410354e-01  9.88410354e-01  9.88410354e-01  9.88410354e-01\n",
      "  9.88410354e-01  9.88410354e-01  9.88410354e-01  9.88410354e-01\n",
      "  9.88410354e-01  9.88410354e-01  9.88410354e-01  9.88410354e-01\n",
      "  9.88410354e-01  9.88410354e-01  9.88410354e-01  9.88410354e-01\n",
      "  9.88410354e-01  9.88410354e-01  9.88410354e-01  9.88410354e-01\n",
      "  9.88410354e-01  9.88410354e-01  9.88410354e-01  9.88410354e-01\n",
      "  9.88410354e-01  9.88410354e-01  9.88410354e-01  9.88410354e-01\n",
      "  9.88410354e-01  9.88410354e-01  9.88410354e-01  9.88410354e-01\n",
      "  9.88410354e-01  9.88410354e-01  9.88410354e-01  9.88410354e-01\n",
      "  9.88410354e-01  9.88410354e-01  9.88410354e-01  9.88410354e-01\n",
      "  9.88410354e-01  9.88410354e-01  9.88410354e-01  9.88410354e-01\n",
      "  9.88410354e-01  9.88410354e-01  9.88410354e-01  9.88410354e-01\n",
      "  9.88410354e-01  9.88410354e-01  9.88410354e-01  9.88410354e-01\n",
      "  9.88410354e-01  9.88410354e-01  9.88410354e-01  9.88410354e-01\n",
      "  9.88410354e-01  9.88410354e-01  9.88410354e-01  9.88410354e-01\n",
      "  9.88410354e-01  9.88410354e-01  9.88410354e-01  9.88410354e-01\n",
      "  9.88410354e-01  9.88410354e-01  9.88410354e-01  9.88410354e-01\n",
      "  9.88410354e-01  9.88410354e-01  9.88410354e-01  9.88410354e-01\n",
      "  9.88410354e-01  9.88410354e-01  9.88410354e-01  9.88410354e-01\n",
      "  9.88410354e-01  9.88410354e-01  9.88410354e-01  9.88410354e-01\n",
      "  9.88410354e-01  9.88410354e-01  9.88410354e-01  9.88410354e-01\n",
      "  9.88410354e-01  9.88410354e-01  9.88410354e-01  9.88410354e-01]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "X_test_all,Y_test_all = test_dataset.next_batch(1)\n",
    "X_test,length = X_test_all\n",
    "Y_test,length = Y_test_all\n",
    "X_test = X_test[:,:,np.newaxis]\n",
    "Y_test = Y_test[:,:,np.newaxis]\n",
    "Y_hat = model.predict(X_test)\n",
    "Y_hat = np.squeeze(Y_hat)\n",
    "print(Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow -GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
