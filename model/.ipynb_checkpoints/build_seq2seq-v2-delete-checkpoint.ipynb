{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.activations import relu\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解决keras 显存问题\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.25\n",
    "set_session(tf.Session(config=config)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = 10\n",
    "input_dim = 1\n",
    "MAX_LENGTH = 300 # 单个字符的长度\n",
    "output_length = 10\n",
    "output_dim = 4\n",
    "\n",
    "samples = 100\n",
    "hidden_dim = 3\n",
    "hidden_size = 150 # must be a half of Max_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random((samples, MAX_LENGTH)) ## 100,300\n",
    "y = np.random.random((samples, MAX_LENGTH))\n",
    "z = np.random.random((samples, MAX_LENGTH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](./encoder-network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_size = [embedding_length,embedding_dim,input_length]\n",
    "embedding_size = [1000,64,300]\n",
    "# hidden_size is the length of the kernel utils  = enc_units\n",
    "class EncoderRNN(Model):\n",
    "    def __init__(self, embedding_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = Embedding(embedding_size[0], embedding_size[1],input_length=embedding_size[2])\n",
    "        self.gru = GRU(hidden_size, return_sequences=True, return_state=True)\n",
    "        self.deepgru = GRU(hidden_size*2, return_sequences=True, return_state=True)\n",
    "        self.lastdeepgru = GRU(hidden_size*2, return_sequences=False, return_state=True)\n",
    "        self.bigru = Bidirectional(self.gru)\n",
    "\n",
    "    def __call__(self, en_input):\n",
    "        state_h = []\n",
    "        emb = self.embedding(en_inputs)\n",
    "        encoder_out, fwd_h1, bck_h1 = self.bigru(emb)\n",
    "        state_h.append(concatenate([fwd_h1, bck_h1]))\n",
    "        if hidden_dim>1:\n",
    "            for i in range(1,hidden_dim):\n",
    "                encoder_out, en_hidden = self.deepgru(encoder_out)\n",
    "                state_h.append(en_hidden)\n",
    "        output = encoder_out\n",
    "        hidden = state_h\n",
    "        return output, hidden\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        init_state = [tf.zeros((1, self.hidden_size)) for i in range(2)]\n",
    "        return init_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](./decoder-network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(Model):\n",
    "    def __init__(self, embedding_size, hidden_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = Embedding(embedding_size[0], embedding_size[1],input_length=embedding_size[2])\n",
    "        self.gru = GRU(hidden_size, return_sequences=True, return_state=True)\n",
    "        self.deepgru = GRU(hidden_size*2, return_sequences=True, return_state=True)\n",
    "        self.lastdeepgru = GRU(hidden_size*2, return_sequences=False, return_state=True)\n",
    "        self.bigru = Bidirectional(self.gru)\n",
    "        \n",
    "        self.out = Dense(output_length)\n",
    "        self.softmax = Activation('softmax')\n",
    "\n",
    "    def __call__(self, de_input, hiddens):\n",
    "        state_h = []\n",
    "        emb = self.embedding(en_inputs)\n",
    "        emb = Activation('relu')(emb)\n",
    "        # !挖个坑，这里没有初始化hidden_state\n",
    "        output, fwd_h, bck_h = self.bigru(emb)\n",
    "        state_h.append(concatenate([fwd_h, bck_h]))\n",
    "        if len(hiddens)>2:\n",
    "            for hidden in hiddens[1:-1]:\n",
    "                output, de_hidden = self.deepgru(output, hidden)\n",
    "                state_h.append(de_hidden)\n",
    "        if len(hiddens)>1:\n",
    "            output, de_hidden = self.lastdeepgru(output, hidden)\n",
    "            state_h.append(de_hidden)\n",
    "        hidden = state_h\n",
    "        output = self.softmax(output)\n",
    "        print(type(output))\n",
    "        return output, hidden\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        init_state = tf.zeros((1, self.hidden_size))\n",
    "        return init_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](./attention-decoder-network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttnDecoderRNN(Model):\n",
    "    def __init__(self, embedding_size ,hidden_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = Embedding(embedding_size[0], embedding_size[1],input_length=embedding_size[2],name ='atten_embed')\n",
    "        self.attn = Dense(self.max_length,name = 'atten_attn')\n",
    "        self.attn_combine = Dense(self.hidden_size,name = 'atten_combine')\n",
    "        self.dropout = Dropout(self.dropout_p,name ='atten_dropout')\n",
    "        \n",
    "        self.gru = GRU(hidden_size, return_sequences=True, return_state=True,name = 'atten_gru')\n",
    "        self.deepgru = GRU(hidden_size*2, return_sequences=True, return_state=True,name = 'atten_deepgru')\n",
    "        self.lastdeepgru = GRU(hidden_size*2, return_sequences=False, return_state=True,name='atten_lastdeepgru')\n",
    "        self.bigru = Bidirectional(self.gru,name='atten_bigru')\n",
    "        self.batch_dot = Lambda(lambda layers:K.batch_dot(layers[0],layers[1]))\n",
    "        self.out = Dense(output_length)\n",
    "        self.softmax =Softmax(axis=-1)\n",
    "        #包装层\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def __call__(self, de_input, hiddens, encoder_outputs):\n",
    "        state_h = []\n",
    "        embedded = self.embedding(de_input)\n",
    "        print(embedded.shape)\n",
    "        # TODO ; use lambda https://www.cnblogs.com/jqpy1994/p/11433746.html or  https://keras.io/zh/layers/core/\n",
    "        #embedded = K.reshape(embedded,[samples,embedding_size[1]*embedding_size[2]])\n",
    "        embedded = Reshape((1,embedding_size[1]*embedding_size[2]))(embedded)\n",
    "        print(type(embedded))\n",
    "        embedded = Lambda(lambda x:K.squeeze(x,1))(embedded)\n",
    "    \n",
    "        embedded = self.dropout(embedded) # Dim:(Batch Size , Decoder Hidden Size + Embedding Size)\n",
    "        if type(hiddens) == type(list()):\n",
    "            hidden = hiddens[-1]\n",
    "        \n",
    "        # hidden = K.reshape(hidden,[samples,self.hidden_size*2]) \n",
    "        hidden = Reshape((1,self.hidden_size*2))(hidden)\n",
    "        hidden = Lambda(lambda x:K.squeeze(x,1))(hidden)\n",
    "        concat = Concatenate(1,name='atten_concat2')([embedded, hidden])\n",
    "        # note: 从这里开始，把两个向量拼接起来    \n",
    "        attn_weights =self.softmax(\n",
    "            Dense(self.max_length)(concat))\n",
    "        atten_weights = Reshape((1,-1))(attn_weights)\n",
    "        attn_applied = self.batch_dot([atten_weights,encoder_outputs])\n",
    "        print(attn_applied.shape)\n",
    "        attn_applied = Lambda(lambda x:K.squeeze(x,1))(attn_applied)\n",
    "\n",
    "        \n",
    "        output = Concatenate(1)([embedded, attn_applied])\n",
    "\n",
    "        output = self.attn_combine(output)\n",
    "        \n",
    "        output = ReLU()(output)\n",
    "        output = Reshape((1,-1))(output)\n",
    "        output, fwd_h, bck_h = self.bigru(output)\n",
    "        state_h.append(concatenate([fwd_h, bck_h]))\n",
    "        if len(hiddens)>2:\n",
    "            for hidden in hiddens[1:-1]:\n",
    "                output, de_hidden = self.deepgru(output, hidden)\n",
    "                state_h.append(de_hidden)\n",
    "        if len(hiddens)>1:\n",
    "            output, de_hidden = self.lastdeepgru(output, hidden)\n",
    "            state_h.append(de_hidden)\n",
    "        hidden = state_h\n",
    "        output = self.softmax(output)\n",
    "        print(output.shape)\n",
    "        return output, hidden\n",
    "    \n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "(?, 300, 64)\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "(?, 1, 300)\n",
      "(?, 300)\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderRNN(embedding_size,hidden_size)\n",
    "decoder = DecoderRNN(embedding_size,hidden_size)\n",
    "atten = AttnDecoderRNN(embedding_size,hidden_size)# hidden_size = 6\n",
    "\n",
    "en_inputs = Input(shape=(300,),dtype=float)\n",
    "de_inputs = Input(shape=(300,),dtype=float)\n",
    "en_output, en_hidden = encoder(en_inputs)\n",
    "de_output, de_hidden = decoder(de_inputs,en_hidden)\n",
    "attent_output,attn_hidden = atten(de_inputs,de_hidden,en_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/100 [==============================] - 14s 136ms/step - loss: 0.3309\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.3309\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 4s 41ms/step - loss: 0.3309\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.3309\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 0.3309\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-b32c1e579d30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m model.fit([x,y],z,\n\u001b[1;32m      7\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m           batch_size=100)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# model.fit(x, y, nb_epoch=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Model([en_inputs,de_inputs], attent_output)\n",
    "\n",
    "model.compile(loss='mse', optimizer='sgd')\n",
    "# m = model.predict(z)\n",
    "# print(m.shape)\n",
    "model.fit([x,y],z,\n",
    "          epochs=20,\n",
    "          batch_size=100)\n",
    "# model.fit(x, y, nb_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow -GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
