{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.layers import Input, Dense, Conv1D, Dropout, MaxPooling1D, Flatten, Embedding, concatenate, LSTM, Activation\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train_one_label.csv')\n",
    "test = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vectors(train, test):\n",
    "    '''\n",
    "    使用keras对文本数据做预处理\n",
    "    '''\n",
    "    tokenizer = Tokenizer(num_words=1000)\n",
    "    tokenizer.fit_on_texts(train['comment_text'])\n",
    "\n",
    "    train_seq = tokenizer.texts_to_sequences(train['comment_text'])\n",
    "    test_seq = tokenizer.texts_to_sequences(test['comment_text'])\n",
    "\n",
    "    x_train = sequence.pad_sequences(train_seq, maxlen=200)  # shape  (* , 200)\n",
    "    y_train = train['toxic']\n",
    "    x_test = sequence.pad_sequences(test_seq, maxlen=200)  # shape (* , 200)\n",
    "    return torch.tensor(x_train).long(), torch.tensor(y_train.values).long(), torch.tensor(x_test).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test = word_vectors(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 200])\n"
     ]
    }
   ],
   "source": [
    "train = data.TensorDataset(x_train, y_train)\n",
    "test = data.TensorDataset(x_test)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = data.DataLoader(train, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(1000, 300)  # embedding之后的shape: torch.Size([200, 8, 300])\n",
    "        # 若使用预训练的词向量，需在此处指定预训练的权重\n",
    "        # embedding.weight.data.copy_(weight_matrix)\n",
    "        self.lstm = nn.LSTM(input_size=300, hidden_size=128, num_layers=1)  # torch.Size([200, 8, 128])\n",
    "        self.decoder = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out = self.lstm(embeds)[0]  # torch.Size([8, 200, 128])\n",
    "        # 取最后一个时间步\n",
    "        final = lstm_out[:, -1, :]  # 8*128\n",
    "        y = self.decoder(final)  # 8*2 \n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM()\n",
    "model.train()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)\n",
    "loss_funtion = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  0,   0,   0,  ..., 530,  42, 215],\n",
      "        [  0,   0,   0,  ...,   2,  28,  78],\n",
      "        [  0,   0,   0,  ...,   1, 475, 476],\n",
      "        ...,\n",
      "        [  0,   0,   0,  ..., 533, 534, 535],\n",
      "        [  0,   0,   0,  ..., 562, 563,   9],\n",
      "        [  0,   0,   0,  ..., 574, 575, 576]]), tensor([0, 0, 0, 0, 1, 1, 0, 0])]\n",
      "tensor(0.4170, grad_fn=<NllLossBackward>)\n",
      "[tensor([[  0,   0,   0,  ..., 108,  49,  97],\n",
      "        [  0,   0,   0,  ...,  76, 313,  38],\n",
      "        [  0,   0,   0,  ...,  19, 151,  11],\n",
      "        ...,\n",
      "        [  0,   0,   0,  ...,  44, 305, 306],\n",
      "        [  0,   0,   0,  ...,  10, 487,  71],\n",
      "        [  0,   0,   0,  ..., 201,  19, 622]]), tensor([0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "tensor(0.0414, grad_fn=<NllLossBackward>)\n",
      "[tensor([[  0,   0,   0,  ...,  11,  42, 316],\n",
      "        [  0,   0,   0,  ..., 345, 346, 347],\n",
      "        [  0,   0,   0,  ..., 578,  93, 579],\n",
      "        ...,\n",
      "        [  0,   0,   0,  ..., 104,   4, 350],\n",
      "        [  0,   0,   0,  ...,   1, 274, 140],\n",
      "        [  0,   0,   0,  ..., 251, 252, 253]]), tensor([1, 0, 0, 0, 0, 0, 0, 0])]\n",
      "tensor(0.1137, grad_fn=<NllLossBackward>)\n",
      "[tensor([[ 34, 632,   8,   1, 633,   4, 634, 231, 635,  15,   1, 146, 128,   7,\n",
      "          46, 636, 637,   2,  12, 163, 159,  10, 171,   8,  57,  39,  83, 638,\n",
      "         639,  17,  58, 640, 641,   6,  10, 106, 118, 104, 642,  25,  55, 142,\n",
      "          15, 643,   4,  42, 644,   8,  12,  28,  99,  14,  92,  10,  79, 645,\n",
      "         646,   2, 230, 647,  57, 648,  39,   1, 192, 649,   4, 650, 651, 652,\n",
      "         653, 654, 655, 656, 657, 232, 233, 234, 658, 232, 233, 234, 659,   1,\n",
      "         660, 661,  79,   1, 662, 663,   4,   1,  40,  44,  39,  29,  36,   4,\n",
      "          64,  35, 664, 665,  14, 666,  85, 231, 667,  15,   1, 668, 128,  10,\n",
      "          72,  22, 204,   9, 669,   2, 670,   5, 671,  90, 672, 673,   4, 674,\n",
      "         129, 125,  86, 209, 675,   8,  70,  11,   1, 676, 129, 677, 678, 126,\n",
      "         679, 680, 681, 682,  75, 683,  17, 684,   2, 685,  39,   1, 686,   4,\n",
      "         687,  10,  79, 688,  12, 689, 690, 691,   1, 217, 692, 693,  60,  46,\n",
      "         220, 694,  90,   7,   1,  36, 695, 696, 125,  86,  26, 697, 698,   2,\n",
      "         699,  41, 129, 700,   2, 701,  18,   5, 702,  34, 703,  12,  28,   9,\n",
      "         704,   2,  14, 705]]), tensor([0])]\n",
      "tensor(0.1348, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for epoch, batch in enumerate(train_iter):\n",
    "    optimizer.zero_grad()\n",
    "    predicted = model(batch[0])\n",
    "    print(batch)\n",
    "    loss = loss_funtion(predicted, batch[1])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 补充：使用Keras搭建LSTM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 1000   # 词汇表大小\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "# 裁剪文本为 maxlen 大小的长度（取最后部分，基于前 max_features 个常用词）\n",
    "maxlen = 200 \n",
    "batch_size = 8  # 批数据量大小\n",
    "\n",
    "model = Sequential()\n",
    "# 嵌入层，每个词维度为128\n",
    "model.add(Embedding(max_features, 128))\n",
    "# LSTM层，输出维度128，可以尝试着换成 GRU 试试\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))  # try using a GRU instead, for fun\n",
    "model.add(Dense(1))   # 单神经元全连接层\n",
    "model.add(Activation('sigmoid'))   # sigmoid 激活函数层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22 samples, validate on 3 samples\n",
      "Epoch 1/3\n",
      "22/22 [==============================] - 2s 96ms/step - loss: 0.6901 - acc: 0.6364 - val_loss: 0.6740 - val_acc: 1.0000\n",
      "Epoch 2/3\n",
      "22/22 [==============================] - 1s 35ms/step - loss: 0.6653 - acc: 0.8636 - val_loss: 0.6378 - val_acc: 1.0000\n",
      "Epoch 3/3\n",
      "22/22 [==============================] - 1s 32ms/step - loss: 0.6218 - acc: 0.8636 - val_loss: 0.5511 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x127685748>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train.numpy(), y_train.numpy(), \n",
    "          validation_split=0.1, \n",
    "          batch_size=batch_size, \n",
    "          epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
